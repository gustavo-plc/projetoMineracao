# üìò Manual T√©cnico: Pipeline de Engenharia de Dados (ETL)

**Projeto:** Minera√ß√£o de Gastos P√∫blicos (Cart√£o Corporativo)
**N√≠vel:** Iniciante / Intermedi√°rio
**Tecnologia:** Python + PySpark (Local no Windows)

---

## 1. Vis√£o Geral do Projeto

Este projeto tem como objetivo processar uma grande quantidade de planilhas governamentais (despesas p√∫blicas), que est√£o despadronizadas e em formatos variados, transformando-as em uma base de dados √∫nica, limpa e otimizada para Intelig√™ncia Artificial.

### O Fluxo de Dados (Pipeline)


[Image of data processing pipeline]

1.  **Entrada:** Arquivos Excel (`.xls`, `.xlsx`) e OpenDocument (`.ods`) separados por ano.
2.  **Pr√©-processamento:** Convers√£o de formatos n√£o suportados.
3.  **Processamento (Spark):** Leitura, limpeza de texto, padroniza√ß√£o de colunas e convers√£o de tipos.
4.  **Armazenamento Intermedi√°rio:** Convers√£o para formato Parquet (colunar).
5.  **Sa√≠da Final:** Um arquivo √∫nico consolidado (`Consolidado.parquet`).

---

## 2. Configura√ß√£o do Ambiente (Pr√©-requisitos)

Para que o Spark (originalmente feito para Linux) rode no Windows, o ambiente foi configurado manualmente.

* **Motor:** PySpark 3.5.3 (Vers√£o LTS est√°vel).
* **Java:** JDK 17.
* **Emulador Hadoop (Winutils):**
    * **O que √©:** Pequenos programas (`winutils.exe`, `hadoop.dll`) que "enganam" o Spark para ele achar que est√° num cluster Hadoop.
    * **Instala√ß√£o:**
        * `winutils.exe` -> `C:\hadoop\bin`
        * `hadoop.dll` -> `C:\Windows\System32` (Crucial para evitar erro `NativeIO`).
    * **Fonte:** [Reposit√≥rio Winutils (Hadoop 3.3.5)](https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.5/bin)

---

## 3. Detalhamento das Etapas (Step-by-Step)

### Etapa 0: Convers√£o de Arquivos (Script `converter_arquivos.py`)
O Spark n√£o l√™ nativamente arquivos `.ods` (LibreOffice) com efici√™ncia. Antes de tudo, convertemos eles.

* **Entrada:** Pasta `dados/input` contendo arquivos misturados (`.ods`, `.xls`, `.xlsx`).
* **A√ß√£o:** O script varre as pastas, detecta arquivos `.ods` e usa a biblioteca `odfpy` + `pandas` para salvar uma c√≥pia em `.xlsx`.
* **Sa√≠da:** Arquivos `.xlsx` criados ao lado dos originais.

---

### Etapa 1: Configura√ß√£o da Sess√£o (C√©lula 1)
Inicia o "motor" do Spark.

* **Configura√ß√µes Chave:**
    * `spark.driver.memory = "4g"`: D√° 4GB de RAM para o processo.
    * `shuffle.partitions = "8"`: Divide os dados em 8 peda√ßos (ideal para processadores de 8-16 threads locais).
    * `bindAddress = "127.0.0.1"`: For√ßa o Spark a usar a rede local interna, evitando queda de conex√£o se o Wi-Fi oscilar.

---

### Etapa 2: Defini√ß√£o de Regras (C√©lulas 2 a 5)
Aqui n√£o processamos dados ainda, apenas ensinamos ao Spark "como" processar.

#### A. O Schema (Contrato de Dados)
Define que a coluna "valor" deve ser decimal e "ano" deve ser inteiro.
* **Entrada:** Nenhuma.
* **Sa√≠da:** Objeto `StructType` (Schema).

#### B. Fun√ß√µes de Limpeza (Nativas)
Criamos fun√ß√µes que rodam direto na JVM (Java Virtual Machine) do Spark para m√°xima velocidade.

1.  **`clean_column_names(df)`**:
    * **Entrada:** DataFrame com colunas sujas (ex: `Objeto da Aquisi√ß√£o`, `Valor (R$)`).
    * **A√ß√£o:** Remove acentos, troca espa√ßos por `_` e remove par√™nteses.
    * **Sa√≠da:** DataFrame com colunas limpas (ex: `objeto_aquisicao`, `valor`).

2.  **`process_dataframe(df)`**:
    * **Entrada:** DataFrame bruto.
    * **A√ß√£o:**
        * *Texto:* `translate` (troca `√ß`->`c`, `√£`->`a`) + `regexp_replace` (remove s√≠mbolos).
        * *Valor:* Remove "R$", pontos e converte para n√∫mero.
        * *CPF:* Remove pontos e tra√ßos.
    * **Sa√≠da:** DataFrame higienizado.

---

### Etapa 3: Ingest√£o e Convers√£o (C√©lula 6)
O "cora√ß√£o" do processamento. Transforma planilhas lentas em dados r√°pidos.

* **Entrada:** Pastas de anos (`dados/input/2016`, `2017`...) contendo arquivos Excel.
* **Processo (Loop):**
    1.  Usa `pandas` para abrir apenas o cabe√ßalho do Excel e descobrir o **nome da primeira aba** (evita erro "Sheet1 not found").
    2.  O Spark l√™ essa aba espec√≠fica.
    3.  Aplica `process_dataframe`.
    4.  Salva em Parquet.
* **Sa√≠da:** Milhares de arquivos `.parquet` organizados em `dados/Parquet/{ANO}/{ARQUIVO}`.

> **Por que Parquet?** √â um formato bin√°rio e colunar. Um arquivo Excel de 50MB vira um Parquet de 5MB e o Spark consegue l√™-lo 100x mais r√°pido.

---

### Etapa 4: Consolida√ß√£o (C√©lula 7)
Junta os milhares de pedacinhos em um arquivo mestre.

* **Entrada:** Todas as subpastas v√°lidas dentro de `dados/Parquet`.
* **A√ß√£o:**
    1.  Usa `glob` para listar apenas pastas que realmente cont√™m dados (ignora pastas vazias de erros passados).
    2.  `spark.read.parquet(*pastas)`: L√™ tudo simultaneamente.
    3.  Aplica filtro final (remove linhas onde Valor √© 0 ou Descri√ß√£o √© vazia).
    4.  `coalesce(1)`: Funde os dados em um √∫nico arquivo f√≠sico.
* **Sa√≠da:** Um √∫nico arquivo (pasta) em `dados/Consolidado`.

---

## 4. Resumo de Comandos √öteis

* **Rodar o script:**
    ```powershell
    python analise_dados.py
    ```
* **Instalar nova biblioteca:**
    ```powershell
    pip install nome_da_lib
    ```
* **Verificar se o Spark est√° vivo:** Olhe o terminal. Se houver uma barra de progresso `[Stage 0:=>   (0 + 1) / 1]`, ele est√° trabalhando.

---

**Status Atual:** O ETL foi executado com sucesso. 72 arquivos processados, 0 falhas, gerando uma base consolidada de ~54 mil registros prontos para Machine Learning.
