# üìÑ Documenta√ß√£o T√©cnica: Pipeline de ETL Local (PySpark) - Fase 1 Completa

**Projeto:** Minera√ß√£o de Dados de Gastos P√∫blicos (Cart√£o Corporativo)
**Ambiente:** Local (Windows 11 / VS Code)
**Tecnologia:** Python 3.11 + PySpark 3.5.3 (Single Node)
**Status:** Ingest√£o, Limpeza e Convers√£o para Parquet (Conclu√≠do)

## üèóÔ∏è Configura√ß√£o Cr√≠tica do Ambiente (Windows)

Para viabilizar a execu√ß√£o do Spark no Windows sem erros de *NativeIO* ou *Hadoop*, as seguintes configura√ß√µes foram aplicadas:

### 1. Bin√°rios do Hadoop (Winutils)
O Spark requer emula√ß√£o do sistema de arquivos HDFS.
* **Vers√£o Hadoop:** 3.3.5 (Compat√≠vel com Spark 3.5.3).
* **Vari√°vel de Ambiente:** `HADOOP_HOME` configurada para `C:\hadoop`.
* **Path:** `%HADOOP_HOME%\bin` adicionado ao Path do sistema.

### 2. Corre√ß√£o de DLL (UnsatisfiedLinkError)
Para corrigir o erro `java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0`, foi necess√°rio instalar manualmente a biblioteca din√¢mica do Hadoop.

* **Arquivo:** `hadoop.dll`
* **Origem (Download):** [https://github.com/cdarlint/winutils/blob/master/hadoop-3.3.5/bin/hadoop.dll](https://github.com/cdarlint/winutils/blob/master/hadoop-3.3.5/bin/hadoop.dll)
* **Instala√ß√£o:** O arquivo foi copiado para:
    1.  `C:\hadoop\bin`
    2.  `C:\Windows\System32` (Essencial para o carregamento global pelo Java).

### 3. Bibliotecas Python Adicionais
Al√©m do PySpark, foram instaladas depend√™ncias para manipula√ß√£o de arquivos legados:
* **`xlrd`**: Necess√°rio para ler arquivos Excel antigos (`.xls`) gerados antes de 2019.
* **`odfpy`**: Para convers√£o de arquivos OpenDocument (`.ods`).
* **`pandas`**: Utilizado para introspec√ß√£o r√°pida de metadados (nomes de abas) antes da leitura com Spark.

---

## üõ†Ô∏è Detalhamento do Pipeline (C√©lula a C√©lula)

### C√©lula 1: Configura√ß√£o e Inicializa√ß√£o
* **Spark Session:** Downgrade estrat√©gico para **PySpark 3.5.3** para estabilidade.
* **Configura√ß√£o de Rede:** `spark.driver.bindAddress` fixado em `127.0.0.1` para evitar quedas de conex√£o (*WinError 10054*).
* **Recursos:** Mem√≥ria do driver ajustada para `4g` e parti√ß√µes de shuffle reduzidas para `8` para evitar *OOM (Out Of Memory)* no processamento local.

### C√©lula 2: Schema e Mapeamento
* Define a estrutura r√≠gida dos dados (`DecimalType` para valores) e normaliza nomes de colunas variantes (ex: "Motivo" vs "Objeto da Aquisi√ß√£o").

### C√©lulas 3, 4 e 5: Processamento Nativo (Otimizado)
* **Estrat√©gia:** Substitui√ß√£o de UDFs (Python) por fun√ß√µes nativas do Spark (`expr`, `regexp_replace`).
* **Ganho:** Elimina√ß√£o da sobrecarga de serializa√ß√£o Python/JVM, resultando em processamento in-memory de alta performance.
* **Limpeza:** Remo√ß√£o de acentos, caracteres especiais e formata√ß√£o monet√°ria realizada em uma √∫nica passada (*lazy evaluation*).

### C√©lula 6: Motor de Execu√ß√£o e Convers√£o (XLS -> Parquet)
Esta c√©lula orquestra a leitura e grava√ß√£o dos dados massivos.

**L√≥gica de Execu√ß√£o:**
1.  **Itera√ß√£o:** Varre as pastas de input ano a ano (2016-2025).
2.  **Detec√ß√£o Din√¢mica de Abas:** Utiliza `pandas.ExcelFile` para ler os metadados do arquivo e descobrir o nome exato da primeira aba (ex: "Planilha1", "Sheet1", "Relatorio"), evitando erros de "Unknown Sheet".
3.  **Leitura Spark:** Carrega os dados usando a biblioteca `com.crealytics:spark-excel`.
4.  **Transforma√ß√£o:** Aplica a fun√ß√£o `process_dataframe` (C√©lulas 3-5).
5.  **Carga (Write):** Salva o resultado particionado em formato **Parquet** com compress√£o **Snappy**.

**Resultado da Execu√ß√£o:**
* **Total Processado:** 72 arquivos.
* **Sucesso:** 100% (72/72).
* **Erros Superados:**
    * *Missing xlrd:* Resolvido via instala√ß√£o da lib.
    * *Unknown Sheet:* Resolvido via detec√ß√£o din√¢mica com Pandas.
    * *NativeIO/DLL:* Resolvido via `hadoop.dll` no System32.

---

## üìÇ Estrutura de Sa√≠da
Os dados limpos encontram-se em:
`C:\VSCode\projetoMineracao\dados\Parquet\{ANO}\{NOME_ARQUIVO}`

Cada pasta cont√©m os arquivos `.parquet` prontos para a etapa de consolida√ß√£o e an√°lise (Machine Learning).
