import os

# ConteÃºdo da documentaÃ§Ã£o didÃ¡tica e completa
conteudo_docs = """# ğŸ“˜ Manual TÃ©cnico: Pipeline de Engenharia de Dados (ETL)

**Projeto:** MineraÃ§Ã£o de Gastos PÃºblicos (CartÃ£o Corporativo)
**NÃ­vel:** Iniciante / IntermediÃ¡rio
**Tecnologia:** Python + PySpark (Local no Windows)

---

## 1. VisÃ£o Geral do Projeto

Este projeto tem como objetivo processar uma grande quantidade de planilhas governamentais (despesas pÃºblicas), que estÃ£o despadronizadas e em formatos variados, transformando-as em uma base de dados Ãºnica, limpa e otimizada para InteligÃªncia Artificial.

### O Fluxo de Dados (Pipeline)


[Image of data processing pipeline]

1.  **Entrada:** Arquivos Excel (`.xls`, `.xlsx`) e OpenDocument (`.ods`) separados por ano.
2.  **PrÃ©-processamento:** ConversÃ£o de formatos nÃ£o suportados.
3.  **Processamento (Spark):** Leitura, limpeza de texto, padronizaÃ§Ã£o de colunas e conversÃ£o de tipos.
4.  **Armazenamento IntermediÃ¡rio:** ConversÃ£o para formato Parquet (colunar).
5.  **SaÃ­da Final:** Um arquivo Ãºnico consolidado (`Consolidado.parquet`).

---

## 2. ConfiguraÃ§Ã£o do Ambiente (PrÃ©-requisitos)

Para que o Spark (originalmente feito para Linux) rode no Windows, o ambiente foi configurado manualmente.

* **Motor:** PySpark 3.5.3 (VersÃ£o LTS estÃ¡vel).
* **Java:** JDK 17.
* **Emulador Hadoop (Winutils):**
    * **O que Ã©:** Pequenos programas (`winutils.exe`, `hadoop.dll`) que "enganam" o Spark para ele achar que estÃ¡ num cluster Hadoop.
    * **InstalaÃ§Ã£o:**
        * `winutils.exe` -> `C:\\hadoop\\bin`
        * `hadoop.dll` -> `C:\\Windows\\System32` (Crucial para evitar erro `NativeIO`).
    * **Fonte:** [RepositÃ³rio Winutils (Hadoop 3.3.5)](https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.5/bin)

---

## 3. Detalhamento das Etapas (Step-by-Step)

### Etapa 0: ConversÃ£o de Arquivos (Script `converter_arquivos.py`)
O Spark nÃ£o lÃª nativamente arquivos `.ods` (LibreOffice) com eficiÃªncia. Antes de tudo, convertemos eles.

* **Entrada:** Pasta `dados/input` contendo arquivos misturados (`.ods`, `.xls`, `.xlsx`).
* **AÃ§Ã£o:** O script varre as pastas, detecta arquivos `.ods` e usa a biblioteca `odfpy` + `pandas` para salvar uma cÃ³pia em `.xlsx`.
* **SaÃ­da:** Arquivos `.xlsx` criados ao lado dos originais.

---

### Etapa 1: ConfiguraÃ§Ã£o da SessÃ£o (CÃ©lula 1)
Inicia o "motor" do Spark.

* **ConfiguraÃ§Ãµes Chave:**
    * `spark.driver.memory = "4g"`: DÃ¡ 4GB de RAM para o processo.
    * `shuffle.partitions = "8"`: Divide os dados em 8 pedaÃ§os (ideal para processadores de 8-16 threads locais).
    * `bindAddress = "127.0.0.1"`: ForÃ§a o Spark a usar a rede local interna, evitando queda de conexÃ£o se o Wi-Fi oscilar.

---

### Etapa 2: DefiniÃ§Ã£o de Regras (CÃ©lulas 2 a 5)
Aqui nÃ£o processamos dados ainda, apenas ensinamos ao Spark "como" processar.

#### A. O Schema (Contrato de Dados)
Define que a coluna "valor" deve ser decimal e "ano" deve ser inteiro.
* **Entrada:** Nenhuma.
* **SaÃ­da:** Objeto `StructType` (Schema).

#### B. FunÃ§Ãµes de Limpeza (Nativas)
Criamos funÃ§Ãµes que rodam direto na JVM (Java Virtual Machine) do Spark para mÃ¡xima velocidade.

1.  **`clean_column_names(df)`**:
    * **Entrada:** DataFrame com colunas sujas (ex: `Objeto da AquisiÃ§Ã£o`, `Valor (R$)`).
    * **AÃ§Ã£o:** Remove acentos, troca espaÃ§os por `_` e remove parÃªnteses.
    * **SaÃ­da:** DataFrame com colunas limpas (ex: `objeto_aquisicao`, `valor`).

2.  **`process_dataframe(df)`**:
    * **Entrada:** DataFrame bruto.
    * **AÃ§Ã£o:**
        * *Texto:* `translate` (troca `Ã§`->`c`, `Ã£`->`a`) + `regexp_replace` (remove sÃ­mbolos).
        * *Valor:* Remove "R$", pontos e converte para nÃºmero.
        * *CPF:* Remove pontos e traÃ§os.
    * **SaÃ­da:** DataFrame higienizado.

---

### Etapa 3: IngestÃ£o e ConversÃ£o (CÃ©lula 6)
O "coraÃ§Ã£o" do processamento. Transforma planilhas lentas em dados rÃ¡pidos.

* **Entrada:** Pastas de anos (`dados/input/2016`, `2017`...) contendo arquivos Excel.
* **Processo (Loop):**
    1.  Usa `pandas` para abrir apenas o cabeÃ§alho do Excel e descobrir o **nome da primeira aba** (evita erro "Sheet1 not found").
    2.  O Spark lÃª essa aba especÃ­fica.
    3.  Aplica `process_dataframe`.
    4.  Salva em Parquet.
* **SaÃ­da:** Milhares de arquivos `.parquet` organizados em `dados/Parquet/{ANO}/{ARQUIVO}`.

> **Por que Parquet?** Ã‰ um formato binÃ¡rio e colunar. Um arquivo Excel de 50MB vira um Parquet de 5MB e o Spark consegue lÃª-lo 100x mais rÃ¡pido.

---

### Etapa 4: ConsolidaÃ§Ã£o (CÃ©lula 7)
Junta os milhares de pedacinhos em um arquivo mestre.

* **Entrada:** Todas as subpastas vÃ¡lidas dentro de `dados/Parquet`.
* **AÃ§Ã£o:**
    1.  Usa `glob` para listar apenas pastas que realmente contÃªm dados (ignora pastas vazias de erros passados).
    2.  `spark.read.parquet(*pastas)`: LÃª tudo simultaneamente.
    3.  Aplica filtro final (remove linhas onde Valor Ã© 0 ou DescriÃ§Ã£o Ã© vazia).
    4.  `coalesce(1)`: Funde os dados em um Ãºnico arquivo fÃ­sico.
* **SaÃ­da:** Um Ãºnico arquivo (pasta) em `dados/Consolidado`.

---

## 4. Resumo de Comandos Ãšteis

* **Rodar o script:**
    ```powershell
    python analise_dados.py
    ```
* **Instalar nova biblioteca:**
    ```powershell
    pip install nome_da_lib
    ```
* **Verificar se o Spark estÃ¡ vivo:** Olhe o terminal. Se houver uma barra de progresso `[Stage 0:=>   (0 + 1) / 1]`, ele estÃ¡ trabalhando.

---

**Status Atual:** O ETL foi executado com sucesso. 72 arquivos processados, 0 falhas, gerando uma base consolidada de ~54 mil registros prontos para Machine Learning.
"""

nome_arquivo = "Manual_Tecnico_ETL_Iniciante.md"
caminho_completo = os.path.join(os.getcwd(), nome_arquivo)

try:
    with open(caminho_completo, "w", encoding="utf-8") as f:
        f.write(conteudo_docs)
    print(f"âœ… Manual TÃ©cnico gerado com sucesso!")
    print(f"ğŸ“‚ Local: {caminho_completo}")
    print("ğŸ’¡ Abra no VS Code para ler com formataÃ§Ã£o.")
except Exception as e:
    print(f"âŒ Erro: {e}")