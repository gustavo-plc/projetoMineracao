import os

# Conte√∫do da documenta√ß√£o
conteudo_docs = """# üìÑ Documenta√ß√£o T√©cnica: Pipeline de ETL Local (PySpark)

**Projeto:** Minera√ß√£o de Dados de Gastos P√∫blicos (Cart√£o Corporativo)
**Ambiente:** Local (Windows 11 / VS Code)
**Tecnologia:** Python 3.11 + PySpark 3.5.3 (Single Node)
**Escopo:** Configura√ß√£o at√© C√©lula 5 (Pr√©-processamento)

## Vis√£o Geral
Este script realiza a Extra√ß√£o, Transforma√ß√£o e Carga (ETL) de arquivos de despesas p√∫blicas (originalmente em `.ods`/`.xlsx`). O objetivo √© normalizar dados heterog√™neos provenientes de diferentes anos, limpar inconsist√™ncias de texto e valores, e preparar o dataset para an√°lise em formato colunar (Parquet).

---

## üõ†Ô∏è Detalhamento por C√©lula

### C√©lula 1: Configura√ß√£o de Ambiente e Inicializa√ß√£o
**Objetivo:** Preparar o sistema operacional e instanciar o motor de processamento distribu√≠do (Spark) em modo local.

* **Corre√ß√£o de Ambiente Windows:**
    * Define vari√°veis `PYSPARK_PYTHON` e `PYSPARK_DRIVER_PYTHON` apontando para o execut√°vel do `venv`. Isso previne o erro onde o Spark tenta invocar o Python global ou da Microsoft Store.
* **Gest√£o de Diret√≥rios:**
    * Utiliza `os.getcwd()` e `os.path.join` para definir caminhos relativos (`dados/input`, `dados/Parquet`). Isso torna o projeto port√°vel.
    * Cria diret√≥rios automaticamente com `os.makedirs`.
* **Instancia√ß√£o da `SparkSession`:**
    * **Vers√£o:** Configurada para PySpark 3.5.3 (LTS) para estabilidade no Windows.
    * **Depend√™ncia Excel:** Carrega o pacote `com.crealytics:spark-excel_2.12:3.5.0_0.20.3` via Maven para leitura nativa de planilhas.
    * **Configura√ß√µes de Rede:** Define `spark.driver.bindAddress` como `127.0.0.1` para evitar erros de *heartbeat* e queda de conex√£o (WinError 10054).
    * **Compatibilidade:** Define modos `LEGACY` para escrita de datas em Parquet.

### C√©lula 2: Defini√ß√£o de Metadados (Schema e Mapeamento)
**Objetivo:** Estabelecer as regras de neg√≥cio para a estrutura dos dados.

* **Dicion√°rio de Mapeamento (`column_name_mapping`):**
    * Atua como um "tradutor" (De -> Para). Resolve o problema de varia√ß√£o de nomenclatura nos arquivos governamentais ao longo dos anos (ex: "Motivo" -> "Objeto da Aquisi√ß√£o").
* **Tipagem Forte (`schema_base`):**
    * Define explicitamente o tipo de dado esperado (`StructType`).
    * `valor`: Decimal(12,2) para precis√£o financeira.
    * `ano`: IntegerType.
    * `texto`: StringType.

### C√©lula 3: Fun√ß√µes de Limpeza (UDFs)
**Objetivo:** Criar fun√ß√µes puras de Python para higieniza√ß√£o de strings e registr√°-las no motor do Spark.

* **Normaliza√ß√£o de Texto (`standardize_text`):**
    * Remove acentos (Normaliza√ß√£o Unicode NFKD).
    * Remove caracteres especiais (mant√©m apenas alfanum√©ricos).
    * Converte tudo para min√∫sculas (`lower()`).
* **Padroniza√ß√£o de Colunas (`standardize_column_name`):**
    * Converte nomes de colunas para o padr√£o *snake_case* (ex: `Valor Total` -> `valor_total`).
* **Registro de UDF:**
    * A fun√ß√£o `standardize_text` √© registrada como uma **UDF (User Defined Function)**, permitindo aplica√ß√£o distribu√≠da em DataFrames.

### C√©lula 4: Padroniza√ß√£o Din√¢mica de Colunas
**Objetivo:** Garantir que o DataFrame tenha os nomes de colunas corretos antes de tentar processar os dados.

* **L√≥gica de Renomea√ß√£o:**
    * Itera sobre as colunas do arquivo bruto.
    * Verifica se o nome existe no dicion√°rio de mapeamento (C√©lula 2).
    * Se n√£o existir, aplica uma padroniza√ß√£o gen√©rica (*snake_case*).
* **Tratamento de Colis√£o:**
    * Implementa um algoritmo para detectar nomes duplicados e adiciona sufixos num√©ricos automaticamente (`valor_1`, `valor_2`) para evitar conflitos no Spark.

### C√©lula 5: Processamento e Transforma√ß√£o (ETL)
**Objetivo:** Aplicar as regras de limpeza nos dados brutos e garantir a conformidade com o schema final.

* **Limpeza de Texto:** Aplica a UDF `standardize_text` na coluna `objeto_aquisicao`.
* **Limpeza Financeira (Coluna `valor`):** Remove caracteres n√£o num√©ricos ("R$", pontos) e converte para `Decimal`.
* **Limpeza de Documentos (CPF/CNPJ):** Remove pontua√ß√£o (pontos, tra√ßos), mantendo apenas d√≠gitos.
* **Sele√ß√£o Final:** Reordena as colunas de acordo com o `schema_base` e cria colunas `NULL` caso alguma coluna esperada n√£o exista no arquivo original.

---

## üèóÔ∏è Decis√µes de Arquitetura (Ambiente Windows)

1.  **Downgrade para Spark 3.5.3:** A vers√£o 4.0.1 mostrou-se inst√°vel no Windows. A vers√£o 3.5.3 (LTS) garantiu estabilidade.
2.  **Convers√£o Pr√©via (ODS -> XLSX):** Utilizou-se `odfpy` + `pandas` para converter arquivos OpenDocument, pois a leitura nativa do Spark para ODS √© limitada.
3.  **Winutils:** Configura√ß√£o do bin√°rio `winutils.exe` (Hadoop 3.3.5) para emular o sistema de arquivos HDFS no Windows.
"""

nome_arquivo = "documentacao_tecnica_ate_celula_5.md"
caminho_completo = os.path.join(os.getcwd(), nome_arquivo)

try:
    with open(caminho_completo, "w", encoding="utf-8") as f:
        f.write(conteudo_docs)
    print(f"‚úÖ Arquivo gerado com sucesso!")
    print(f"üìÇ Local: {caminho_completo}")
    print("üí° Dica: Abra este arquivo no VS Code e pressione 'Ctrl + Shift + V' para visualizar formatado.")
except Exception as e:
    print(f"‚ùå Erro ao gerar arquivo: {e}")