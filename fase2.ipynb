{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c21c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Fase 2: An√°lise Explorat√≥ria ---\n",
      "üìÇ Buscando base consolidada em: c:\\VSCode\\projetoMineracao\\dados\\Consolidado_Final\n",
      "‚úÖ Base carregada: 54196 registros.\n",
      "\n",
      "--- Saneamento da Base ---\n",
      "Total Bruto: 54196\n",
      "‚úÖ Total Real (√önicos): 12572\n",
      "üóëÔ∏è Lixo Removido: 41624\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FASE 2: An√°lise Explorat√≥ria de Dados (EDA) e Minera√ß√£o (Completo)\n",
    "# Arquivo: analise_fase2.py\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Importa√ß√µes Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, size, lower, avg, stddev, abs as _abs, round as _round, max as _max, min as _min, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# --- 1. Configura√ß√£o de Ambiente (Windows) ---\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "print(\"--- Iniciando Fase 2: An√°lise Explorat√≥ria ---\")\n",
    "\n",
    "# --- 2. Inicializando Sess√£o Spark ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analise_Gastos_Fase2\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Otimiza√ß√£o Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# --- 3. Carregamento dos Dados ---\n",
    "BASE_DIR = os.path.join(os.getcwd(), \"dados\")\n",
    "input_path = os.path.join(BASE_DIR, \"Consolidado_Final\")\n",
    "\n",
    "print(f\"üìÇ Buscando base consolidada em: {input_path}\")\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"‚ùå ARQUIVO N√ÉO ENCONTRADO: {input_path}\")\n",
    "    sys.exit() # Encerra se n√£o achar o arquivo\n",
    "\n",
    "try:\n",
    "    df = spark.read.parquet(input_path)\n",
    "    df.cache() # Cache do dataset bruto\n",
    "    print(f\"‚úÖ Base carregada: {df.count()} registros.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro leitura: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CORRE√á√ÉO CR√çTICA: Remo√ß√£o de Duplicatas\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Saneamento da Base ---\")\n",
    "print(f\"Total Bruto: {df.count()}\")\n",
    "\n",
    "# Remove linhas onde Objeto, Valor e Favorecido s√£o id√™nticos\n",
    "# Isso elimina as repeti√ß√µes causadas pela fus√£o de c√©lulas no Excel\n",
    "df = df.dropDuplicates(['objeto_aquisicao', 'valor_transacao', 'nome_favorecido'])\n",
    "\n",
    "# For√ßa o rec√°lculo e cache na mem√≥ria\n",
    "df.cache()\n",
    "count_real = df.count()\n",
    "\n",
    "print(f\"‚úÖ Total Real (√önicos): {count_real}\")\n",
    "print(f\"üóëÔ∏è Lixo Removido: {54196 - count_real}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61f0b390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando NLP V16 (Foco: Remover Justificativas) ---\n",
      "‚úÖ NLP V16 conclu√≠do.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 6 (V16 - LIMPEZA DE JUSTIFICATIVAS): NLP Refinado\n",
    "# ==============================================================================\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, size, regexp_replace, expr, lower\n",
    "\n",
    "print(\"--- Iniciando NLP V16 (Foco: Remover Justificativas) ---\")\n",
    "\n",
    "# 1. Limpeza de Caracteres\n",
    "df_clean_chars = df.withColumn(\"objeto_limpo\", regexp_replace(lower(col(\"objeto_aquisicao\")), r\"[^a-z]\", \" \"))\n",
    "\n",
    "# 2. Stopwords Expandida (Baseada na sua √∫ltima auditoria)\n",
    "stopwords_pt_custom = [\n",
    "    # Artigos/Preposi√ß√µes B√°sicas\n",
    "    \"de\", \"a\", \"o\", \"que\", \"e\", \"do\", \"da\", \"em\", \"um\", \"para\", \"com\", \"nao\", \"uma\", \"os\", \"no\", \n",
    "    \"se\", \"na\", \"por\", \"mais\", \"as\", \"dos\", \"como\", \"mas\", \"ao\", \"ele\", \"das\", \"seu\", \"sua\", \"ou\", \n",
    "    \"quando\", \"muito\", \"nos\", \"ja\", \"eu\", \"tambem\", \"so\", \"pelo\", \"pela\", \"ate\", \"isso\", \"ela\", \n",
    "    \"entre\", \"depois\", \"sem\", \"mesmo\", \"aos\", \"seus\", \"quem\", \"nas\", \"me\", \"esse\", \"eles\", \"voce\", \n",
    "    \"foi\", \"desta\", \"deste\", \"pelas\", \"pelos\", \"nesta\", \"neste\", \"pois\", \"havia\", \"sede\", \"unidades\", \"procuradoria\",\n",
    "    \n",
    "    # JUSTIFICATIVAS (Os vil√µes dos Clusters 15, 17)\n",
    "    \"falta\", \"prestes\", \"acabar\", \"estoque\", \"razao\", \"motivo\", \"devido\", \"vista\", \"haja\",\n",
    "    \"considerando\", \"referente\", \"referida\", \"relativo\", \"conforme\", \"solicitado\", \"atender\", \n",
    "    \"atendimento\", \"necessidade\", \"necessario\", \"necessarios\", \"visando\", \"objeto\", \"visto\",\n",
    "    \"funcionamento\", \"bom\", \"mau\", \"impossibilidade\", \"urgencia\", \"emergencia\", \"carater\",\n",
    "    \n",
    "    # Processos Burocr√°ticos\n",
    "    \"pagamento\", \"aquisicao\", \"compra\", \"fornecimento\", \"servico\", \"servicos\", \"prestacao\",\n",
    "    \"entrega\", \"entregar\", \"empresa\", \"terceirizada\", \"contratada\", \"vulto\", \"monta\", \"despesa\",\n",
    "    \n",
    "    # Termos Gen√©ricos\n",
    "    \"unidade\", \"unid\", \"qtd\", \"quantidade\", \"material\", \"materiais\", \"consumo\", \"permanente\",\n",
    "    \"item\", \"itens\", \"produto\", \"produtos\", \"uso\", \"utilizacao\", \"aplicacao\", \"estoque\",\n",
    "    \"novo\", \"velho\", \"usado\", \"manutencao\", \"reparo\", \"conserto\", \"troca\", \"substituicao\",\n",
    "    \"especie\", \"tipo\", \"modelo\", \"marca\", \"cor\", \"tamanho\", \"oficial\", \"diversos\",\n",
    "    \n",
    "    # Institucional\n",
    "    \"pr\", \"prm\", \"dr\", \"dra\", \"sr\", \"sra\", \"secretaria\", \"departamento\", \"divisao\", \"setor\",\n",
    "    \"gabinete\", \"coordenadoria\", \"administracao\", \"regional\", \"publico\", \"federal\", \"estadual\",\n",
    "    \"municipio\", \"municipal\", \"processo\", \"protocolo\", \"memorando\", \"oficio\", \"despacho\",\n",
    "    \"lei\", \"decreto\", \"artigo\", \"portaria\", \"resolucao\", \"ata\", \"pregao\", \"licitacao\", \"prr\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    tokenizer = Tokenizer(inputCol=\"objeto_limpo\", outputCol=\"words_raw\")\n",
    "    df_tokenized = tokenizer.transform(df_clean_chars)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"words_raw\", outputCol=\"words_temp\")\n",
    "    remover.setStopWords(stopwords_pt_custom)\n",
    "    df_clean_temp = remover.transform(df_tokenized)\n",
    "\n",
    "    # FILTRO SQL (Mantido e refor√ßado)\n",
    "    filter_expression = \"\"\"\n",
    "        filter(words_temp, x -> \n",
    "            x != '' AND \n",
    "            length(x) > 2 AND \n",
    "            NOT (length(x) == 4 AND substring(x, 1, 2) == 'pr') AND\n",
    "            substring(x, 1, 6) != 'necess' AND\n",
    "            substring(x, 1, 6) != 'demand' AND\n",
    "            substring(x, 1, 7) != 'solicit' AND\n",
    "            substring(x, 1, 7) != 'apresen' AND\n",
    "            substring(x, 1, 7) != 'contrat' AND\n",
    "            substring(x, 1, 7) != 'pagamen' AND\n",
    "            substring(x, 1, 7) != 'forneci' AND\n",
    "            substring(x, 1, 5) != 'possu' AND\n",
    "            substring(x, 1, 6) != 'servid' AND\n",
    "            substring(x, 1, 6) != 'defeit' AND\n",
    "            substring(x, 1, 7) != 'disponi' AND\n",
    "            substring(x, 1, 6) != 'inexis' AND\n",
    "            substring(x, 1, 6) != 'inform' AND\n",
    "            substring(x, 1, 5) != 'urgen' AND\n",
    "            substring(x, 1, 5) != 'emerg' AND\n",
    "            substring(x, 1, 7) != 'justifi' AND\n",
    "            substring(x, 1, 5) != 'almox' AND\n",
    "            substring(x, 1, 5) != 'reemb'\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    df_clean_nlp = df_clean_temp.withColumn(\"words_filtered\", expr(filter_expression))\n",
    "    df_final_nlp = df_clean_nlp.filter(size(col(\"words_filtered\")) > 0)\n",
    "\n",
    "    print(\"‚úÖ NLP V16 conclu√≠do.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro NLP: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c20ef8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Filtragem Estat√≠stica (Remo√ß√£o de Palavras Hiper-Frequentes) ---\n",
      "üìä Total Documentos: 12140\n",
      "‚úÇÔ∏è Corte Estat√≠stico (maxDF 0.5): Palavras com mais de 6070 ocorr√™ncias ser√£o banidas.\n",
      "üö´ Palavras Banidas Estatisticamente (0): []\n",
      "‚úÖ Nenhuma palavra violou o teto estat√≠stico de 50%.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA NOVA: Filtragem Estat√≠stica de Frequ√™ncia (Top-Cut)\n",
    "# ==============================================================================\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "print(\"--- Iniciando Filtragem Estat√≠stica (Remo√ß√£o de Palavras Hiper-Frequentes) ---\")\n",
    "\n",
    "# 1. Configura√ß√£o Estat√≠stica\n",
    "# minDF=1.0 -> Ignora (j√° tratamos com minCount depois)\n",
    "# maxDF=0.5 -> \"Se a palavra aparecer em mais de 50% das linhas, √© LIXO (Stopword)\"\n",
    "cv_stat = CountVectorizer(inputCol=\"words_filtered\", outputCol=\"features_temp\", minDF=1.0, maxDF=0.5)\n",
    "\n",
    "# 2. Treina o modelo para ele \"aprender\" a frequ√™ncia das palavras\n",
    "model_cv_stat = cv_stat.fit(df_final_nlp)\n",
    "\n",
    "# 3. O Pulo do Gato: O CountVectorizer descarta as palavras frequentes do vocabul√°rio dele.\n",
    "# Mas n√≥s queremos saber QUAIS FORAM DESCARTADAS para remov√™-las do nosso texto.\n",
    "# Infelizmente, o Spark n√£o d√° a lista de removidos, ent√£o fazemos o inverso:\n",
    "# Vamos identificar as palavras EXTREMAMENTE frequentes manualmente.\n",
    "\n",
    "# Alternativa Pr√°tica e R√°pida via PySpark SQL (Mais robusta que o CountVectorizer para isso):\n",
    "from pyspark.sql.functions import explode, count, col\n",
    "\n",
    "# A. Contamos todas as palavras\n",
    "total_docs = df_final_nlp.count()\n",
    "corte_frequencia = total_docs * 0.5  # 50% da base (Ex: 25.000 ocorr√™ncias)\n",
    "\n",
    "print(f\"üìä Total Documentos: {total_docs}\")\n",
    "print(f\"‚úÇÔ∏è Corte Estat√≠stico (maxDF 0.5): Palavras com mais de {int(corte_frequencia)} ocorr√™ncias ser√£o banidas.\")\n",
    "\n",
    "# B. Calculamos a frequ√™ncia\n",
    "df_freq = df_final_nlp.select(explode(col(\"words_filtered\")).alias(\"word\")) \\\n",
    "                      .groupBy(\"word\").count() \\\n",
    "                      .filter(col(\"count\") > corte_frequencia) # Pega s√≥ quem estourou o teto\n",
    "\n",
    "# C. Coletamos a \"Lista Negra Estat√≠stica\"\n",
    "lista_banidas_stat = [row['word'] for row in df_freq.collect()]\n",
    "\n",
    "print(f\"üö´ Palavras Banidas Estatisticamente ({len(lista_banidas_stat)}): {lista_banidas_stat}\")\n",
    "\n",
    "# 4. Atualizamos o StopWordsRemover com essa nova lista\n",
    "# (Precisamos rodar uma nova limpeza para remover essas palavras do dataset)\n",
    "\n",
    "if len(lista_banidas_stat) > 0:\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "    \n",
    "    # Adicionamos as banidas √† lista customizada que voc√™ j√° tinha\n",
    "    stopwords_atualizadas = stopwords_pt_custom + lista_banidas_stat\n",
    "    \n",
    "    # Removemos novamente\n",
    "    remover_stat = StopWordsRemover(inputCol=\"words_filtered\", outputCol=\"words_final_stat\")\n",
    "    remover_stat.setStopWords(stopwords_atualizadas)\n",
    "    \n",
    "    df_final_stat = remover_stat.transform(df_final_nlp)\n",
    "    \n",
    "    # Atualizamos o dataframe principal para o pr√≥ximo passo\n",
    "    # Renomeamos para manter compatibilidade com o c√≥digo original\n",
    "    df_pronto_w2v = df_final_stat.drop(\"words_filtered\").withColumnRenamed(\"words_final_stat\", \"words_filtered\")\n",
    "    \n",
    "    print(\"‚úÖ Base refinada estatisticamente.\")\n",
    "else:\n",
    "    df_pronto_w2v = df_final_nlp\n",
    "    print(\"‚úÖ Nenhuma palavra violou o teto estat√≠stico de 50%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27958444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vetoriza√ß√£o V10 (minCount=5 para robustez) ---\n",
      "‚úÖ Vetoriza√ß√£o conclu√≠da.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 7 (V10): Vetoriza√ß√£o Word2Vec (minCount=5)\n",
    "# ==============================================================================\n",
    "from pyspark.ml.feature import Word2Vec, Normalizer\n",
    "\n",
    "print(\"\\n--- Vetoriza√ß√£o V10 (minCount=5 para robustez) ---\")\n",
    "\n",
    "try:\n",
    "    word2Vec = Word2Vec(vectorSize=50, \n",
    "                        minCount=15,   # <--- AUMENTAMOS PARA LIMPAR RU√çDO\n",
    "                        inputCol=\"words_filtered\", \n",
    "                        outputCol=\"raw_features\",\n",
    "                        windowSize=2,\n",
    "                        maxIter=20,\n",
    "                        stepSize=0.025,\n",
    "                        seed=42)\n",
    "    \n",
    "    model_w2v = word2Vec.fit(df_final_nlp)\n",
    "    df_w2v = model_w2v.transform(df_final_nlp)\n",
    "    \n",
    "    normalizer = Normalizer(inputCol=\"raw_features\", outputCol=\"features\", p=2.0)\n",
    "    df_tfidf = normalizer.transform(df_w2v) \n",
    "    \n",
    "    df_tfidf.cache()\n",
    "    print(f\"‚úÖ Vetoriza√ß√£o conclu√≠da.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro Word2Vec: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b90abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Auditando a Intelig√™ncia do Modelo Word2Vec ---\n",
      "\n",
      "üîé Palavras mais pr√≥ximas de 'chave':\n",
      "+------+-------------------+\n",
      "|word  |similarity         |\n",
      "+------+-------------------+\n",
      "|chaves|0.6038686037063599 |\n",
      "|copia |0.5934143662452698 |\n",
      "|copias|0.49010199308395386|\n",
      "|yale  |0.48335614800453186|\n",
      "|mapa  |0.4387739300727844 |\n",
      "+------+-------------------+\n",
      "\n",
      "\n",
      "üîé Palavras mais pr√≥ximas de 'torneira':\n",
      "+---------+-------------------+\n",
      "|word     |similarity         |\n",
      "+---------+-------------------+\n",
      "|sifao    |0.5615171790122986 |\n",
      "|vazamento|0.49589988589286804|\n",
      "|pia      |0.49183934926986694|\n",
      "|plastica |0.48842206597328186|\n",
      "|bebedouro|0.4822388291358948 |\n",
      "+---------+-------------------+\n",
      "\n",
      "\n",
      "üîé Palavras mais pr√≥ximas de 'extintor':\n",
      "+--------+-------------------+\n",
      "|word    |similarity         |\n",
      "+--------+-------------------+\n",
      "|painel  |0.5038012862205505 |\n",
      "|dados   |0.48881903290748596|\n",
      "|teste   |0.4452507197856903 |\n",
      "|incendio|0.4422137439250946 |\n",
      "|fixacao |0.4282378852367401 |\n",
      "+--------+-------------------+\n",
      "\n",
      "\n",
      "üîé Palavras mais pr√≥ximas de 'gasolina':\n",
      "+------+-------------------+\n",
      "|word  |similarity         |\n",
      "+------+-------------------+\n",
      "|palio |0.45475950837135315|\n",
      "|lona  |0.4486105144023895 |\n",
      "|diesel|0.4281925559043884 |\n",
      "|oleo  |0.42024022340774536|\n",
      "|comum |0.39719778299331665|\n",
      "+------+-------------------+\n",
      "\n",
      "\n",
      "üîé Palavras mais pr√≥ximas de 'limpeza':\n",
      "+------------+-------------------+\n",
      "|word        |similarity         |\n",
      "+------------+-------------------+\n",
      "|higienizacao|0.44564810395240784|\n",
      "|prmniteroi  |0.44427579641342163|\n",
      "|conservacao |0.4318535625934601 |\n",
      "|lavagem     |0.40970635414123535|\n",
      "|maos        |0.4002189040184021 |\n",
      "+------------+-------------------+\n",
      "\n",
      "\n",
      "üîé Palavras mais pr√≥ximas de 'caneta':\n",
      "   ‚ö†Ô∏è A palavra 'caneta' n√£o foi encontrada no vocabul√°rio (talvez cortada pelo minCount).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DIAGN√ìSTICO W2V: Teste de Similaridade Sem√¢ntica\n",
    "# ==============================================================================\n",
    "print(\"--- Auditando a Intelig√™ncia do Modelo Word2Vec ---\")\n",
    "\n",
    "# Escolha palavras que voc√™ sabe que existem na sua base e representam grupos distintos\n",
    "palavras_teste = [\"chave\", \"torneira\", \"extintor\", \"gasolina\", \"limpeza\", \"caneta\"]\n",
    "\n",
    "try:\n",
    "    for palavra in palavras_teste:\n",
    "        print(f\"\\nüîé Palavras mais pr√≥ximas de '{palavra}':\")\n",
    "        \n",
    "        # O m√©todo findSynonyms busca os vizinhos mais pr√≥ximos no espa√ßo vetorial\n",
    "        # O segundo argumento (5) √© quantas palavras queremos ver\n",
    "        try:\n",
    "            sinonimos = model_w2v.findSynonyms(palavra, 5)\n",
    "            sinonimos.show(truncate=False)\n",
    "        except Exception:\n",
    "            print(f\"   ‚ö†Ô∏è A palavra '{palavra}' n√£o foi encontrada no vocabul√°rio (talvez cortada pelo minCount).\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"‚ùå Erro: A vari√°vel 'model_w2v' n√£o existe. Rode a C√©lula 7 primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9b4b663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aplicando Bisecting K-Means (k=20) ---\n",
      "üìä Registros v√°lidos para clusteriza√ß√£o: 11521\n",
      "‚úÖ Clusteriza√ß√£o k=20 conclu√≠da.\n",
      "\n",
      "--- Distribui√ß√£o dos Clusters ---\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  508|\n",
      "|         1|  357|\n",
      "|         2|  818|\n",
      "|         3|  499|\n",
      "|         4|  781|\n",
      "|         5| 1001|\n",
      "|         6|  548|\n",
      "|         7|  743|\n",
      "|         8|  861|\n",
      "|         9|  767|\n",
      "|        10|  607|\n",
      "|        11|  331|\n",
      "|        12|  355|\n",
      "|        13|  610|\n",
      "|        14|  297|\n",
      "|        15|  723|\n",
      "|        16|  339|\n",
      "|        17|  363|\n",
      "|        18|  569|\n",
      "|        19|  444|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 9 (V4.1): Bisecting K-Means (K=10) - CORRIGIDA\n",
    "# ==============================================================================\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# K REDUZIDO PARA 10 GRUPOS\n",
    "K_FINAL = 20\n",
    "\n",
    "print(f\"\\n--- Aplicando Bisecting K-Means (k={K_FINAL}) ---\")\n",
    "\n",
    "try:\n",
    "    # 1. CRIAMOS UMA UDF PARA CALCULAR A NORMA DO VETOR\n",
    "    # Isso evita o erro de agrega√ß√£o do Summarizer e funciona linha a linha.\n",
    "    @udf(returnType=DoubleType())\n",
    "    def get_vector_norm(v):\n",
    "        try:\n",
    "            # Retorna a norma L2 (magnitude) do vetor denso ou esparso\n",
    "            return float(v.norm(2))\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    # 2. FILTRAGEM SEGURA\n",
    "    # Calculamos a norma e filtramos apenas quem tem tamanho > 0\n",
    "    df_metrics = df_tfidf.withColumn(\"vector_norm\", get_vector_norm(col(\"features\")))\n",
    "    df_input = df_metrics.filter(col(\"vector_norm\") > 0).drop(\"vector_norm\")\n",
    "\n",
    "    total_validos = df_input.count()\n",
    "    print(f\"üìä Registros v√°lidos para clusteriza√ß√£o: {total_validos}\")\n",
    "\n",
    "    # 3. CLUSTERIZA√á√ÉO\n",
    "    bkmeans = BisectingKMeans(featuresCol=\"features\", \n",
    "                              k=K_FINAL, \n",
    "                              seed=1, \n",
    "                              predictionCol=\"prediction\", \n",
    "                              minDivisibleClusterSize=100, \n",
    "                              distanceMeasure=\"cosine\")\n",
    "    \n",
    "    model_final = bkmeans.fit(df_input)\n",
    "    df_clustered = model_final.transform(df_input)\n",
    "    \n",
    "    print(f\"‚úÖ Clusteriza√ß√£o k={K_FINAL} conclu√≠da.\")\n",
    "    \n",
    "    print(\"\\n--- Distribui√ß√£o dos Clusters ---\")\n",
    "    df_clustered.groupBy(\"prediction\").count().orderBy(\"prediction\").show(25)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "229d9d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Auditoria Detalhada dos Clusters (Keywords + Exemplos) ---\n",
      "\n",
      "1. Calculando as palavras mais frequentes de cada grupo...\n",
      "2. Selecionando amostras aleat√≥rias...\n",
      "\n",
      "====================================================================================================\n",
      "                                       RELAT√ìRIO DE CLUSTERS                                        \n",
      "====================================================================================================\n",
      "\n",
      "üìÇ CLUSTER 0\n",
      "üîë PALAVRAS-CHAVE: [CHAVES, COPIAS, CHAVE, COPIA, CONFECCAO, PORTAS, SALA]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     35.00 | placas de sinalizacao chave geral e proibido estacionar e parar       \n",
      " ‚Ä¢ R$      6.00 | copia de chave da sala 1601  tag                                      \n",
      " ‚Ä¢ R$     31.00 | copia de chaves para o arquivo morto do subsolo da prmg               \n",
      " ‚Ä¢ R$     24.00 | copias reprograficas especiais                                        \n",
      " ‚Ä¢ R$      8.00 | copia de chave                                                        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 1\n",
      "üîë PALAVRAS-CHAVE: [PILHAS, SELOG, PILHA, ALCALINA, CONTROLES, BATERIA, AAA]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$      7.80 | visando a manutencao de fechaduras eletronicas da prmnh sera necessari\n",
      " ‚Ä¢ R$    116.35 | nao havia disponibilidade no estoque da selog                         \n",
      " ‚Ä¢ R$     60.00 | necessidade de pilhas para controle remoto de ar condicionado         \n",
      " ‚Ä¢ R$    185.00 | 06 un pilha alcalina palito aaa  \n",
      "01 un pilha alcalina pequena aa     \n",
      " ‚Ä¢ R$     50.00 | pilhas para os controles da garagem da prm chapeco                    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 2\n",
      "üîë PALAVRAS-CHAVE: [PORTA, PORTAO, FECHADURA, ACESSO, PLACA, ELETRONICO, CONTROLE]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     40.46 | porta avisos acrilico parede ref 865                                  \n",
      " ‚Ä¢ R$    214.40 | a aquisicao visa atender a necessidade da divisao de apoio administrat\n",
      " ‚Ä¢ R$    162.67 | compra bobinas para senha recepcao                                    \n",
      " ‚Ä¢ R$    500.00 | servico de concerto do portao de acesso de veiculos do predio sede do \n",
      " ‚Ä¢ R$      7.00 | baterias para controle do portao eletronico da garagem                \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 3\n",
      "üîë PALAVRAS-CHAVE: [LAMPADAS, LED, LAMPADA, QUEIMADAS, REPOSICAO, REATOR, LUMINARIAS]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     12.00 | pes para troca nas fragmentadoras de papelanteriores estavam quebradas\n",
      " ‚Ä¢ R$     83.30 | substituicao das luminarias de emergencia                             \n",
      " ‚Ä¢ R$     20.96 | a necessidade de trocar lampadas queimadas que nao estavam no estoque \n",
      " ‚Ä¢ R$     83.88 | lampadas de led para escadas garagem e mezanino                       \n",
      " ‚Ä¢ R$    308.24 | substituicao de reatores para lampadas fluorescentes danificados      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 4\n",
      "üîë PALAVRAS-CHAVE: [CONDICIONADO, APARELHO, SALA, ELETRICA, SISTEMA, INSTALACAO, CPD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     45.00 | instalacao de controle de acesso biometrico a sala 608                \n",
      " ‚Ä¢ R$      5.00 | para uso em instalacao eletrica que sera feita na secao de logistica d\n",
      " ‚Ä¢ R$    330.00 | aquisicao de palete em madeira onde foram acomodadas as baterias do no\n",
      " ‚Ä¢ R$      3.05 | para servico de colocacao de aparelho split na sala 605 da prrj       \n",
      " ‚Ä¢ R$    140.00 | troca de peca capacitor de aparelho condicionador de ar tipo split    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 5\n",
      "üîë PALAVRAS-CHAVE: [INSTALACAO, SALA, CABO, PREDIO, REDE, CABOS, AUDITORIO]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     98.90 | na transferencia do ponto hdmi do notebook ligado ao projetor na sala \n",
      " ‚Ä¢ R$     12.00 | aquisicao de cabo dc note p4 para camera de video para uso na sede da \n",
      " ‚Ä¢ R$     34.00 | para instalacao de 2 computadores na sala de oitivas p que o procurado\n",
      " ‚Ä¢ R$     20.00 | referente aquisicao de duzentos conectores de torcao cz 0825mm\n",
      "para ma\n",
      " ‚Ä¢ R$     58.05 | aquisicao de chumbadores tipo parabolt necessarios para fixacao de equ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 6\n",
      "üîë PALAVRAS-CHAVE: [CAIXA, SANITARIO, VALVULA, DESCARGA, VASO, ACOPLADA, BANHEIRO]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     26.70 | anel de vedacao para vaso sanitario 3 x 890                           \n",
      " ‚Ä¢ R$     92.23 | mecanismo completo com entrada 300mm e obturador univ valv saida dagua\n",
      " ‚Ä¢ R$     21.04 | justificase a necessidade da aquisicao de um assento sanitario para fi\n",
      " ‚Ä¢ R$     26.47 | 01 assento sanitario cor branca                                       \n",
      " ‚Ä¢ R$     73.90 | valvula de succao para poco material utilizado na cisterna do edificio\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 7\n",
      "üîë PALAVRAS-CHAVE: [BANHEIRO, TORNEIRA, ANDAR, COPA, VAZAMENTO, BANHEIROS, AGUA]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     37.50 | compra de kit vedante sifao sanfonado e tecla para acabamento docol pa\n",
      " ‚Ä¢ R$      6.00 | 01 sifao flexivel                                                     \n",
      " ‚Ä¢ R$      8.00 | aquisicao de desentupidor de vaso para manutencao dos banheiros da pro\n",
      " ‚Ä¢ R$    815.00 | torneira pvc herc para lavatorio 11951 2un x r 9                      \n",
      " ‚Ä¢ R$    131.67 | necessidade de intervencao emergencial da rede de esgoto ecaixa de gor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 8\n",
      "üîë PALAVRAS-CHAVE: [FIXACAO, PARAFUSOS, PINTURA, BLUMENAU, PARAFUSO, REPAROS, ACO]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     19.00 | acabamento reforma atico torre 1                                      \n",
      " ‚Ä¢ R$     48.00 | referente aquisicao de cinquenta buchas n 4 para drywall duzentos para\n",
      " ‚Ä¢ R$     57.00 | substituicao de adesivos danificados                                  \n",
      " ‚Ä¢ R$      6.45 | bucha plastica 10mm 150 x 0043                                        \n",
      " ‚Ä¢ R$    199.00 | rejunte flex 10                                                       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 9\n",
      "üîë PALAVRAS-CHAVE: [FITA, PVC, ADESIVO, TUBO, ROSCA, FACE, DUPLA]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$      3.90 | bucha de reducao longa 50x25 hidraulica                               \n",
      " ‚Ä¢ R$      4.00 | sleek br mod tecla simples                                            \n",
      " ‚Ä¢ R$     32.95 | itens sem estoque ou prestes a acabar necessarios ao atendimento medic\n",
      " ‚Ä¢ R$    143.00 | compra emergencial para enfermagem                                    \n",
      " ‚Ä¢ R$     38.00 | inseticida spray\n",
      "adaptador 2p 10a                                     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 10\n",
      "üîë PALAVRAS-CHAVE: [COVID, PROTECAO, PANDEMIA, PREVENCAO, RETORNO, SEGURANCA, ATIVIDADES]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     59.00 | aquisicao de refletor para ser substituido na lateral da prmcriciumasc\n",
      " ‚Ä¢ R$     24.20 | aquisicao de material indisponivel nesta prrj                         \n",
      " ‚Ä¢ R$    109.00 | folhas de gelatina verdepara colorir a iluminacao da fachada frontal d\n",
      " ‚Ä¢ R$     30.00 | aquisicao de luvas latex para serem utilizadas pelos agentes de segura\n",
      " ‚Ä¢ R$    117.96 | compra urgente de material nao constante no almoxarifado da unidade   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 11\n",
      "üîë PALAVRAS-CHAVE: [ALCOOL, GEL, ARQUIVO, LIMPEZA, COVID, LIQUIDO, ODONTOLOGIA]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     18.98 | aquisicao de papel multiuso para utilizacao nos veiculos da prmcricium\n",
      " ‚Ä¢ R$      9.80 | despesas com locomocao  ciclo de treinamento semestral 2020 osasco sao\n",
      " ‚Ä¢ R$     32.80 | aquisicao de 01 dispenser saboneteira  01 reservatorio para alcool gel\n",
      " ‚Ä¢ R$     99.95 | mascara descartavel tripla com elastico cremer                        \n",
      " ‚Ä¢ R$     29.97 | pulverizador para utilizar alcool liquido                             \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 12\n",
      "üîë PALAVRAS-CHAVE: [ODONTOLOGICO, PAPEL, MEDICO, MEDICAMENTOS, CARIMBO, DIVERSO, POSTO]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     47.40 | material para uso no gabinete odontologico                            \n",
      " ‚Ä¢ R$     52.00 | aquisicao de um pacote com 16 pilhas aaa15 para atender a demanda inte\n",
      " ‚Ä¢ R$    138.14 | compra de medicamentos e materiais em falta ou prestes a acabar necess\n",
      " ‚Ä¢ R$     73.17 | manutencao nas rodas da fragmentadora de papel                        \n",
      " ‚Ä¢ R$    221.54 | medicamentos                                                          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 13\n",
      "üîë PALAVRAS-CHAVE: [PEQUENO, PREDIAL, AGUA, VALOR, MINERAL, DESPESAS, PEQUENA]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     75.00 | atendendo despesas de pequeno vulto por falta de disponibilizacao em e\n",
      " ‚Ä¢ R$     55.00 | compra de material de consumo de pequeno valor para utilizacao na prmm\n",
      " ‚Ä¢ R$     50.00 | aquisicao necessaria de servico cujo valor foi de pequena monta       \n",
      " ‚Ä¢ R$    110.60 | material para manutencao predial                                      \n",
      " ‚Ä¢ R$     32.00 | execucao de nova instalacoes eletricas que serao executadas na prr4 p \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 14\n",
      "üîë PALAVRAS-CHAVE: [GAS, COZINHA, GLP, BOTIJAO, COPA, RECARGA, FOGAO]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$    199.90 | substituicao de fogao a gas por cooktop exigindo a substituicao das pa\n",
      " ‚Ä¢ R$    150.00 | gas glp acondicionado em vasilhame p 13kg                             \n",
      " ‚Ä¢ R$     57.00 | botijao de gas glp 13kg                                               \n",
      " ‚Ä¢ R$     62.00 | aquisicao de 01 botijao de gas na 13kg                                \n",
      " ‚Ä¢ R$     80.00 | aquisicao de gas glp 13kg para consumo na prm                         \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 15\n",
      "üîë PALAVRAS-CHAVE: [CAFE, COPA, AGUA, FILTRO, SUPRIMENTO, ACUCAR, FUNDOS]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$     15.97 | compra de produtos para copa                                          \n",
      " ‚Ä¢ R$    100.00 | reparo de vazamento em um purificador de agua libell acquaflex da prmd\n",
      " ‚Ä¢ R$     34.90 | compra de filtro para cafe por motivo de nao disponibilidade no almoxa\n",
      " ‚Ä¢ R$    269.80 | compra de itens para copa engenharia e gab do pc                      \n",
      " ‚Ä¢ R$     30.85 | caneta esf omm bic cristal                                            \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 16\n",
      "üîë PALAVRAS-CHAVE: [PEDAGIO, DESPESAS, LINS, GENEROS, CARGA, VIAGEM, REF]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$      5.10 | despesas para pagamento de pedagio                                    \n",
      " ‚Ä¢ R$     35.40 | pagamento de pedagio                                                  \n",
      " ‚Ä¢ R$     20.80 | pagamento pedagio                                                     \n",
      " ‚Ä¢ R$     80.00 | locomocao para instalacao de 01 raio x de coluna                      \n",
      " ‚Ä¢ R$      8.50 | pagamento de pedagio                                                  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 17\n",
      "üîë PALAVRAS-CHAVE: [PEDAGIO, VIAGEM, SAO, ITINERANCIA, ISENCAO, PEDAGIOS, DILIGENCIA]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$    233.01 | 62994 litros de gasolina                                              \n",
      " ‚Ä¢ R$    140.00 | aq cartao micro sd 64 gb ultra                                        \n",
      " ‚Ä¢ R$     16.90 | necessidade de atender demandas de manutencao desta prmniteroi conside\n",
      " ‚Ä¢ R$     40.00 | nao havia suprimento de fundos concedido para o servidor jose jodeilso\n",
      " ‚Ä¢ R$    274.90 | material utilizado na obra para inauguracao da prm de sao goncalo     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 18\n",
      "üîë PALAVRAS-CHAVE: [PROCURADOR, REPUBLICA, CARIMBOS, CARIMBO, TRANSPORTE, CONFECCAO, CHEFE]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$      6.90 | justificativa prestada pelo chefe transporte                          \n",
      " ‚Ä¢ R$    279.00 | para degravacao deoitivas de modo a apoiar as atividades finalisticas \n",
      " ‚Ä¢ R$    192.00 | aquisicao 4 kg cafe graos gabinete procurador chefe                   \n",
      " ‚Ä¢ R$     80.00 | 2 borrachas para carimbo                                              \n",
      " ‚Ä¢ R$     90.00 | aquisicao de cartoes de visita a  pedido do procurador regional da rep\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üìÇ CLUSTER 19\n",
      "üîë PALAVRAS-CHAVE: [VEICULO, LIMPEZA, LAVAGEM, ABASTECIMENTO, OLEO, DIESEL, VEICULOS]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " ‚Ä¢ R$    420.00 | locacao de 04 pecas escoramento com 46 m durante 300 dias             \n",
      " ‚Ä¢ R$     12.06 | reconhecimento de firma em documento visando a doacao de veiculo      \n",
      " ‚Ä¢ R$     60.00 | lavagem do veiculo oficial l200 triton                                \n",
      " ‚Ä¢ R$    240.20 | itens adquiridos para utilizacao na viatura oficial da unidade manuten\n",
      " ‚Ä¢ R$     19.70 | compra de oleo para motor 5w30 para veiculo oficial  01 unidade       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA DE AUDITORIA (V2): Top Palavras + Exemplos\n",
    "# ==============================================================================\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, rand, explode, count, desc\n",
    "\n",
    "print(\"--- Auditoria Detalhada dos Clusters (Keywords + Exemplos) ---\\n\")\n",
    "\n",
    "# --- PARTE 1: Descobrir as Top 7 Palavras por Cluster ---\n",
    "print(\"1. Calculando as palavras mais frequentes de cada grupo...\")\n",
    "\n",
    "# 1. Explode: Transforma ['pneu', 'aro'] em duas linhas: 'pneu' e 'aro'\n",
    "df_exploded = df_clustered.withColumn(\"word\", explode(col(\"words_filtered\")))\n",
    "\n",
    "# 2. Conta frequ√™ncia: Quantas vezes a palavra aparece em cada cluster\n",
    "df_word_counts = df_exploded.groupBy(\"prediction\", \"word\").count()\n",
    "\n",
    "# 3. Rankeia: Pega as Top 7\n",
    "w_rank = Window.partitionBy(\"prediction\").orderBy(col(\"count\").desc())\n",
    "df_top_keywords = df_word_counts.withColumn(\"rank\", row_number().over(w_rank)) \\\n",
    "                                .filter(col(\"rank\") <= 7) \\\n",
    "                                .orderBy(\"prediction\", \"rank\")\n",
    "\n",
    "# 4. Traz para a mem√≥ria (Dicion√°rio Python) para exibi√ß√£o r√°pida\n",
    "# Estrutura final: {0: \"pneu, aro, camara...\", 1: \"caneta, lapis...\"}\n",
    "keywords_data = df_top_keywords.collect()\n",
    "keywords_dict = {}\n",
    "\n",
    "for row in keywords_data:\n",
    "    cluster_id = row['prediction']\n",
    "    word = row['word']\n",
    "    if cluster_id not in keywords_dict:\n",
    "        keywords_dict[cluster_id] = []\n",
    "    keywords_dict[cluster_id].append(word)\n",
    "\n",
    "# --- PARTE 2: Pegar 5 Exemplos Aleat√≥rios (C√≥digo anterior) ---\n",
    "print(\"2. Selecionando amostras aleat√≥rias...\")\n",
    "w_sample = Window.partitionBy(\"prediction\").orderBy(rand(seed=42))\n",
    "df_amostra = df_clustered.withColumn(\"rn\", row_number().over(w_sample)) \\\n",
    "                          .filter(col(\"rn\") <= 5) \\\n",
    "                          .select(\"prediction\", \"objeto_aquisicao\", \"valor_transacao\", \"words_filtered\")\n",
    "\n",
    "amostras = df_amostra.collect()\n",
    "amostras_ordenadas = sorted(amostras, key=lambda x: x['prediction'])\n",
    "\n",
    "# --- PARTE 3: Exibi√ß√£o do Relat√≥rio ---\n",
    "from itertools import groupby\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'RELAT√ìRIO DE CLUSTERS':^100}\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "for cluster_id, itens in groupby(amostras_ordenadas, key=lambda x: x['prediction']):\n",
    "    \n",
    "    # Monta a string de palavras-chave\n",
    "    top_words = keywords_dict.get(cluster_id, [\"(Sem palavras suficientes)\"])\n",
    "    top_words_str = \", \".join(top_words).upper()\n",
    "    \n",
    "    print(f\"üìÇ CLUSTER {cluster_id}\")\n",
    "    print(f\"üîë PALAVRAS-CHAVE: [{top_words_str}]\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for item in itens:\n",
    "        # Formata√ß√£o: Pre√ßo alinhado √† direita | Texto original truncado\n",
    "        print(f\" ‚Ä¢ R$ {item['valor_transacao']:>9.2f} | {item['objeto_aquisicao'][:70]:<70}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01611c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Calculando Estat√≠sticas Globais dos Clusters ---\n",
      "\n",
      "--- 2. Resumo de Contamina√ß√£o por Cluster ---\n",
      "+----------+-------------------+------------+-------------+\n",
      "|prediction|total_itens_cluster|qtd_outliers|perc_outliers|\n",
      "+----------+-------------------+------------+-------------+\n",
      "|        16|                339|          43|        12.68|\n",
      "|         9|                767|          93|        12.13|\n",
      "|         0|                508|          54|        10.63|\n",
      "|         8|                861|          91|        10.57|\n",
      "|         2|                818|          78|         9.54|\n",
      "|        13|                610|          55|         9.02|\n",
      "|         6|                548|          49|         8.94|\n",
      "|         5|               1001|          89|         8.89|\n",
      "|         7|                743|          64|         8.61|\n",
      "|        10|                607|          50|         8.24|\n",
      "|        14|                297|          24|         8.08|\n",
      "|        18|                569|          44|         7.73|\n",
      "|        11|                331|          24|         7.25|\n",
      "|         1|                357|          24|         6.72|\n",
      "|         4|                781|          51|         6.53|\n",
      "|        12|                355|          22|          6.2|\n",
      "|         3|                499|          30|         6.01|\n",
      "|        17|                363|          21|         5.79|\n",
      "|        15|                723|          39|         5.39|\n",
      "|        19|                444|          19|         4.28|\n",
      "+----------+-------------------+------------+-------------+\n",
      "\n",
      "--- 3. Extraindo os Top 10 Casos Graves por Cluster ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VSCode\\projetoMineracao\\venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Arquivo 'relatorio_top10_outliers.csv' gerado com sucesso!\n",
      "   Conte√∫do: Os 10 maiores desvios de cada um dos clusters.\n",
      "   Local: c:\\VSCode\\projetoMineracao/relatorio_top10_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 10 (Relat√≥rio Estruturado): Top 10 Outliers + Estat√≠sticas por Cluster\n",
    "# ==============================================================================\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import expr, col, round, count, desc, row_number, lit\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- 1. Calculando Estat√≠sticas Globais dos Clusters ---\")\n",
    "\n",
    "# 1. Totais por Cluster (Quantos itens existem no total?)\n",
    "df_totais = df_clustered.groupBy(\"prediction\").agg(count(\"*\").alias(\"total_itens_cluster\"))\n",
    "\n",
    "# 2. C√°lculo dos Limites IQR (Q1, Mediana, Q3, Teto)\n",
    "df_quartis = df_clustered.groupBy(\"prediction\").agg(\n",
    "    expr(\"percentile_approx(valor_transacao, 0.25)\").alias(\"Q1\"),\n",
    "    expr(\"percentile_approx(valor_transacao, 0.50)\").alias(\"Mediana\"),\n",
    "    expr(\"percentile_approx(valor_transacao, 0.75)\").alias(\"Q3\")\n",
    ")\n",
    "\n",
    "df_limites = df_quartis.withColumn(\"IQR\", col(\"Q3\") - col(\"Q1\")) \\\n",
    "                       .withColumn(\"limite_superior\", col(\"Q3\") + (1.5 * col(\"IQR\")))\n",
    "\n",
    "# 3. Cruzamento com dados originais\n",
    "df_analise = df_clustered.join(df_limites, on=\"prediction\", how=\"inner\") \\\n",
    "                          .join(df_totais, on=\"prediction\", how=\"inner\")\n",
    "\n",
    "# 4. Filtrando Outliers\n",
    "# Regra: Acima do teto E acima de R$ 50,00\n",
    "df_outliers_raw = df_analise.filter((col(\"valor_transacao\") > col(\"limite_superior\")) & \n",
    "                                    (col(\"valor_transacao\") > 50))\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE A: ESTAT√çSTICAS DE PORCENTAGEM (Visualiza√ß√£o no Console)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 2. Resumo de Contamina√ß√£o por Cluster ---\")\n",
    "\n",
    "# Conta quantos outliers tem em cada cluster\n",
    "df_stats_outliers = df_outliers_raw.groupBy(\"prediction\", \"total_itens_cluster\").agg(count(\"*\").alias(\"qtd_outliers\"))\n",
    "\n",
    "# Calcula a porcentagem\n",
    "df_resumo = df_stats_outliers.withColumn(\"perc_outliers\", round((col(\"qtd_outliers\") / col(\"total_itens_cluster\")) * 100, 2)) \\\n",
    "                             .orderBy(desc(\"perc_outliers\"))\n",
    "\n",
    "df_resumo.show(20)\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE B: TOP 10 MAIS DISCREPANTES POR CLUSTER (Arquivo CSV)\n",
    "# ==============================================================================\n",
    "print(\"--- 3. Extraindo os Top 10 Casos Graves por Cluster ---\")\n",
    "\n",
    "# Definimos uma \"Janela\" por cluster, ordenando pelo valor mais alto (mais grave)\n",
    "janela_cluster = Window.partitionBy(\"prediction\").orderBy(col(\"valor_transacao\").desc())\n",
    "\n",
    "# Criamos um Ranking (1¬∫, 2¬∫, 3¬∫...) e filtramos s√≥ at√© o 10¬∫\n",
    "df_top10 = df_outliers_raw.withColumn(\"rank\", row_number().over(janela_cluster)) \\\n",
    "                          .filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Calculamos m√©tricas finais para o relat√≥rio\n",
    "df_export = df_top10.withColumn(\n",
    "    \"diferenca_valor\", \n",
    "    col(\"valor_transacao\") - col(\"Mediana\")\n",
    ").withColumn(\n",
    "    \"x_vezes_mediana\", \n",
    "    round(col(\"valor_transacao\") / col(\"Mediana\"), 1)\n",
    ").select(\n",
    "    col(\"prediction\").alias(\"Cluster\"),\n",
    "    col(\"rank\").alias(\"Ranking_Gravidade\"),\n",
    "    col(\"objeto_aquisicao\").alias(\"Descricao_Item\"),\n",
    "    col(\"valor_transacao\").alias(\"Preco_Pago\"),\n",
    "    col(\"Mediana\").alias(\"Preco_Medio_Cluster\"),\n",
    "    col(\"limite_superior\").alias(\"Teto_Aceitavel\"),\n",
    "    col(\"x_vezes_mediana\").alias(\"Quantas_Vezes_Mais_Caro\"),\n",
    "    col(\"total_itens_cluster\").alias(\"Tamanho_Cluster\")\n",
    ").orderBy(\"Cluster\", \"Ranking_Gravidade\")\n",
    "\n",
    "# --- Exporta√ß√£o ---\n",
    "try:\n",
    "    pdf_top10 = df_export.toPandas()\n",
    "    nome_arquivo = \"relatorio_top10_outliers.csv\"\n",
    "    \n",
    "    # Salva com encoding correto para Excel/PT-BR\n",
    "    pdf_top10.to_csv(nome_arquivo, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Arquivo '{nome_arquivo}' gerado com sucesso!\")\n",
    "    print(f\"   Conte√∫do: Os 10 maiores desvios de cada um dos clusters.\")\n",
    "    print(f\"   Local: {os.getcwd()}/{nome_arquivo}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao exportar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eb55aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Auditoria Preditiva (Random Forest) ---\n",
      "Par√¢metros: Trees=50 | Depth=8 | MinInstances=5 | Target=Log(Preco)\n",
      "‚è≥ Treinando o modelo (analisando padr√µes globais)...\n",
      "üö© O modelo encontrou 1559 transa√ß√µes suspeitas.\n",
      "\n",
      "--- Gerando Arquivo de Auditoria ML ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VSCode\\projetoMineracao\\venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Arquivo gerado: 'auditoria_ml_random_forest.csv'\n",
      "\n",
      "--- Top 5 Discrep√¢ncias (Vis√£o do Rob√¥) ---\n",
      "                                      Descricao_Item  Preco_Pago_Real  \\\n",
      "0  confeccao de resinas para carimbos material ut...     2.016800e+09   \n",
      "1                                      anel superior     1.015300e+09   \n",
      "2           aquisicao de de dois numeros em aco inox     2.116025e+07   \n",
      "3                       valvula reversora para split     7.300000e+05   \n",
      "4     5o laminas em digital e 100 laminas em digital     2.102970e+05   \n",
      "\n",
      "   Preco_Justo_Estimado  Quantas_Vezes_Mais_Caro  \n",
      "0                669.77                3011120.7  \n",
      "1                426.27                2381773.2  \n",
      "2               1144.31                  18491.6  \n",
      "3                202.53                   3604.3  \n",
      "4                136.18                   1544.1  \n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 11 (CORRIGIDA): Auditoria ML (Random Forest Regressor)\n",
    "# ==============================================================================\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import col, log, exp, abs as spark_abs, round, desc\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Iniciando Auditoria Preditiva (Random Forest) ---\")\n",
    "print(f\"Par√¢metros: Trees=50 | Depth=8 | MinInstances=5 | Target=Log(Preco)\")\n",
    "\n",
    "# 1. PREPARA√á√ÉO (CORRE√á√ÉO AQUI: Usamos df_clustered em vez de df_tfidf)\n",
    "# Assim garantimos que a coluna 'prediction' (Cluster ID) exista para o relat√≥rio final.\n",
    "df_ml = df_clustered.withColumn(\"label\", log(col(\"valor_transacao\") + 1.0))\n",
    "\n",
    "# 2. TREINAMENTO DO MODELO\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction_log\", # Nome exclusivo para n√£o conflitar com o cluster\n",
    "    seed=42,\n",
    "    numTrees=50,\n",
    "    maxDepth=8,\n",
    "    minInstancesPerNode=5\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Treinando o modelo (analisando padr√µes globais)...\")\n",
    "model_rf = rf.fit(df_ml)\n",
    "\n",
    "# 3. PREDI√á√ÉO\n",
    "predictions = model_rf.transform(df_ml)\n",
    "\n",
    "# 4. C√ÅLCULO DE DISCREP√ÇNCIA\n",
    "df_analise_ml = predictions.withColumn(\"preco_estimado_ml\", exp(col(\"prediction_log\")) - 1.0) \\\n",
    "                           .withColumn(\"razao_sobrepreco\", col(\"valor_transacao\") / (col(\"preco_estimado_ml\") + 0.01)) \\\n",
    "                           .withColumn(\"diferenca_valor\", col(\"valor_transacao\") - col(\"preco_estimado_ml\"))\n",
    "\n",
    "# 5. FILTRAGEM\n",
    "# Regra: Pre√ßo pago > 3x o estimado E Diferen√ßa > R$ 50\n",
    "df_suspeitas_ml = df_analise_ml.filter((col(\"razao_sobrepreco\") > 3) & \n",
    "                                       (col(\"diferenca_valor\") > 50))\n",
    "\n",
    "# Sele√ß√£o de colunas (Agora 'prediction' vai funcionar pois veio do df_clustered)\n",
    "df_export_ml = df_suspeitas_ml.select(\n",
    "    col(\"prediction\").alias(\"Cluster_Original\"), # <-- Agora esta coluna existe!\n",
    "    col(\"objeto_aquisicao\").alias(\"Descricao_Item\"),\n",
    "    col(\"valor_transacao\").alias(\"Preco_Pago_Real\"),\n",
    "    round(col(\"preco_estimado_ml\"), 2).alias(\"Preco_Justo_Estimado\"),\n",
    "    round(col(\"razao_sobrepreco\"), 1).alias(\"Quantas_Vezes_Mais_Caro\"),\n",
    "    col(\"ano\"),\n",
    "    col(\"unidade_gestora\")\n",
    ").orderBy(desc(\"razao_sobrepreco\"))\n",
    "\n",
    "total_suspeitas = df_export_ml.count()\n",
    "print(f\"üö© O modelo encontrou {total_suspeitas} transa√ß√µes suspeitas.\")\n",
    "\n",
    "# --- 6. EXPORTA√á√ÉO ---\n",
    "print(\"\\n--- Gerando Arquivo de Auditoria ML ---\")\n",
    "\n",
    "try:\n",
    "    pdf_ml = df_export_ml.toPandas()\n",
    "    nome_arquivo_ml = \"auditoria_ml_random_forest.csv\"\n",
    "    \n",
    "    pdf_ml.to_csv(nome_arquivo_ml, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "    \n",
    "    print(f\"‚úÖ Arquivo gerado: '{nome_arquivo_ml}'\")\n",
    "    \n",
    "    print(\"\\n--- Top 5 Discrep√¢ncias (Vis√£o do Rob√¥) ---\")\n",
    "    print(pdf_ml.head(5)[[\"Descricao_Item\", \"Preco_Pago_Real\", \"Preco_Justo_Estimado\", \"Quantas_Vezes_Mais_Caro\"]])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na exporta√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40ba8fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Cruzamento de Auditorias (Filtro: Score > 10) ---\n",
      "üìÇ Entradas: IQR (1040 linhas) | ML (1559 linhas)\n",
      "üö© Registros Cr√≠ticos (Score > 10): 162\n",
      "\n",
      "‚úÖ Relat√≥rio Cr√≠tico Gerado: 'auditoria_critica_score_10.csv'\n",
      "   Local: c:\\VSCode\\projetoMineracao/auditoria_critica_score_10.csv\n",
      "\n",
      "--- TOP 10 CASOS MAIS GRAVES (ALERTA VERMELHO) ---\n",
      "                                                                                                                                             Descricao_Item   Preco_Pago  Score_Risco  Quantas_Vezes_Mais_Caro\n",
      "confeccao de resinas para carimbos material utilizado para atender as necessidades do protocolo obs a nota fiscal foi preenchida sem a data de emissao logo 2016800184.0 8891003.6184                3011120.7\n",
      "                                                                                                                                              anel superior 1015300400.0 7338083.6474                2381773.2\n",
      "                                                                                                                   aquisicao de de dois numeros em aco inox   21160252.0  134896.4312                  18491.6\n",
      "                                                                                                                               valvula reversora para split     730000.0    5815.4212                   3604.3\n",
      "                                                                                                             5o laminas em digital e 100 laminas em digital     210297.0    2069.5664                   1544.1\n",
      "                                                                                                                                  joelho de pvc para esgoto      90100.0    1615.2834                   1130.7\n",
      "                                                                                                                               360 tubo soldado 50 mm krona      49050.0    1260.4492                    997.1\n",
      "                                                                                                                            anel de borracha cano de esgoto      10022.0     205.8236                    157.9\n",
      "                                                                                                                                     abracadeira de nylon 4       8400.0     180.3708                    136.1\n",
      "                                                                                                                    compra de etiqueta inkjetlaser carta 33       9101.0      98.8335                     73.3\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA DE CONSOLIDA√á√ÉO (V2): Cruzamento IQR x ML (Filtro Risco > 10)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Iniciando Cruzamento de Auditorias (Filtro: Score > 10) ---\")\n",
    "\n",
    "# Defini√ß√£o dos arquivos de entrada\n",
    "file_iqr = \"auditoria_outliers_iqr.csv\"\n",
    "file_ml = \"auditoria_ml_random_forest.csv\"\n",
    "\n",
    "# Verifica√ß√£o de exist√™ncia\n",
    "if not os.path.exists(file_iqr) or not os.path.exists(file_ml):\n",
    "    print(\"‚ùå Erro: Um dos arquivos de entrada n√£o foi encontrado.\")\n",
    "    print(\"   Certifique-se de ter rodado a C√©lula 10 (IQR) e a C√©lula 11 (ML).\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Carregar os Relat√≥rios\n",
    "        # Usamos float_precision='high' para garantir precis√£o nos valores monet√°rios\n",
    "        df_iqr = pd.read_csv(file_iqr, sep=';', encoding='utf-8-sig')\n",
    "        df_ml = pd.read_csv(file_ml, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "        print(f\"üìÇ Entradas: IQR ({len(df_iqr)} linhas) | ML ({len(df_ml)} linhas)\")\n",
    "\n",
    "        # 2. Padroniza√ß√£o\n",
    "        df_ml.rename(columns={'Preco_Pago_Real': 'Preco_Pago'}, inplace=True)\n",
    "\n",
    "        # 3. O Cruzamento (Interse√ß√£o)\n",
    "        # Identificamos os itens que aparecem nos DOIS relat√≥rios\n",
    "        df_consenso = pd.merge(\n",
    "            df_iqr, \n",
    "            df_ml, \n",
    "            on=['Descricao_Item', 'Preco_Pago', 'ano', 'unidade_gestora'],\n",
    "            how='inner',\n",
    "            suffixes=('_IQR', '_ML')\n",
    "        )\n",
    "        \n",
    "        # 4. C√°lculo do Score de Risco Unificado\n",
    "        # F√≥rmula: (Vezes mais caro ML) + (Excesso IQR / 100)\n",
    "        # Ex: Se ML diz que √© 10x mais caro e IQR diz que estourou 500% o teto -> Score = 10 + 5 = 15\n",
    "        df_consenso['Score_Risco'] = df_consenso['Quantas_Vezes_Mais_Caro'] + (df_consenso['Perc_Excesso'] / 100)\n",
    "        \n",
    "        # 5. FILTRAGEM AGRESSIVA (Score > 10)\n",
    "        # S√≥ passamos para o arquivo final se o risco for alt√≠ssimo\n",
    "        df_final = df_consenso[df_consenso['Score_Risco'] > 10].copy()\n",
    "        \n",
    "        # Organiza√ß√£o das Colunas\n",
    "        colunas_finais = [\n",
    "            'Cluster_ID',              \n",
    "            'Descricao_Item',          \n",
    "            'Preco_Pago',              \n",
    "            'Score_Risco',             \n",
    "            'Preco_Justo_Estimado',    # Vis√£o ML\n",
    "            'Teto_Estatistico',        # Vis√£o Estat√≠stica\n",
    "            'Quantas_Vezes_Mais_Caro', # Indicador ML\n",
    "            'Perc_Excesso',            # Indicador IQR\n",
    "            'ano',\n",
    "            'unidade_gestora'\n",
    "        ]\n",
    "        \n",
    "        # Ordena: O maior risco no topo\n",
    "        df_final = df_final[colunas_finais].sort_values(by='Score_Risco', ascending=False)\n",
    "        \n",
    "        qtd_total = len(df_final)\n",
    "        print(f\"üö© Registros Cr√≠ticos (Score > 10): {qtd_total}\")\n",
    "\n",
    "        # 6. Exporta√ß√£o\n",
    "        nome_arquivo_final = \"auditoria_critica_score_10.csv\"\n",
    "        \n",
    "        if qtd_total > 0:\n",
    "            df_final.to_csv(nome_arquivo_final, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "            print(f\"\\n‚úÖ Relat√≥rio Cr√≠tico Gerado: '{nome_arquivo_final}'\")\n",
    "            print(f\"   Local: {os.getcwd()}/{nome_arquivo_final}\")\n",
    "            \n",
    "            print(\"\\n--- TOP 10 CASOS MAIS GRAVES (ALERTA VERMELHO) ---\")\n",
    "            display_cols = ['Descricao_Item', 'Preco_Pago', 'Score_Risco', 'Quantas_Vezes_Mais_Caro']\n",
    "            # Formata√ß√£o para leitura no console\n",
    "            print(df_final[display_cols].head(10).to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Nenhum registro superou o Score de Risco > 10.\")\n",
    "            print(\"   Isso significa que, embora existam outliers, nenhum √© t√£o extremo a ponto de cruzar os dois m√©todos com essa intensidade.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante o processamento: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e5a2007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Enriquecimento dos Dados (Recuperando CPFs e Favorecidos) ---\n",
      "üìÇ Carregados 162 registros cr√≠ticos para enriquecimento.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VSCode\\projetoMineracao\\venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "c:\\VSCode\\projetoMineracao\\venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Relat√≥rio Completo Gerado: 'AUDITORIA_COMPLETA_RASTREAVEL.csv'\n",
      "   Conte√∫do: 101 linhas com dados cadastrais completos.\n",
      "   Local: c:\\VSCode\\projetoMineracao/AUDITORIA_COMPLETA_RASTREAVEL.csv\n",
      "\n",
      "--- Exemplo de Registro Completo (Top 1) ---\n",
      "                                                                        0\n",
      "Score_Risco                                                         63.31\n",
      "Fator_Sobrepreco                                                     40.6\n",
      "Perc_Acima_Teto_Cluster                                           2271.33\n",
      "Preco_Ref_ML                                                       108.42\n",
      "Preco_Ref_Cluster                                                  185.55\n",
      "Descricao_Original       compra emergencial para enfermagem mascara caixa\n",
      "Valor_Pago                                                         4400.0\n",
      "data_aquisicao                                                 17/09/2020\n",
      "ano                                                                  2020\n",
      "unidade_gestora                                                       pgr\n",
      "Cluster_ID                                                              9\n",
      "Nome_Suprido                                          antonia rosa vieira\n",
      "CPF_Suprido                                                   31369502168\n",
      "Nome_Favorecido              emedf comercio de produtos odontologicos epp\n",
      "CNPJ_Favorecido                                            01686183000103\n",
      "periodo_aplicacao                                 10/09/2020 a 13/11/2020\n",
      "aprovado                                                              sim\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA FINAL (CORRIGIDA): Enriquecimento (Join com Dados Originais)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- Iniciando Enriquecimento dos Dados (Recuperando CPFs e Favorecidos) ---\")\n",
    "\n",
    "# Arquivo de entrada\n",
    "file_critico = \"auditoria_critica_score_10.csv\"\n",
    "\n",
    "if not os.path.exists(file_critico):\n",
    "    print(f\"‚ùå Erro: O arquivo '{file_critico}' n√£o foi encontrado.\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Carregar as Anomalias (Pandas)\n",
    "        pdf_criticos = pd.read_csv(file_critico, sep=';', encoding='utf-8-sig')\n",
    "        qtd_criticos = len(pdf_criticos)\n",
    "        print(f\"üìÇ Carregados {qtd_criticos} registros cr√≠ticos para enriquecimento.\")\n",
    "\n",
    "        if qtd_criticos > 0:\n",
    "            # 2. Sele√ß√£o e Renomea√ß√£o (CORRIGIDO)\n",
    "            \n",
    "            # Lista das colunas que existem no CSV\n",
    "            cols_csv = ['Descricao_Item', 'Preco_Pago', 'ano', 'unidade_gestora', \n",
    "                        'Score_Risco', 'Preco_Justo_Estimado', 'Teto_Estatistico', \n",
    "                        'Quantas_Vezes_Mais_Caro', 'Perc_Excesso']\n",
    "            \n",
    "            # Filtramos o PDF apenas com essas colunas\n",
    "            pdf_filtrado = pdf_criticos[cols_csv].copy()\n",
    "            \n",
    "            # Agora renomeamos para bater com os nomes do Spark (df_clustered)\n",
    "            pdf_renomeado = pdf_filtrado.rename(columns={\n",
    "                'Descricao_Item': 'objeto_aquisicao', \n",
    "                'Preco_Pago': 'valor_transacao'\n",
    "            })\n",
    "            \n",
    "            # 3. Cria√ß√£o do DataFrame Spark (Chaves para o Join)\n",
    "            # O Spark vai receber 'objeto_aquisicao' e 'valor_transacao' corretamente agora\n",
    "            df_keys = spark.createDataFrame(pdf_renomeado)\n",
    "\n",
    "            # 4. O JOIN (Recuperando os dados originais)\n",
    "            # Fazemos o join usando as colunas renomeadas\n",
    "            df_completo = df_clustered.join(\n",
    "                df_keys,\n",
    "                on=['objeto_aquisicao', 'valor_transacao', 'ano', 'unidade_gestora'],\n",
    "                how='inner'\n",
    "            )\n",
    "\n",
    "            # 5. Sele√ß√£o Final para o Relat√≥rio (Organiza√ß√£o)\n",
    "            df_relatorio_final = df_completo.select(\n",
    "                # M√©tricas de Auditoria\n",
    "                col(\"Score_Risco\"),\n",
    "                col(\"Quantas_Vezes_Mais_Caro\").alias(\"Fator_Sobrepreco\"),\n",
    "                col(\"Perc_Excesso\").alias(\"Perc_Acima_Teto_Cluster\"),\n",
    "                col(\"Preco_Justo_Estimado\").alias(\"Preco_Ref_ML\"),\n",
    "                col(\"Teto_Estatistico\").alias(\"Preco_Ref_Cluster\"),\n",
    "                \n",
    "                # Dados da Compra\n",
    "                col(\"objeto_aquisicao\").alias(\"Descricao_Original\"),\n",
    "                col(\"valor_transacao\").alias(\"Valor_Pago\"),\n",
    "                col(\"data_aquisicao\"),\n",
    "                col(\"ano\"),\n",
    "                col(\"unidade_gestora\"),\n",
    "                col(\"prediction\").alias(\"Cluster_ID\"),\n",
    "\n",
    "                # Dados Cadastrais (Ouro)\n",
    "                col(\"nome_suprido\").alias(\"Nome_Suprido\"),\n",
    "                col(\"cpf_suprido\").alias(\"CPF_Suprido\"),\n",
    "                col(\"nome_favorecido\").alias(\"Nome_Favorecido\"),\n",
    "                col(\"cpf_cnpj_favorecido\").alias(\"CNPJ_Favorecido\"),\n",
    "                \n",
    "                col(\"periodo_aplicacao\"),\n",
    "                col(\"aprovado\")\n",
    "            ).orderBy(col(\"Score_Risco\").desc())\n",
    "\n",
    "            # 6. Exporta√ß√£o Final\n",
    "            nome_arquivo_rastreio = \"AUDITORIA_COMPLETA_RASTREAVEL.csv\"\n",
    "            \n",
    "            pdf_final = df_relatorio_final.toPandas()\n",
    "            pdf_final.to_csv(nome_arquivo_rastreio, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "\n",
    "            print(f\"\\n‚úÖ Relat√≥rio Completo Gerado: '{nome_arquivo_rastreio}'\")\n",
    "            print(f\"   Conte√∫do: {len(pdf_final)} linhas com dados cadastrais completos.\")\n",
    "            print(f\"   Local: {os.getcwd()}/{nome_arquivo_rastreio}\")\n",
    "            \n",
    "            print(\"\\n--- Exemplo de Registro Completo (Top 1) ---\")\n",
    "            print(pdf_final.head(1).T)\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è N√£o h√° registros cr√≠ticos para enriquecer.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no enriquecimento: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39bfe624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raio-X das Palavras Mais Frequentes ---\n",
      "Total de Documentos: 12140\n",
      "+------------+----------+------------------+\n",
      "|        word|frequencia|       porcentagem|\n",
      "+------------+----------+------------------+\n",
      "|  instalacao|       417|3.4349258649093906|\n",
      "|        sala|       389|3.2042833607907744|\n",
      "|     pedagio|       362| 2.981878088962109|\n",
      "|        agua|       357| 2.940691927512356|\n",
      "|       porta|       333| 2.742998352553542|\n",
      "|      predio|       325|2.6771004942339376|\n",
      "|        copa|       322| 2.652388797364086|\n",
      "|    lampadas|       280|2.3064250411861615|\n",
      "|      chaves|       256|2.1087314662273475|\n",
      "|      pilhas|       251|2.0675453047775947|\n",
      "|   seguranca|       250| 2.059308072487644|\n",
      "|        fita|       246| 2.026359143327842|\n",
      "|         gas|       245| 2.018121911037891|\n",
      "|      portao|       242|1.9934102141680394|\n",
      "|     limpeza|       239| 1.968698517298188|\n",
      "|       caixa|       234| 1.927512355848435|\n",
      "|    banheiro|       231| 1.902800658978583|\n",
      "|condicionado|       220|1.8121911037891267|\n",
      "|       andar|       219| 1.803953871499176|\n",
      "|   confeccao|       206|1.6968698517298189|\n",
      "+------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA DE DIAGN√ìSTICO: Top 20 Palavras Mais Frequentes (%)\n",
    "# ==============================================================================\n",
    "from pyspark.sql.functions import explode, count, desc, col, lit\n",
    "\n",
    "print(\"--- Raio-X das Palavras Mais Frequentes ---\")\n",
    "\n",
    "# 1. Total de Documentos\n",
    "total_docs = df_final_nlp.count()\n",
    "\n",
    "# 2. Contagem de Palavras\n",
    "df_top_words = df_final_nlp.select(explode(col(\"words_filtered\")).alias(\"word\")) \\\n",
    "                           .groupBy(\"word\") \\\n",
    "                           .agg(count(\"*\").alias(\"frequencia\")) \\\n",
    "                           .withColumn(\"porcentagem\", (col(\"frequencia\") / lit(total_docs)) * 100) \\\n",
    "                           .orderBy(desc(\"frequencia\"))\n",
    "\n",
    "# 3. Mostra o Top 20\n",
    "print(f\"Total de Documentos: {total_docs}\")\n",
    "df_top_words.show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
