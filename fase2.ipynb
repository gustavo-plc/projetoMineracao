{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c21c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FASE 2: An√°lise Explorat√≥ria de Dados (EDA) e Minera√ß√£o (Completo)\n",
    "# Arquivo: analise_fase2.py\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Importa√ß√µes Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, size, lower, avg, stddev, abs as _abs, round as _round, max as _max, min as _min, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# --- 1. Configura√ß√£o de Ambiente (Windows) ---\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "print(\"--- Iniciando Fase 2: An√°lise Explorat√≥ria ---\")\n",
    "\n",
    "# --- 2. Inicializando Sess√£o Spark ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analise_Gastos_Fase2\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Otimiza√ß√£o Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# --- 3. Carregamento dos Dados ---\n",
    "BASE_DIR = os.path.join(os.getcwd(), \"dados\")\n",
    "input_path = os.path.join(BASE_DIR, \"Consolidado_Final\")\n",
    "\n",
    "print(f\"üìÇ Buscando base consolidada em: {input_path}\")\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"‚ùå ARQUIVO N√ÉO ENCONTRADO: {input_path}\")\n",
    "    sys.exit() # Encerra se n√£o achar o arquivo\n",
    "\n",
    "try:\n",
    "    df = spark.read.parquet(input_path)\n",
    "    df.cache() # Cache do dataset bruto\n",
    "    print(f\"‚úÖ Base carregada: {df.count()} registros.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro leitura: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CORRE√á√ÉO CR√çTICA: Remo√ß√£o de Duplicatas\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Saneamento da Base ---\")\n",
    "print(f\"Total Bruto: {df.count()}\")\n",
    "\n",
    "# Remove linhas onde Objeto, Valor e Favorecido s√£o id√™nticos\n",
    "# Isso elimina as repeti√ß√µes causadas pela fus√£o de c√©lulas no Excel\n",
    "df = df.dropDuplicates(['objeto_aquisicao', 'valor_transacao', 'nome_favorecido'])\n",
    "\n",
    "# For√ßa o rec√°lculo e cache na mem√≥ria\n",
    "df.cache()\n",
    "count_real = df.count()\n",
    "\n",
    "print(f\"‚úÖ Total Real (√önicos): {count_real}\")\n",
    "print(f\"üóëÔ∏è Lixo Removido: {54196 - count_real}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 6 (V16 - LIMPEZA DE JUSTIFICATIVAS): NLP Refinado\n",
    "# ==============================================================================\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, size, regexp_replace, expr, lower\n",
    "\n",
    "print(\"--- Iniciando NLP V16 (Foco: Remover Justificativas) ---\")\n",
    "\n",
    "# 1. Limpeza de Caracteres\n",
    "df_clean_chars = df.withColumn(\"objeto_limpo\", regexp_replace(lower(col(\"objeto_aquisicao\")), r\"[^a-z]\", \" \"))\n",
    "\n",
    "# 2. Stopwords Expandida (Baseada na sua √∫ltima auditoria)\n",
    "stopwords_pt_custom = [\n",
    "    # Artigos/Preposi√ß√µes B√°sicas\n",
    "    \"de\", \"a\", \"o\", \"que\", \"e\", \"do\", \"da\", \"em\", \"um\", \"para\", \"com\", \"nao\", \"uma\", \"os\", \"no\", \n",
    "    \"se\", \"na\", \"por\", \"mais\", \"as\", \"dos\", \"como\", \"mas\", \"ao\", \"ele\", \"das\", \"seu\", \"sua\", \"ou\", \n",
    "    \"quando\", \"muito\", \"nos\", \"ja\", \"eu\", \"tambem\", \"so\", \"pelo\", \"pela\", \"ate\", \"isso\", \"ela\", \n",
    "    \"entre\", \"depois\", \"sem\", \"mesmo\", \"aos\", \"seus\", \"quem\", \"nas\", \"me\", \"esse\", \"eles\", \"voce\", \n",
    "    \"foi\", \"desta\", \"deste\", \"pelas\", \"pelos\", \"nesta\", \"neste\", \"pois\", \"havia\",\n",
    "    \n",
    "    # JUSTIFICATIVAS (Os vil√µes dos Clusters 15, 17)\n",
    "    \"falta\", \"prestes\", \"acabar\", \"estoque\", \"razao\", \"motivo\", \"devido\", \"vista\", \"haja\",\n",
    "    \"considerando\", \"referente\", \"referida\", \"relativo\", \"conforme\", \"solicitado\", \"atender\", \n",
    "    \"atendimento\", \"necessidade\", \"necessario\", \"necessarios\", \"visando\", \"objeto\", \"visto\",\n",
    "    \"funcionamento\", \"bom\", \"mau\", \"impossibilidade\", \"urgencia\", \"emergencia\", \"carater\",\n",
    "    \n",
    "    # Processos Burocr√°ticos\n",
    "    \"pagamento\", \"aquisicao\", \"compra\", \"fornecimento\", \"servico\", \"servicos\", \"prestacao\",\n",
    "    \"entrega\", \"entregar\", \"empresa\", \"terceirizada\", \"contratada\", \"vulto\", \"monta\", \"despesa\",\n",
    "    \n",
    "    # Termos Gen√©ricos\n",
    "    \"unidade\", \"unid\", \"qtd\", \"quantidade\", \"material\", \"materiais\", \"consumo\", \"permanente\",\n",
    "    \"item\", \"itens\", \"produto\", \"produtos\", \"uso\", \"utilizacao\", \"aplicacao\", \"estoque\",\n",
    "    \"novo\", \"velho\", \"usado\", \"manutencao\", \"reparo\", \"conserto\", \"troca\", \"substituicao\",\n",
    "    \"especie\", \"tipo\", \"modelo\", \"marca\", \"cor\", \"tamanho\", \"oficial\", \"diversos\",\n",
    "    \n",
    "    # Institucional\n",
    "    \"pr\", \"prm\", \"dr\", \"dra\", \"sr\", \"sra\", \"secretaria\", \"departamento\", \"divisao\", \"setor\",\n",
    "    \"gabinete\", \"coordenadoria\", \"administracao\", \"regional\", \"publico\", \"federal\", \"estadual\",\n",
    "    \"municipio\", \"municipal\", \"processo\", \"protocolo\", \"memorando\", \"oficio\", \"despacho\",\n",
    "    \"lei\", \"decreto\", \"artigo\", \"portaria\", \"resolucao\", \"ata\", \"pregao\", \"licitacao\", \"prr\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    tokenizer = Tokenizer(inputCol=\"objeto_limpo\", outputCol=\"words_raw\")\n",
    "    df_tokenized = tokenizer.transform(df_clean_chars)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"words_raw\", outputCol=\"words_temp\")\n",
    "    remover.setStopWords(stopwords_pt_custom)\n",
    "    df_clean_temp = remover.transform(df_tokenized)\n",
    "\n",
    "    # FILTRO SQL (Mantido e refor√ßado)\n",
    "    filter_expression = \"\"\"\n",
    "        filter(words_temp, x -> \n",
    "            x != '' AND \n",
    "            length(x) > 2 AND \n",
    "            NOT (length(x) == 4 AND substring(x, 1, 2) == 'pr') AND\n",
    "            substring(x, 1, 6) != 'necess' AND\n",
    "            substring(x, 1, 6) != 'demand' AND\n",
    "            substring(x, 1, 7) != 'solicit' AND\n",
    "            substring(x, 1, 7) != 'apresen' AND\n",
    "            substring(x, 1, 7) != 'contrat' AND\n",
    "            substring(x, 1, 7) != 'pagamen' AND\n",
    "            substring(x, 1, 7) != 'forneci' AND\n",
    "            substring(x, 1, 5) != 'possu' AND\n",
    "            substring(x, 1, 6) != 'servid' AND\n",
    "            substring(x, 1, 6) != 'defeit' AND\n",
    "            substring(x, 1, 7) != 'disponi' AND\n",
    "            substring(x, 1, 6) != 'inexis' AND\n",
    "            substring(x, 1, 6) != 'inform' AND\n",
    "            substring(x, 1, 5) != 'urgen' AND\n",
    "            substring(x, 1, 5) != 'emerg' AND\n",
    "            substring(x, 1, 7) != 'justifi' AND\n",
    "            substring(x, 1, 5) != 'almox' AND\n",
    "            substring(x, 1, 5) != 'reemb'\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    df_clean_nlp = df_clean_temp.withColumn(\"words_filtered\", expr(filter_expression))\n",
    "    df_final_nlp = df_clean_nlp.filter(size(col(\"words_filtered\")) > 0)\n",
    "\n",
    "    print(\"‚úÖ NLP V16 conclu√≠do.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro NLP: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27958444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 7 (V10): Vetoriza√ß√£o Word2Vec (minCount=5)\n",
    "# ==============================================================================\n",
    "from pyspark.ml.feature import Word2Vec, Normalizer\n",
    "\n",
    "print(\"\\n--- Vetoriza√ß√£o V10 (minCount=5 para robustez) ---\")\n",
    "\n",
    "try:\n",
    "    word2Vec = Word2Vec(vectorSize=50, \n",
    "                        minCount=5,   # <--- AUMENTAMOS PARA LIMPAR RU√çDO\n",
    "                        inputCol=\"words_filtered\", \n",
    "                        outputCol=\"raw_features\",\n",
    "                        windowSize=2,\n",
    "                        maxIter=20,\n",
    "                        stepSize=0.025,\n",
    "                        seed=42)\n",
    "    \n",
    "    model_w2v = word2Vec.fit(df_final_nlp)\n",
    "    df_w2v = model_w2v.transform(df_final_nlp)\n",
    "    \n",
    "    normalizer = Normalizer(inputCol=\"raw_features\", outputCol=\"features\", p=2.0)\n",
    "    df_tfidf = normalizer.transform(df_w2v) \n",
    "    \n",
    "    df_tfidf.cache()\n",
    "    print(f\"‚úÖ Vetoriza√ß√£o conclu√≠da.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro Word2Vec: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b90abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DIAGN√ìSTICO W2V: Teste de Similaridade Sem√¢ntica\n",
    "# ==============================================================================\n",
    "print(\"--- Auditando a Intelig√™ncia do Modelo Word2Vec ---\")\n",
    "\n",
    "# Escolha palavras que voc√™ sabe que existem na sua base e representam grupos distintos\n",
    "palavras_teste = [\"chave\", \"torneira\", \"extintor\", \"gasolina\", \"limpeza\", \"caneta\"]\n",
    "\n",
    "try:\n",
    "    for palavra in palavras_teste:\n",
    "        print(f\"\\nüîé Palavras mais pr√≥ximas de '{palavra}':\")\n",
    "        \n",
    "        # O m√©todo findSynonyms busca os vizinhos mais pr√≥ximos no espa√ßo vetorial\n",
    "        # O segundo argumento (5) √© quantas palavras queremos ver\n",
    "        try:\n",
    "            sinonimos = model_w2v.findSynonyms(palavra, 5)\n",
    "            sinonimos.show(truncate=False)\n",
    "        except Exception:\n",
    "            print(f\"   ‚ö†Ô∏è A palavra '{palavra}' n√£o foi encontrada no vocabul√°rio (talvez cortada pelo minCount).\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"‚ùå Erro: A vari√°vel 'model_w2v' n√£o existe. Rode a C√©lula 7 primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 9 (V4.1): Bisecting K-Means (K=10) - CORRIGIDA\n",
    "# ==============================================================================\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# K REDUZIDO PARA 10 GRUPOS\n",
    "K_FINAL = 20\n",
    "\n",
    "print(f\"\\n--- Aplicando Bisecting K-Means (k={K_FINAL}) ---\")\n",
    "\n",
    "try:\n",
    "    # 1. CRIAMOS UMA UDF PARA CALCULAR A NORMA DO VETOR\n",
    "    # Isso evita o erro de agrega√ß√£o do Summarizer e funciona linha a linha.\n",
    "    @udf(returnType=DoubleType())\n",
    "    def get_vector_norm(v):\n",
    "        try:\n",
    "            # Retorna a norma L2 (magnitude) do vetor denso ou esparso\n",
    "            return float(v.norm(2))\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    # 2. FILTRAGEM SEGURA\n",
    "    # Calculamos a norma e filtramos apenas quem tem tamanho > 0\n",
    "    df_metrics = df_tfidf.withColumn(\"vector_norm\", get_vector_norm(col(\"features\")))\n",
    "    df_input = df_metrics.filter(col(\"vector_norm\") > 0).drop(\"vector_norm\")\n",
    "\n",
    "    total_validos = df_input.count()\n",
    "    print(f\"üìä Registros v√°lidos para clusteriza√ß√£o: {total_validos}\")\n",
    "\n",
    "    # 3. CLUSTERIZA√á√ÉO\n",
    "    bkmeans = BisectingKMeans(featuresCol=\"features\", \n",
    "                              k=K_FINAL, \n",
    "                              seed=1, \n",
    "                              predictionCol=\"prediction\", \n",
    "                              minDivisibleClusterSize=100, \n",
    "                              distanceMeasure=\"cosine\")\n",
    "    \n",
    "    model_final = bkmeans.fit(df_input)\n",
    "    df_clustered = model_final.transform(df_input)\n",
    "    \n",
    "    print(f\"‚úÖ Clusteriza√ß√£o k={K_FINAL} conclu√≠da.\")\n",
    "    \n",
    "    print(\"\\n--- Distribui√ß√£o dos Clusters ---\")\n",
    "    df_clustered.groupBy(\"prediction\").count().orderBy(\"prediction\").show(25)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA DE AUDITORIA (V2): Top Palavras + Exemplos\n",
    "# ==============================================================================\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, rand, explode, count, desc\n",
    "\n",
    "print(\"--- Auditoria Detalhada dos Clusters (Keywords + Exemplos) ---\\n\")\n",
    "\n",
    "# --- PARTE 1: Descobrir as Top 7 Palavras por Cluster ---\n",
    "print(\"1. Calculando as palavras mais frequentes de cada grupo...\")\n",
    "\n",
    "# 1. Explode: Transforma ['pneu', 'aro'] em duas linhas: 'pneu' e 'aro'\n",
    "df_exploded = df_clustered.withColumn(\"word\", explode(col(\"words_filtered\")))\n",
    "\n",
    "# 2. Conta frequ√™ncia: Quantas vezes a palavra aparece em cada cluster\n",
    "df_word_counts = df_exploded.groupBy(\"prediction\", \"word\").count()\n",
    "\n",
    "# 3. Rankeia: Pega as Top 7\n",
    "w_rank = Window.partitionBy(\"prediction\").orderBy(col(\"count\").desc())\n",
    "df_top_keywords = df_word_counts.withColumn(\"rank\", row_number().over(w_rank)) \\\n",
    "                                .filter(col(\"rank\") <= 7) \\\n",
    "                                .orderBy(\"prediction\", \"rank\")\n",
    "\n",
    "# 4. Traz para a mem√≥ria (Dicion√°rio Python) para exibi√ß√£o r√°pida\n",
    "# Estrutura final: {0: \"pneu, aro, camara...\", 1: \"caneta, lapis...\"}\n",
    "keywords_data = df_top_keywords.collect()\n",
    "keywords_dict = {}\n",
    "\n",
    "for row in keywords_data:\n",
    "    cluster_id = row['prediction']\n",
    "    word = row['word']\n",
    "    if cluster_id not in keywords_dict:\n",
    "        keywords_dict[cluster_id] = []\n",
    "    keywords_dict[cluster_id].append(word)\n",
    "\n",
    "# --- PARTE 2: Pegar 5 Exemplos Aleat√≥rios (C√≥digo anterior) ---\n",
    "print(\"2. Selecionando amostras aleat√≥rias...\")\n",
    "w_sample = Window.partitionBy(\"prediction\").orderBy(rand(seed=42))\n",
    "df_amostra = df_clustered.withColumn(\"rn\", row_number().over(w_sample)) \\\n",
    "                          .filter(col(\"rn\") <= 5) \\\n",
    "                          .select(\"prediction\", \"objeto_aquisicao\", \"valor_transacao\", \"words_filtered\")\n",
    "\n",
    "amostras = df_amostra.collect()\n",
    "amostras_ordenadas = sorted(amostras, key=lambda x: x['prediction'])\n",
    "\n",
    "# --- PARTE 3: Exibi√ß√£o do Relat√≥rio ---\n",
    "from itertools import groupby\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'RELAT√ìRIO DE CLUSTERS':^100}\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "for cluster_id, itens in groupby(amostras_ordenadas, key=lambda x: x['prediction']):\n",
    "    \n",
    "    # Monta a string de palavras-chave\n",
    "    top_words = keywords_dict.get(cluster_id, [\"(Sem palavras suficientes)\"])\n",
    "    top_words_str = \", \".join(top_words).upper()\n",
    "    \n",
    "    print(f\"üìÇ CLUSTER {cluster_id}\")\n",
    "    print(f\"üîë PALAVRAS-CHAVE: [{top_words_str}]\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for item in itens:\n",
    "        # Formata√ß√£o: Pre√ßo alinhado √† direita | Texto original truncado\n",
    "        print(f\" ‚Ä¢ R$ {item['valor_transacao']:>9.2f} | {item['objeto_aquisicao'][:70]:<70}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01611c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 10 (Relat√≥rio Estruturado): Top 10 Outliers + Estat√≠sticas por Cluster\n",
    "# ==============================================================================\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import expr, col, round, count, desc, row_number, lit\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- 1. Calculando Estat√≠sticas Globais dos Clusters ---\")\n",
    "\n",
    "# 1. Totais por Cluster (Quantos itens existem no total?)\n",
    "df_totais = df_clustered.groupBy(\"prediction\").agg(count(\"*\").alias(\"total_itens_cluster\"))\n",
    "\n",
    "# 2. C√°lculo dos Limites IQR (Q1, Mediana, Q3, Teto)\n",
    "df_quartis = df_clustered.groupBy(\"prediction\").agg(\n",
    "    expr(\"percentile_approx(valor_transacao, 0.25)\").alias(\"Q1\"),\n",
    "    expr(\"percentile_approx(valor_transacao, 0.50)\").alias(\"Mediana\"),\n",
    "    expr(\"percentile_approx(valor_transacao, 0.75)\").alias(\"Q3\")\n",
    ")\n",
    "\n",
    "df_limites = df_quartis.withColumn(\"IQR\", col(\"Q3\") - col(\"Q1\")) \\\n",
    "                       .withColumn(\"limite_superior\", col(\"Q3\") + (1.5 * col(\"IQR\")))\n",
    "\n",
    "# 3. Cruzamento com dados originais\n",
    "df_analise = df_clustered.join(df_limites, on=\"prediction\", how=\"inner\") \\\n",
    "                          .join(df_totais, on=\"prediction\", how=\"inner\")\n",
    "\n",
    "# 4. Filtrando Outliers\n",
    "# Regra: Acima do teto E acima de R$ 50,00\n",
    "df_outliers_raw = df_analise.filter((col(\"valor_transacao\") > col(\"limite_superior\")) & \n",
    "                                    (col(\"valor_transacao\") > 50))\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE A: ESTAT√çSTICAS DE PORCENTAGEM (Visualiza√ß√£o no Console)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 2. Resumo de Contamina√ß√£o por Cluster ---\")\n",
    "\n",
    "# Conta quantos outliers tem em cada cluster\n",
    "df_stats_outliers = df_outliers_raw.groupBy(\"prediction\", \"total_itens_cluster\").agg(count(\"*\").alias(\"qtd_outliers\"))\n",
    "\n",
    "# Calcula a porcentagem\n",
    "df_resumo = df_stats_outliers.withColumn(\"perc_outliers\", round((col(\"qtd_outliers\") / col(\"total_itens_cluster\")) * 100, 2)) \\\n",
    "                             .orderBy(desc(\"perc_outliers\"))\n",
    "\n",
    "df_resumo.show(20)\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE B: TOP 10 MAIS DISCREPANTES POR CLUSTER (Arquivo CSV)\n",
    "# ==============================================================================\n",
    "print(\"--- 3. Extraindo os Top 10 Casos Graves por Cluster ---\")\n",
    "\n",
    "# Definimos uma \"Janela\" por cluster, ordenando pelo valor mais alto (mais grave)\n",
    "janela_cluster = Window.partitionBy(\"prediction\").orderBy(col(\"valor_transacao\").desc())\n",
    "\n",
    "# Criamos um Ranking (1¬∫, 2¬∫, 3¬∫...) e filtramos s√≥ at√© o 10¬∫\n",
    "df_top10 = df_outliers_raw.withColumn(\"rank\", row_number().over(janela_cluster)) \\\n",
    "                          .filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Calculamos m√©tricas finais para o relat√≥rio\n",
    "df_export = df_top10.withColumn(\n",
    "    \"diferenca_valor\", \n",
    "    col(\"valor_transacao\") - col(\"Mediana\")\n",
    ").withColumn(\n",
    "    \"x_vezes_mediana\", \n",
    "    round(col(\"valor_transacao\") / col(\"Mediana\"), 1)\n",
    ").select(\n",
    "    col(\"prediction\").alias(\"Cluster\"),\n",
    "    col(\"rank\").alias(\"Ranking_Gravidade\"),\n",
    "    col(\"objeto_aquisicao\").alias(\"Descricao_Item\"),\n",
    "    col(\"valor_transacao\").alias(\"Preco_Pago\"),\n",
    "    col(\"Mediana\").alias(\"Preco_Medio_Cluster\"),\n",
    "    col(\"limite_superior\").alias(\"Teto_Aceitavel\"),\n",
    "    col(\"x_vezes_mediana\").alias(\"Quantas_Vezes_Mais_Caro\"),\n",
    "    col(\"total_itens_cluster\").alias(\"Tamanho_Cluster\")\n",
    ").orderBy(\"Cluster\", \"Ranking_Gravidade\")\n",
    "\n",
    "# --- Exporta√ß√£o ---\n",
    "try:\n",
    "    pdf_top10 = df_export.toPandas()\n",
    "    nome_arquivo = \"relatorio_top10_outliers.csv\"\n",
    "    \n",
    "    # Salva com encoding correto para Excel/PT-BR\n",
    "    pdf_top10.to_csv(nome_arquivo, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Arquivo '{nome_arquivo}' gerado com sucesso!\")\n",
    "    print(f\"   Conte√∫do: Os 10 maiores desvios de cada um dos clusters.\")\n",
    "    print(f\"   Local: {os.getcwd()}/{nome_arquivo}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao exportar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb55aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 11 (CORRIGIDA): Auditoria ML (Random Forest Regressor)\n",
    "# ==============================================================================\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import col, log, exp, abs as spark_abs, round, desc\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Iniciando Auditoria Preditiva (Random Forest) ---\")\n",
    "print(f\"Par√¢metros: Trees=50 | Depth=8 | MinInstances=5 | Target=Log(Preco)\")\n",
    "\n",
    "# 1. PREPARA√á√ÉO (CORRE√á√ÉO AQUI: Usamos df_clustered em vez de df_tfidf)\n",
    "# Assim garantimos que a coluna 'prediction' (Cluster ID) exista para o relat√≥rio final.\n",
    "df_ml = df_clustered.withColumn(\"label\", log(col(\"valor_transacao\") + 1.0))\n",
    "\n",
    "# 2. TREINAMENTO DO MODELO\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction_log\", # Nome exclusivo para n√£o conflitar com o cluster\n",
    "    seed=42,\n",
    "    numTrees=50,\n",
    "    maxDepth=8,\n",
    "    minInstancesPerNode=5\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Treinando o modelo (analisando padr√µes globais)...\")\n",
    "model_rf = rf.fit(df_ml)\n",
    "\n",
    "# 3. PREDI√á√ÉO\n",
    "predictions = model_rf.transform(df_ml)\n",
    "\n",
    "# 4. C√ÅLCULO DE DISCREP√ÇNCIA\n",
    "df_analise_ml = predictions.withColumn(\"preco_estimado_ml\", exp(col(\"prediction_log\")) - 1.0) \\\n",
    "                           .withColumn(\"razao_sobrepreco\", col(\"valor_transacao\") / (col(\"preco_estimado_ml\") + 0.01)) \\\n",
    "                           .withColumn(\"diferenca_valor\", col(\"valor_transacao\") - col(\"preco_estimado_ml\"))\n",
    "\n",
    "# 5. FILTRAGEM\n",
    "# Regra: Pre√ßo pago > 3x o estimado E Diferen√ßa > R$ 50\n",
    "df_suspeitas_ml = df_analise_ml.filter((col(\"razao_sobrepreco\") > 3) & \n",
    "                                       (col(\"diferenca_valor\") > 50))\n",
    "\n",
    "# Sele√ß√£o de colunas (Agora 'prediction' vai funcionar pois veio do df_clustered)\n",
    "df_export_ml = df_suspeitas_ml.select(\n",
    "    col(\"prediction\").alias(\"Cluster_Original\"), # <-- Agora esta coluna existe!\n",
    "    col(\"objeto_aquisicao\").alias(\"Descricao_Item\"),\n",
    "    col(\"valor_transacao\").alias(\"Preco_Pago_Real\"),\n",
    "    round(col(\"preco_estimado_ml\"), 2).alias(\"Preco_Justo_Estimado\"),\n",
    "    round(col(\"razao_sobrepreco\"), 1).alias(\"Quantas_Vezes_Mais_Caro\"),\n",
    "    col(\"ano\"),\n",
    "    col(\"unidade_gestora\")\n",
    ").orderBy(desc(\"razao_sobrepreco\"))\n",
    "\n",
    "total_suspeitas = df_export_ml.count()\n",
    "print(f\"üö© O modelo encontrou {total_suspeitas} transa√ß√µes suspeitas.\")\n",
    "\n",
    "# --- 6. EXPORTA√á√ÉO ---\n",
    "print(\"\\n--- Gerando Arquivo de Auditoria ML ---\")\n",
    "\n",
    "try:\n",
    "    pdf_ml = df_export_ml.toPandas()\n",
    "    nome_arquivo_ml = \"auditoria_ml_random_forest.csv\"\n",
    "    \n",
    "    pdf_ml.to_csv(nome_arquivo_ml, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "    \n",
    "    print(f\"‚úÖ Arquivo gerado: '{nome_arquivo_ml}'\")\n",
    "    \n",
    "    print(\"\\n--- Top 5 Discrep√¢ncias (Vis√£o do Rob√¥) ---\")\n",
    "    print(pdf_ml.head(5)[[\"Descricao_Item\", \"Preco_Pago_Real\", \"Preco_Justo_Estimado\", \"Quantas_Vezes_Mais_Caro\"]])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na exporta√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA DE CONSOLIDA√á√ÉO (V2): Cruzamento IQR x ML (Filtro Risco > 10)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Iniciando Cruzamento de Auditorias (Filtro: Score > 10) ---\")\n",
    "\n",
    "# Defini√ß√£o dos arquivos de entrada\n",
    "file_iqr = \"auditoria_outliers_iqr.csv\"\n",
    "file_ml = \"auditoria_ml_random_forest.csv\"\n",
    "\n",
    "# Verifica√ß√£o de exist√™ncia\n",
    "if not os.path.exists(file_iqr) or not os.path.exists(file_ml):\n",
    "    print(\"‚ùå Erro: Um dos arquivos de entrada n√£o foi encontrado.\")\n",
    "    print(\"   Certifique-se de ter rodado a C√©lula 10 (IQR) e a C√©lula 11 (ML).\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Carregar os Relat√≥rios\n",
    "        # Usamos float_precision='high' para garantir precis√£o nos valores monet√°rios\n",
    "        df_iqr = pd.read_csv(file_iqr, sep=';', encoding='utf-8-sig')\n",
    "        df_ml = pd.read_csv(file_ml, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "        print(f\"üìÇ Entradas: IQR ({len(df_iqr)} linhas) | ML ({len(df_ml)} linhas)\")\n",
    "\n",
    "        # 2. Padroniza√ß√£o\n",
    "        df_ml.rename(columns={'Preco_Pago_Real': 'Preco_Pago'}, inplace=True)\n",
    "\n",
    "        # 3. O Cruzamento (Interse√ß√£o)\n",
    "        # Identificamos os itens que aparecem nos DOIS relat√≥rios\n",
    "        df_consenso = pd.merge(\n",
    "            df_iqr, \n",
    "            df_ml, \n",
    "            on=['Descricao_Item', 'Preco_Pago', 'ano', 'unidade_gestora'],\n",
    "            how='inner',\n",
    "            suffixes=('_IQR', '_ML')\n",
    "        )\n",
    "        \n",
    "        # 4. C√°lculo do Score de Risco Unificado\n",
    "        # F√≥rmula: (Vezes mais caro ML) + (Excesso IQR / 100)\n",
    "        # Ex: Se ML diz que √© 10x mais caro e IQR diz que estourou 500% o teto -> Score = 10 + 5 = 15\n",
    "        df_consenso['Score_Risco'] = df_consenso['Quantas_Vezes_Mais_Caro'] + (df_consenso['Perc_Excesso'] / 100)\n",
    "        \n",
    "        # 5. FILTRAGEM AGRESSIVA (Score > 10)\n",
    "        # S√≥ passamos para o arquivo final se o risco for alt√≠ssimo\n",
    "        df_final = df_consenso[df_consenso['Score_Risco'] > 10].copy()\n",
    "        \n",
    "        # Organiza√ß√£o das Colunas\n",
    "        colunas_finais = [\n",
    "            'Cluster_ID',              \n",
    "            'Descricao_Item',          \n",
    "            'Preco_Pago',              \n",
    "            'Score_Risco',             \n",
    "            'Preco_Justo_Estimado',    # Vis√£o ML\n",
    "            'Teto_Estatistico',        # Vis√£o Estat√≠stica\n",
    "            'Quantas_Vezes_Mais_Caro', # Indicador ML\n",
    "            'Perc_Excesso',            # Indicador IQR\n",
    "            'ano',\n",
    "            'unidade_gestora'\n",
    "        ]\n",
    "        \n",
    "        # Ordena: O maior risco no topo\n",
    "        df_final = df_final[colunas_finais].sort_values(by='Score_Risco', ascending=False)\n",
    "        \n",
    "        qtd_total = len(df_final)\n",
    "        print(f\"üö© Registros Cr√≠ticos (Score > 10): {qtd_total}\")\n",
    "\n",
    "        # 6. Exporta√ß√£o\n",
    "        nome_arquivo_final = \"auditoria_critica_score_10.csv\"\n",
    "        \n",
    "        if qtd_total > 0:\n",
    "            df_final.to_csv(nome_arquivo_final, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "            print(f\"\\n‚úÖ Relat√≥rio Cr√≠tico Gerado: '{nome_arquivo_final}'\")\n",
    "            print(f\"   Local: {os.getcwd()}/{nome_arquivo_final}\")\n",
    "            \n",
    "            print(\"\\n--- TOP 10 CASOS MAIS GRAVES (ALERTA VERMELHO) ---\")\n",
    "            display_cols = ['Descricao_Item', 'Preco_Pago', 'Score_Risco', 'Quantas_Vezes_Mais_Caro']\n",
    "            # Formata√ß√£o para leitura no console\n",
    "            print(df_final[display_cols].head(10).to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Nenhum registro superou o Score de Risco > 10.\")\n",
    "            print(\"   Isso significa que, embora existam outliers, nenhum √© t√£o extremo a ponto de cruzar os dois m√©todos com essa intensidade.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante o processamento: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA FINAL (CORRIGIDA): Enriquecimento (Join com Dados Originais)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- Iniciando Enriquecimento dos Dados (Recuperando CPFs e Favorecidos) ---\")\n",
    "\n",
    "# Arquivo de entrada\n",
    "file_critico = \"auditoria_critica_score_10.csv\"\n",
    "\n",
    "if not os.path.exists(file_critico):\n",
    "    print(f\"‚ùå Erro: O arquivo '{file_critico}' n√£o foi encontrado.\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Carregar as Anomalias (Pandas)\n",
    "        pdf_criticos = pd.read_csv(file_critico, sep=';', encoding='utf-8-sig')\n",
    "        qtd_criticos = len(pdf_criticos)\n",
    "        print(f\"üìÇ Carregados {qtd_criticos} registros cr√≠ticos para enriquecimento.\")\n",
    "\n",
    "        if qtd_criticos > 0:\n",
    "            # 2. Sele√ß√£o e Renomea√ß√£o (CORRIGIDO)\n",
    "            \n",
    "            # Lista das colunas que existem no CSV\n",
    "            cols_csv = ['Descricao_Item', 'Preco_Pago', 'ano', 'unidade_gestora', \n",
    "                        'Score_Risco', 'Preco_Justo_Estimado', 'Teto_Estatistico', \n",
    "                        'Quantas_Vezes_Mais_Caro', 'Perc_Excesso']\n",
    "            \n",
    "            # Filtramos o PDF apenas com essas colunas\n",
    "            pdf_filtrado = pdf_criticos[cols_csv].copy()\n",
    "            \n",
    "            # Agora renomeamos para bater com os nomes do Spark (df_clustered)\n",
    "            pdf_renomeado = pdf_filtrado.rename(columns={\n",
    "                'Descricao_Item': 'objeto_aquisicao', \n",
    "                'Preco_Pago': 'valor_transacao'\n",
    "            })\n",
    "            \n",
    "            # 3. Cria√ß√£o do DataFrame Spark (Chaves para o Join)\n",
    "            # O Spark vai receber 'objeto_aquisicao' e 'valor_transacao' corretamente agora\n",
    "            df_keys = spark.createDataFrame(pdf_renomeado)\n",
    "\n",
    "            # 4. O JOIN (Recuperando os dados originais)\n",
    "            # Fazemos o join usando as colunas renomeadas\n",
    "            df_completo = df_clustered.join(\n",
    "                df_keys,\n",
    "                on=['objeto_aquisicao', 'valor_transacao', 'ano', 'unidade_gestora'],\n",
    "                how='inner'\n",
    "            )\n",
    "\n",
    "            # 5. Sele√ß√£o Final para o Relat√≥rio (Organiza√ß√£o)\n",
    "            df_relatorio_final = df_completo.select(\n",
    "                # M√©tricas de Auditoria\n",
    "                col(\"Score_Risco\"),\n",
    "                col(\"Quantas_Vezes_Mais_Caro\").alias(\"Fator_Sobrepreco\"),\n",
    "                col(\"Perc_Excesso\").alias(\"Perc_Acima_Teto_Cluster\"),\n",
    "                col(\"Preco_Justo_Estimado\").alias(\"Preco_Ref_ML\"),\n",
    "                col(\"Teto_Estatistico\").alias(\"Preco_Ref_Cluster\"),\n",
    "                \n",
    "                # Dados da Compra\n",
    "                col(\"objeto_aquisicao\").alias(\"Descricao_Original\"),\n",
    "                col(\"valor_transacao\").alias(\"Valor_Pago\"),\n",
    "                col(\"data_aquisicao\"),\n",
    "                col(\"ano\"),\n",
    "                col(\"unidade_gestora\"),\n",
    "                col(\"prediction\").alias(\"Cluster_ID\"),\n",
    "\n",
    "                # Dados Cadastrais (Ouro)\n",
    "                col(\"nome_suprido\").alias(\"Nome_Suprido\"),\n",
    "                col(\"cpf_suprido\").alias(\"CPF_Suprido\"),\n",
    "                col(\"nome_favorecido\").alias(\"Nome_Favorecido\"),\n",
    "                col(\"cpf_cnpj_favorecido\").alias(\"CNPJ_Favorecido\"),\n",
    "                \n",
    "                col(\"periodo_aplicacao\"),\n",
    "                col(\"aprovado\")\n",
    "            ).orderBy(col(\"Score_Risco\").desc())\n",
    "\n",
    "            # 6. Exporta√ß√£o Final\n",
    "            nome_arquivo_rastreio = \"AUDITORIA_COMPLETA_RASTREAVEL.csv\"\n",
    "            \n",
    "            pdf_final = df_relatorio_final.toPandas()\n",
    "            pdf_final.to_csv(nome_arquivo_rastreio, index=False, sep=';', encoding='utf-8-sig', float_format='%.2f')\n",
    "\n",
    "            print(f\"\\n‚úÖ Relat√≥rio Completo Gerado: '{nome_arquivo_rastreio}'\")\n",
    "            print(f\"   Conte√∫do: {len(pdf_final)} linhas com dados cadastrais completos.\")\n",
    "            print(f\"   Local: {os.getcwd()}/{nome_arquivo_rastreio}\")\n",
    "            \n",
    "            print(\"\\n--- Exemplo de Registro Completo (Top 1) ---\")\n",
    "            print(pdf_final.head(1).T)\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è N√£o h√° registros cr√≠ticos para enriquecer.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no enriquecimento: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
