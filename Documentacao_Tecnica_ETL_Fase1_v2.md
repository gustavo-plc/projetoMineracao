# üìò Manual T√©cnico Unificado: Pipeline ETL de Gastos P√∫blicos

**Projeto:** Minera√ß√£o e Auditoria de Cart√£o Corporativo (CPGF)
**Status:** Fase 1 Conclu√≠da (ETL, Limpeza Forense e Consolida√ß√£o "Gold")
**Tecnologia:** Python 3.x + PySpark 3.x (Local/Windows)
**Vers√£o do Pipeline:** 2.0 (Refinada com Slugify e Regex Allowlist)

---

## 1. Vis√£o Geral e Arquitetura

O pipeline foi desenhado para processar arquivos Excel governamentais com baixa padroniza√ß√£o, transformando dados "sujos" em uma camada anal√≠tica confi√°vel ("Gold").

### Fluxo de Dados (Data Lineage)
1.  **Camada Bronze (Input):** Arquivos `.xls` e `.xlsx` originais, organizados por ano.
2.  **Processamento (Silver):**
    * Leitura bruta (Raw) tratando tudo como texto inicialmente.
    * Normaliza√ß√£o de colunas via estrat√©gia *Slugify*.
    * Limpeza agressiva de valores via *Regex Allowlist*.
    * Grava√ß√£o em formato **Parquet** particionado por ano.
3.  **Camada Ouro (Gold/Consolidado):**
    * Leitura unificada das parti√ß√µes.
    * Aplica√ß√£o de Regras de Neg√≥cio (Tratamento de Nulos).
    * Filtragem de "Linhas Fantasmas" (Lixo do Excel).
    * Gera√ß√£o do arquivo √∫nico `Consolidado_Final.parquet`.

---

## 2. Configura√ß√£o do Ambiente Spark (Windows)

Para garantir a execu√ß√£o local no Windows, o script realiza configura√ß√µes autom√°ticas de ambiente na **C√©lula 1**.

* **Engine:** Spark Session com `spark-excel` (com.crealytics vers√£o `3.5.0_0.20.3`).
* **Hadoop/Winutils:** Configura√ß√£o necess√°ria para simular HDFS no Windows.
* **Mem√≥ria:** `spark.driver.memory = "4g"` (Otimizado para volume m√©dio de dados).
* **Fix de Rede:** `spark.driver.bindAddress = "127.0.0.1"` (Evita erros de VPN/Wi-Fi).

---

## 3. Detalhamento T√©cnico das Etapas (ETL)

### 3.1. Estrat√©gia de Mapeamento "Slugify" (C√©lula 2)
**Problema:** Os arquivos governamentais variam os cabe√ßalhos entre anos (ex: "Objeto da Aquisi√ß√£o", "Objeto da Aquisicao", "MOTIVO").
**Solu√ß√£o:** Implementa√ß√£o de um dicion√°rio baseado em *slugs*.
1.  O script remove acentos, espa√ßos e converte o nome da coluna para min√∫sculo (ex: "Objeto da Aquisi√ß√£o" -> `objetodaaquisicao`).
2.  O dicion√°rio mapeia o *slug* para o nome final (`objetodaaquisicao` -> `objeto_aquisicao`).
3.  **Benef√≠cio:** Imunidade a erros de digita√ß√£o, acentua√ß√£o ou *Case Sensitivity* nos arquivos originais.

### 3.2. Limpeza "Blindada" (C√©lula 3)
Fun√ß√µes nativas do Spark (`pyspark.sql.functions`) substitu√≠ram loops Python para performance.

#### A. Limpeza de Valor (Regex Allowlist)
O maior desafio foi limpar campos monet√°rios sujos (ex: `R$    1.200,50` com caracteres ocultos).
* **L√≥gica Antiga:** Tentar remover o que *n√£o* queremos (R$, espa√ßos). Falhava com caracteres invis√≠veis.
* **L√≥gica Nova (Blindada):** Manter apenas o que *queremos*.
    * Regex: `[^0-9,-]` (Apaga tudo que n√£o for n√∫mero, v√≠rgula ou tra√ßo).
    * Resultado: `R$ .. 1.200,50` vira `1200,50`.
    * Convers√£o: Troca `,` por `.` e converte para `DoubleType`.

#### B. Limpeza de Texto
* Uso de `translate` para converter caracteres acentuados (√° -> a, √ß -> c).
* Uso de `regexp_replace` para manter apenas letras e n√∫meros (`[^a-z0-9\s]`).

### 3.3. Processamento e Particionamento (C√©lula 4)
* **Leitura Segura:** `inferSchema="false"`. O Spark l√™ tudo como String. A tipagem forte (Integer, Double) √© aplicada apenas *ap√≥s* a limpeza, evitando erros de convers√£o prematura.
* **Particionamento:** Os dados s√£o salvos em `dados/Parquet/final/ano_partition=YYYY`. Isso permite leitura otimizada (Pruning) no futuro.

---

## 4. Regras de Neg√≥cio e Consolida√ß√£o (C√©lula 5)

Esta etapa transforma os dados t√©cnicos em dados de neg√≥cio, separando o "Lixo" do "Ouro".

### 4.1. O Problema das "Linhas Fantasmas"
* **Diagn√≥stico:** Arquivos Excel frequentemente possuem milhares de linhas em branco formatadas, que o Spark l√™ como linhas nulas. Em 2019, 47% das linhas eram lixo.
* **Solu√ß√£o:** Filtro `valor_transacao > 0`. Se n√£o h√° sa√≠da de dinheiro, o registro √© descartado.

### 4.2. Recupera√ß√£o de Dados Parciais ("Salvar a Leroy Merlin")
* **Diagn√≥stico:** Alguns registros v√°lidos (com valor monet√°rio e favorecido, ex: Leroy Merlin) n√£o possu√≠am descri√ß√£o (`objeto_aquisicao` vazio ou "N/A").
* **Regra de Neg√≥cio:** N√£o descartar dinheiro real por falta de texto.
* **A√ß√£o:** Se `objeto_aquisicao` for nulo/vazio, o sistema preenche com **"NAO INFORMADO"** e mant√©m o registro na base.

---

## 5. Dicion√°rio de Dados Final (Schema)

O arquivo `Consolidado_Final` possui a seguinte estrutura garantida:

| Coluna | Tipo (Spark) | Descri√ß√£o | Regra de Limpeza |
| :--- | :--- | :--- | :--- |
| `ano` | Integer | Ano do exerc√≠cio | Preenchido via nome da pasta se nulo |
| `unidade_gestora` | String | √ìrg√£o respons√°vel | Slugify + Clean Text |
| `nome_suprido` | String | Servidor portador | Slugify + Clean Text |
| `cpf_suprido` | String | CPF do servidor | Apenas n√∫meros |
| `nome_favorecido` | String | Empresa/Pessoa que recebeu | Slugify + Clean Text |
| `cpf_cnpj_favorecido`| String | Documento do recebedor | Apenas n√∫meros |
| `objeto_aquisicao` | String | Descri√ß√£o da compra | "NAO INFORMADO" se vazio |
| `data_aquisicao` | String | Data da compra | Mantido original (limpo) |
| `valor_transacao` | Double | Valor gasto (R$) | Regex Allowlist + Cast Double |

---

## 6. Como Utilizar

1.  **Entrada:** Coloque os arquivos `.xlsx` em `dados/input/{ANO}/`.
2.  **Execu√ß√£o:** Rode o script completo (C√©lulas 1 a 5).
3.  **Sa√≠da:** O arquivo final estar√° em `dados/Consolidado_Final`.
4.  **An√°lise:** Carregue este Parquet no Power BI, Tableau ou Pandas. Ele j√° est√° limpo, tipado e sem lixo.

