{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e874c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configura√ß√£o Inicial ---\n",
      "Vers√£o do Python: 3.11.9\n",
      "--- Iniciando Sess√£o Spark ---\n",
      "Diret√≥rios configurados. Processando anos: ['2016', '2017', '2018', '2019', '2020', '2021']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SCRIPT DE ETL COMPLETO: Extra√ß√£o, Transforma√ß√£o e Carga (Fase 1)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Configura√ß√£o Inicial e Importa√ß√µes ---\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "\n",
    "# Importa√ß√µes do PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, lower, lit, when\n",
    "from pyspark.sql.types import (\n",
    "    DecimalType, StringType, DateType, IntegerType, DoubleType,\n",
    "    StructType, StructField\n",
    ")\n",
    "\n",
    "# Configura√ß√£o para Windows\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "print(\"--- Configura√ß√£o Inicial ---\")\n",
    "print(f\"Vers√£o do Python: {sys.version.split()[0]}\")\n",
    "\n",
    "# --- 2. Inicializando Spark ---\n",
    "print(\"--- Iniciando Sess√£o Spark ---\")\n",
    "excel_maven_package = \"com.crealytics:spark-excel_2.12:3.5.0_0.20.3\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnaliseA3_Local\") \\\n",
    "    .config(\"spark.jars.packages\", excel_maven_package) \\\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\") \\\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"LEGACY\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# --- 3. Diret√≥rios ---\n",
    "BASE_DIR = os.path.join(os.getcwd(), \"dados\")\n",
    "input_base_path = os.path.join(BASE_DIR, \"input\")\n",
    "output_base_path = os.path.join(BASE_DIR, \"Parquet\")\n",
    "\n",
    "os.makedirs(input_base_path, exist_ok=True)\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "anos_a_processar = [str(ano) for ano in range(2016, 2022)]\n",
    "\n",
    "print(f\"Diret√≥rios configurados. Processando anos: {anos_a_processar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfe4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema definido.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 2: Mapeamento de Colunas (Slugify)\n",
    "# ==============================================================================\n",
    "\n",
    "SCHEMA_COLUMNS_MAP = {\n",
    "    # --- Chaves Temporais e Organizacionais ---\n",
    "    \"ano\": \"ano\",\n",
    "    \"unidadegestora\": \"unidade_gestora\",\n",
    "    \"periododeaplicacao\": \"periodo_aplicacao\",\n",
    "    \n",
    "    # --- Identifica√ß√£o ---\n",
    "    \"suprido\": \"nome_suprido\",\n",
    "    \"nomedosuprido\": \"nome_suprido\",\n",
    "    \"cpfdosuprido\": \"cpf_suprido\",\n",
    "    \"cpfportador\": \"cpf_suprido\",\n",
    "    \"aprovado\": \"aprovado\",\n",
    "    \n",
    "    # --- Favorecido ---\n",
    "    \"nomedofavorecido\": \"nome_favorecido\",\n",
    "    \"nomefavorecido\": \"nome_favorecido\",\n",
    "    \"favorecido\": \"nome_favorecido\",\n",
    "    \n",
    "    \"cpfcnpjfavorecido\": \"cpf_cnpj_favorecido\",\n",
    "    \"cpfcnpjdofavorecido\": \"cpf_cnpj_favorecido\",\n",
    "    \"cnpjoucpffavorecido\": \"cpf_cnpj_favorecido\",\n",
    "    \n",
    "    # --- Detalhes ---\n",
    "    \"datadaaquisicao\": \"data_aquisicao\",\n",
    "    \"data\": \"data_aquisicao\",\n",
    "    \n",
    "    # Varia√ß√µes cr√≠ticas\n",
    "    \"objetodaaquisicao\": \"objeto_aquisicao\", \n",
    "    \"motivo\": \"objeto_aquisicao\",\n",
    "    \n",
    "    # --- Valores ---\n",
    "    \"valor\": \"valor_transacao\",\n",
    "    \"valortotal\": \"valor_transacao\"\n",
    "}\n",
    "\n",
    "COLUNAS_FINAIS_ORDENADAS = [\n",
    "    \"ano\",\n",
    "    \"unidade_gestora\",\n",
    "    \"nome_suprido\",\n",
    "    \"cpf_suprido\",\n",
    "    \"periodo_aplicacao\",\n",
    "    \"aprovado\",\n",
    "    \"data_aquisicao\",\n",
    "    \"nome_favorecido\",\n",
    "    \"cpf_cnpj_favorecido\",\n",
    "    \"objeto_aquisicao\",\n",
    "    \"valor_transacao\"\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Schema definido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f782498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Definindo Fun√ß√µes de Limpeza ---\n",
      "‚úÖ Fun√ß√µes definidas.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 3: Fun√ß√µes de Limpeza (Blindadas)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Definindo Fun√ß√µes de Limpeza ---\")\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Padroniza colunas para slug (sem acento, min√∫sculo, sem espa√ßo)\"\"\"\n",
    "    new_columns = []\n",
    "    accents_src = '√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á√ë'\n",
    "    accents_tgt = 'aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN'\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        clean = col_name\n",
    "        trans_table = str.maketrans(accents_src, accents_tgt)\n",
    "        clean = clean.translate(trans_table).lower()\n",
    "        clean_slug = re.sub(r'[^a-z0-9]', '', clean)\n",
    "        \n",
    "        final_name = SCHEMA_COLUMNS_MAP.get(clean_slug)\n",
    "        if not final_name: final_name = clean_slug \n",
    "            \n",
    "        new_columns.append(F.col(f\"`{col_name}`\").alias(final_name))\n",
    "    \n",
    "    return df.select(*new_columns)\n",
    "\n",
    "def process_dataframe(df):\n",
    "    \"\"\"Aplica limpeza e tipagem forte\"\"\"\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    # Helpers de Regex\n",
    "    src_chars = \"√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á√ë\"\n",
    "    tgt_chars = \"aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN\"\n",
    "    \n",
    "    def clean_text_expr(col_name):\n",
    "        return F.trim(F.regexp_replace(F.translate(F.lower(F.col(col_name)), src_chars, tgt_chars), r\"[^a-z0-9\\s]\", \"\"))\n",
    "\n",
    "    # Limpeza de Valor (Allowlist: s√≥ n√∫meros, v√≠rgula e tra√ßo)\n",
    "    val_clean_expr = F.regexp_replace(\n",
    "        F.regexp_replace(F.col(\"valor_transacao\").cast(\"string\"), r\"[^0-9,-]\", \"\"), \n",
    "        \",\", \".\"\n",
    "    ).cast(DoubleType())\n",
    "\n",
    "    doc_clean_expr = lambda c: F.regexp_replace(F.col(c).cast(\"string\"), r\"[^0-9]\", \"\")\n",
    "\n",
    "    # Montagem do Select Final\n",
    "    final_cols = []\n",
    "    col_defs = {\n",
    "        \"ano\": (IntegerType(), F.col(\"ano\") if \"ano\" in df.columns else F.lit(None)),\n",
    "        \"unidade_gestora\": (StringType(), clean_text_expr(\"unidade_gestora\") if \"unidade_gestora\" in df.columns else F.lit(None)),\n",
    "        \"periodo_aplicacao\": (StringType(), F.col(\"periodo_aplicacao\").cast(StringType()) if \"periodo_aplicacao\" in df.columns else F.lit(None)),\n",
    "        \"nome_suprido\": (StringType(), clean_text_expr(\"nome_suprido\") if \"nome_suprido\" in df.columns else F.lit(None)),\n",
    "        \"cpf_suprido\": (StringType(), doc_clean_expr(\"cpf_suprido\") if \"cpf_suprido\" in df.columns else F.lit(None)),\n",
    "        \"aprovado\": (StringType(), clean_text_expr(\"aprovado\") if \"aprovado\" in df.columns else F.lit(None)),\n",
    "        \"data_aquisicao\": (StringType(), F.trim(F.col(\"data_aquisicao\").cast(\"string\")) if \"data_aquisicao\" in df.columns else F.lit(None)),\n",
    "        \"nome_favorecido\": (StringType(), clean_text_expr(\"nome_favorecido\") if \"nome_favorecido\" in df.columns else F.lit(None)),\n",
    "        \"cpf_cnpj_favorecido\": (StringType(), doc_clean_expr(\"cpf_cnpj_favorecido\") if \"cpf_cnpj_favorecido\" in df.columns else F.lit(None)),\n",
    "        \"objeto_aquisicao\": (StringType(), clean_text_expr(\"objeto_aquisicao\") if \"objeto_aquisicao\" in df.columns else F.lit(None)),\n",
    "        \"valor_transacao\": (DoubleType(), val_clean_expr if \"valor_transacao\" in df.columns else F.lit(None))\n",
    "    }\n",
    "\n",
    "    for name in COLUNAS_FINAIS_ORDENADAS:\n",
    "        dtype, expr = col_defs[name]\n",
    "        final_cols.append(expr.cast(dtype).alias(name))\n",
    "\n",
    "    return df.select(*final_cols)\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc6a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Executando Processamento ---\n",
      "\n",
      ">>> Processando 12 arquivos de 2016...\n",
      "   üíæ 2016: Salvo (8753 linhas)\n",
      "\n",
      ">>> Processando 12 arquivos de 2017...\n",
      "   üíæ 2017: Salvo (8090 linhas)\n",
      "\n",
      ">>> Processando 12 arquivos de 2018...\n",
      "   üíæ 2018: Salvo (17930 linhas)\n",
      "\n",
      ">>> Processando 12 arquivos de 2019...\n",
      "   üíæ 2019: Salvo (26741 linhas)\n",
      "\n",
      ">>> Processando 12 arquivos de 2020...\n",
      "   üíæ 2020: Salvo (9232 linhas)\n",
      "\n",
      ">>> Processando 12 arquivos de 2021...\n",
      "   üíæ 2021: Salvo (7662 linhas)\n",
      "\n",
      "‚úÖ Reprocessamento conclu√≠do.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 4: Execu√ß√£o do Pipeline (Leitura -> Particionamento)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Executando Processamento ---\")\n",
    "\n",
    "output_path_final = os.path.join(output_base_path, \"final\")\n",
    "\n",
    "for ano in anos_a_processar:\n",
    "    caminho_ano = os.path.join(input_base_path, ano)\n",
    "    \n",
    "    if not os.path.exists(caminho_ano):\n",
    "        continue\n",
    "\n",
    "    arquivos = [f for f in os.listdir(caminho_ano) if f.endswith(('.xlsx', '.xls'))]\n",
    "    if not arquivos: continue\n",
    "        \n",
    "    print(f\"\\n>>> Processando {len(arquivos)} arquivos de {ano}...\")\n",
    "    \n",
    "    dfs_ano = []\n",
    "    \n",
    "    for arquivo in arquivos:\n",
    "        path_file = os.path.join(caminho_ano, arquivo)\n",
    "        try:\n",
    "            df_raw = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"false\") \\\n",
    "                .load(path_file)\n",
    "            \n",
    "            df_clean = process_dataframe(df_raw)\n",
    "            \n",
    "            # Garante coluna ano se vier nula\n",
    "            df_clean = df_clean.withColumn(\"ano\", F.when(F.col(\"ano\").isNull(), F.lit(int(ano))).otherwise(F.col(\"ano\")))\n",
    "                \n",
    "            dfs_ano.append(df_clean)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro em {arquivo}: {e}\")\n",
    "\n",
    "    if dfs_ano:\n",
    "        try:\n",
    "            df_ano_final = reduce(lambda df1, df2: df1.unionByName(df2), dfs_ano)\n",
    "            output_dir = os.path.join(output_path_final, f\"ano_partition={ano}\")\n",
    "            df_ano_final.write.mode(\"overwrite\").parquet(output_dir)\n",
    "            print(f\"   üíæ {ano}: Salvo ({df_ano_final.count()} linhas)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro consolidando {ano}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Reprocessamento conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aa71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Executando Consolida√ß√£o ---\n",
      "‚úÖ Total Bruto Carregado: 78408\n",
      "‚úÖ Total V√°lido Final: 54196\n",
      "üöÆ Descartados (Lixo/Sem Valor): 24212\n",
      "üíæ Salvando Dataset Consolidado em: c:\\VSCode\\projetoMineracao\\dados\\Consolidado_Final\n",
      "‚úÖ Consolida√ß√£o conclu√≠da!\n",
      "\n",
      "--- Amostra Final dos Dados (Todas as Colunas - Top 5) ---\n",
      "+----+---------------+--------------------+-----------+-------------------+--------+--------------+--------------------+-------------------+--------------------+---------------+-------------+\n",
      "| ano|unidade_gestora|        nome_suprido|cpf_suprido|  periodo_aplicacao|aprovado|data_aquisicao|     nome_favorecido|cpf_cnpj_favorecido|    objeto_aquisicao|valor_transacao|ano_partition|\n",
      "+----+---------------+--------------------+-----------+-------------------+--------+--------------+--------------------+-------------------+--------------------+---------------+-------------+\n",
      "|2019|           prrs|luis felipe basse...|64467066004|23/01/19 a 07/04/19|     sim|    01/02/2019|som art frio come...|     02596073000113|01 servico de rep...|          210.0|         2019|\n",
      "|2019|           prrs|luis felipe basse...|64467066004|23/01/19 a 07/04/19|     sim|    14/02/2019|comel com de mat ...|     04123471000148|instalacao de cam...|          101.0|         2019|\n",
      "|2019|           prrs|luis felipe basse...|64467066004|23/01/19 a 07/04/19|     sim|    14/02/2019|comel com de mat ...|     04123471000148|instalacao de tom...|          27.32|         2019|\n",
      "|2019|           prrs|luiz fernando thomaz|00659213052|11/10/19 a 06/12/19|     sim|    20/11/2019|rei das chaves  d...|     87754784000121|realizacao de cop...|          117.0|         2019|\n",
      "|2019|           prrs|luiz fernando thomaz|00659213052|14/01/19 a 29/03/19|     sim|    08/03/2019|leandro dutra santos|        94117667068|servico de instal...|           60.0|         2019|\n",
      "+----+---------------+--------------------+-----------+-------------------+--------+--------------+--------------------+-------------------+--------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--- Schema Final do Arquivo Consolidado ---\n",
      "root\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- unidade_gestora: string (nullable = true)\n",
      " |-- nome_suprido: string (nullable = true)\n",
      " |-- cpf_suprido: string (nullable = true)\n",
      " |-- periodo_aplicacao: string (nullable = true)\n",
      " |-- aprovado: string (nullable = true)\n",
      " |-- data_aquisicao: string (nullable = true)\n",
      " |-- nome_favorecido: string (nullable = true)\n",
      " |-- cpf_cnpj_favorecido: string (nullable = true)\n",
      " |-- objeto_aquisicao: string (nullable = true)\n",
      " |-- valor_transacao: double (nullable = true)\n",
      " |-- ano_partition: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 5: Consolida√ß√£o Final (Gold) e Visualiza√ß√£o\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Executando Consolida√ß√£o ---\")\n",
    "\n",
    "input_parquet_path = os.path.join(output_base_path, \"final\")\n",
    "output_consolidado = os.path.join(BASE_DIR, \"Consolidado_Final\")\n",
    "\n",
    "try:\n",
    "    df_full = spark.read.option(\"basePath\", input_parquet_path).parquet(input_parquet_path)\n",
    "    \n",
    "    total_bruto = df_full.count()\n",
    "    print(f\"‚úÖ Total Bruto Carregado: {total_bruto}\")\n",
    "    \n",
    "    # 1. TRATAMENTO DE CAMPOS VAZIOS\n",
    "    df_treated = df_full.withColumn(\n",
    "        \"objeto_aquisicao\",\n",
    "        F.when(\n",
    "            F.col(\"objeto_aquisicao\").isNull() | \n",
    "            (F.trim(F.col(\"objeto_aquisicao\")) == \"\") | \n",
    "            (F.col(\"objeto_aquisicao\") == \"na\"), \n",
    "            F.lit(\"NAO INFORMADO\")\n",
    "        ).otherwise(F.col(\"objeto_aquisicao\"))\n",
    "    )\n",
    "\n",
    "    # 2. FILTRO FINANCEIRO\n",
    "    df_gold = df_treated.filter(\n",
    "        F.col(\"valor_transacao\").isNotNull() & \n",
    "        (F.col(\"valor_transacao\") > 0)\n",
    "    )\n",
    "    \n",
    "    total_liquido = df_gold.count()\n",
    "    descartados = total_bruto - total_liquido\n",
    "    \n",
    "    print(f\"‚úÖ Total V√°lido Final: {total_liquido}\")\n",
    "    print(f\"üöÆ Descartados (Lixo/Sem Valor): {descartados}\")\n",
    "    \n",
    "    # 3. SALVAMENTO\n",
    "    print(f\"üíæ Salvando Dataset Consolidado em: {output_consolidado}\")\n",
    "    \n",
    "    df_gold.coalesce(1).write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"compression\", \"snappy\") \\\n",
    "        .parquet(output_consolidado)\n",
    "        \n",
    "    print(\"‚úÖ Consolida√ß√£o conclu√≠da!\")\n",
    "    \n",
    "    # 4. VISUALIZA√á√ÉO FINAL (Ajustada)\n",
    "    print(\"\\n--- Amostra Final dos Dados (Todas as Colunas - Top 5) ---\")\n",
    "    df_gold.show(5, truncate=True)\n",
    "    \n",
    "    print(\"\\n--- Schema Final do Arquivo Consolidado ---\")\n",
    "    df_gold.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na consolida√ß√£o: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
