# üìò Manual T√©cnico Unificado: Pipeline de Engenharia de Dados (ETL) com PySpark

**Projeto:** Minera√ß√£o de Gastos P√∫blicos (Cart√£o Corporativo)
**N√≠vel:** Completo (Iniciante ao Avan√ßado)
**Tecnologia:** Python 3.11 + PySpark 3.5.3 (Local no Windows)
**Status:** Fase 1 Conclu√≠da (ETL e Consolida√ß√£o - Vers√£o Est√°vel 1.0)

---

## 1. Vis√£o Geral do Projeto

Este projeto tem como objetivo processar uma grande quantidade de planilhas governamentais (despesas p√∫blicas), que est√£o despadronizadas e em formatos variados, transformando-as em uma base de dados √∫nica, limpa e otimizada para Intelig√™ncia Artificial.

### O Fluxo de Dados (Pipeline)

1.  **Entrada:** Arquivos Excel (`.xls`, `.xlsx`) e OpenDocument (`.ods`) separados por ano (2016-2025).
2.  **Pr√©-processamento:** Convers√£o de formatos n√£o suportados nativamente.
3.  **Processamento (Spark):** Leitura, limpeza de texto, padroniza√ß√£o de colunas e convers√£o de tipos via motor nativo (JVM).
4.  **Armazenamento Intermedi√°rio:** Convers√£o para formato Parquet (colunar).
5.  **Sa√≠da Final:** Um arquivo √∫nico consolidado (`Consolidado.parquet`).

---

## 2. Configura√ß√£o Cr√≠tica do Ambiente (Windows)

Para viabilizar a execu√ß√£o do Spark (originalmente nativo Linux) no Windows e evitar erros de *NativeIO* ou *Heartbeat*, as seguintes configura√ß√µes foram aplicadas obrigatoriamente.

### 2.1 Bin√°rios do Hadoop (Winutils)
O Spark requer emula√ß√£o do sistema de arquivos HDFS.
* **Vers√£o Hadoop:** 3.3.5 (Compat√≠vel com Spark 3.5.3).
* **Vari√°vel de Ambiente:** `HADOOP_HOME` configurada para `C:\hadoop`.
* **Path:** `%HADOOP_HOME%\bin` adicionado ao Path do sistema.

### 2.2 Corre√ß√£o de DLL (Erro `UnsatisfiedLinkError`)
Para corrigir o erro `java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0`, foi necess√°rio instalar manualmente a biblioteca din√¢mica.
* **Arquivo:** `hadoop.dll`
* **Origem (Download):** [GitHub - cdarlint/winutils (Hadoop 3.3.5)](https://github.com/cdarlint/winutils/blob/master/hadoop-3.3.5/bin/hadoop.dll)
* **Instala√ß√£o:** O arquivo foi copiado para:
    1.  `C:\hadoop\bin`
    2.  `C:\Windows\System32` (Essencial para o carregamento global pelo Java).

### 2.3 Bibliotecas Python Adicionais
* **`pyspark==3.5.3`:** Vers√£o LTS escolhida ap√≥s instabilidade da v4.0.1 no Windows (erros de Netty/BlockManager).
* **`xlrd`:** Necess√°rio para ler arquivos Excel antigos (`.xls`) gerados antes de 2019.
* **`odfpy`:** Para convers√£o pr√©via de arquivos OpenDocument (`.ods`).
* **`pandas`:** Utilizado para introspec√ß√£o r√°pida de metadados (nomes de abas) antes da leitura massiva.

---

## 3. Detalhamento do Pipeline (Passo a Passo)

### Etapa 0: Convers√£o de Arquivos (Script `converter_arquivos.py`)
**Motivo:** O Spark n√£o l√™ nativamente arquivos `.ods` (LibreOffice) com efici√™ncia ou estabilidade.
* **Entrada:** Pasta `dados/input` contendo arquivos misturados (`.ods`, `.xls`, `.xlsx`).
* **A√ß√£o:** O script varre as pastas recursivamente, detecta arquivos `.ods` e usa a biblioteca `odfpy` + `pandas` para salvar uma c√≥pia em `.xlsx`.
* **Sa√≠da:** Arquivos `.xlsx` criados ao lado dos originais.

### Etapa 1: Inicializa√ß√£o e Otimiza√ß√£o (C√©lula 1)
Inicia o "motor" do Spark com configura√ß√µes espec√≠ficas para hardware local (Ryzen 7700).
* **Mem√≥ria (`spark.driver.memory="4g"`):** Aumentada para prevenir estouro de mem√≥ria (Heap Space) ao ler m√∫ltiplos Excels.
* **Paralelismo (`shuffle.partitions="8"`):** Reduzido de 200 (padr√£o) para 8, otimizando para a CPU local e evitando overhead.
* **Rede (`bindAddress="127.0.0.1"`):** For√ßa o uso da rede local interna, corrigindo quedas de conex√£o do driver (*WinError 10054*) causadas por oscila√ß√£o de Wi-Fi/VPN.

### Etapa 2: Regras e Limpeza Nativa (C√©lulas 2 a 5)
Nesta etapa, definimos o schema e as fun√ß√µes de transforma√ß√£o. Houve uma mudan√ßa arquitetural importante aqui: **Substitui√ß√£o de UDFs Python por Spark SQL Nativo**.

#### A. O Schema (Contrato de Dados)
Define a estrutura r√≠gida para evitar infer√™ncia lenta.
* `valor`: `DecimalType(12,2)`
* `ano`: `IntegerType`

#### B. Fun√ß√µes de Limpeza (Otimizadas)
Em vez de usar Python (lento devido √† serializa√ß√£o), usamos express√µes nativas da JVM (`pyspark.sql.functions`).

1.  **Padroniza√ß√£o de Colunas (`clean_column_names`):**
    * Utiliza uma √∫nica proje√ß√£o `select` com `alias` em vez de loops com `withColumnRenamed` (que geram planos de execu√ß√£o complexos).
    * Remove acentos e resolve colis√µes de nomes (ex: `valor`, `valor_1`).

2.  **Limpeza de Dados (`process_dataframe`):**
    * **Acentua√ß√£o:** Uso de `F.translate` para mapear caracteres (`√ß` -> `c`, `√£` -> `a`) *antes* da sanitiza√ß√£o. Isso corrige o problema onde "instala√ß√£o" virava "instalao".
    * **Sanitiza√ß√£o:** Uso de `regexp_replace` para remover s√≠mbolos e pontua√ß√£o.
    * **Monet√°rio:** Remove "R$", pontos de milhar e converte para decimal.

### Etapa 3: Ingest√£o e Convers√£o (C√©lula 6)
O motor de processamento que transforma planilhas lentas em dados r√°pidos.

* **Entrada:** Pastas de anos (`dados/input/2016` a `2025`).
* **Desafio Superado (Abas Desconhecidas):** Os arquivos governamentais n√£o padronizam o nome da aba ("Planilha1", "Sheet1", "Dados").
* **Solu√ß√£o:** Implementa√ß√£o de **Detec√ß√£o Din√¢mica**. O script usa `pd.ExcelFile(path).sheet_names[0]` para descobrir o nome real da aba antes de instanciar o leitor do Spark.
* **Desafio Superado (Arquivos Legados):** Instala√ß√£o da lib `xlrd` para suportar arquivos `.xls` de 2016-2018.
* **A√ß√£o:** Leitura, Aplica√ß√£o de `process_dataframe` e Escrita.
* **Sa√≠da:** Milhares de arquivos particionados em formato **Parquet** (compress√£o Snappy) organizados em `dados/Parquet/{ANO}/{ARQUIVO}`.

### Etapa 4: Consolida√ß√£o Blindada (C√©lula 7)
Junta os milhares de arquivos particionados em um √∫nico arquivo mestre.

#### Desafios Cr√≠ticos Resolvidos nesta Etapa:
1.  **Race Condition (Conflito de Leitura/Escrita):**
    * *Erro:* O Spark tentava ler a pasta `dados/Parquet/*` enquanto escrevia o resultado dentro de `dados/Parquet/final`, causando travamento e `FileNotFoundException`.
    * *Solu√ß√£o:* Isolamento de I/O. Leitura ocorre em `dados/Parquet/20*` e a escrita vai para uma pasta externa `dados/Consolidado`.
2.  **Erro de Infer√™ncia (`UNABLE_TO_INFER_SCHEMA`):**
    * *Erro:* Pastas vazias geradas por falhas anteriores impediam o Spark de adivinhar o schema.
    * *Solu√ß√£o:*
        * Uso de `glob` para listar e filtrar apenas pastas que realmente cont√™m arquivos `.parquet`.
        * Aplica√ß√£o for√ßada do schema (`.schema(schema_base)`).

* **A√ß√£o Final:** Leitura unificada, filtragem de qualidade (remo√ß√£o de nulos) e `coalesce(1)` para gerar um arquivo f√≠sico √∫nico.

---

## 4. Estrutura de Sa√≠da e M√©tricas

O pipeline gerou com sucesso a base consolidada em:
`C:\VSCode\projetoMineracao\dados\Consolidado`

**M√©tricas Finais:**
* **Arquivos Processados:** 72 de 72 (100% de sucesso).
* **Registros Totais Brutos:** ~78.000.
* **Registros V√°lidos (Limpos):** ~54.000 (Registros nulos ou vazios descartados).

**Schema Final Garantido:**
| Coluna | Tipo | Descri√ß√£o |
| :--- | :--- | :--- |
| `ano` | Integer | Ano de exerc√≠cio |
| `cpf_suprido` | String | Apenas d√≠gitos |
| `cpf_cnpj_favorecido` | String | Apenas d√≠gitos |
| `objeto_aquisicao` | String | Texto normalizado (sem acentos, min√∫sculo) |
| `valor` | Decimal(12,2) | Valor monet√°rio formatado |

---

## 5. Resumo de Comandos √öteis (Cheat Sheet)

* **Rodar o script:**
    ```powershell
    python analise_dados.py
    ```
* **Instalar nova biblioteca Python:**
    ```powershell
    pip install nome_da_lib
    ```
* **Verificar se o Spark est√° vivo:**
    Olhe o terminal. Se houver uma barra de progresso `[Stage 0:=>   (0 + 1) / 1]`, ele est√° processando.
* **Interpretar Arquivos na pasta Parquet:**
    * `_SUCCESS`: Indica que o processamento terminou bem.
    * `.crc`: Arquivos de verifica√ß√£o de integridade (n√£o apagar).
    * `part-0000...parquet`: O arquivo de dados real.

---
**Pr√≥ximos Passos (Fase 2):** Carregar o arquivo `Consolidado` e aplicar algoritmos de Machine Learning (K-Means) para clusteriza√ß√£o e detec√ß√£o de anomalias (outliers).
