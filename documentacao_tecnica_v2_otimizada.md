# üìÑ Documenta√ß√£o T√©cnica: Pipeline de ETL Local (PySpark) - Vers√£o Otimizada

**Projeto:** Minera√ß√£o de Dados de Gastos P√∫blicos (Cart√£o Corporativo)
**Ambiente:** Local (Windows 11 / VS Code)
**Tecnologia:** Python 3.11 + PySpark 3.5.3 (Single Node)
**Status:** Configura√ß√£o e Pr√©-processamento Otimizado (Native Spark)

## Vis√£o Geral
Este script realiza a Extra√ß√£o, Transforma√ß√£o e Carga (ETL) de arquivos de despesas p√∫blicas. O fluxo foi projetado para **alta performance local**, substituindo fun√ß√µes Python puras por express√µes nativas do Spark (Catalyst Optimizer), eliminando gargalos de serializa√ß√£o.

---

## üõ†Ô∏è Detalhamento por C√©lula

### C√©lula 1: Configura√ß√£o de Ambiente e Inicializa√ß√£o
**Objetivo:** Preparar o sistema operacional e instanciar o motor Spark com corre√ß√µes para Windows.

* **Configura√ß√£o de Ambiente:**
    * Define `PYSPARK_PYTHON` apontando para o `venv` atual, prevenindo conflitos com o Python da Microsoft Store.
* **Gest√£o de Diret√≥rios:**
    * Utiliza caminhos relativos (`dados/input`) para portabilidade do projeto.
* **Instancia√ß√£o da `SparkSession`:**
    * **Vers√£o:** PySpark 3.5.3 (LTS) para estabilidade.
    * **Depend√™ncia:** Carrega `com.crealytics:spark-excel...` para leitura de planilhas.
    * **Rede:** Fixa `bindAddress` em `127.0.0.1` para evitar erros de conex√£o (WinError 10054).

### C√©lula 2: Defini√ß√£o de Metadados (Schema)
**Objetivo:** Estabelecer a tipagem forte dos dados para evitar infer√™ncia lenta.

* **Mapeamento:** Dicion√°rio para normalizar nomes de colunas que mudaram ao longo dos anos.
* **Schema:** Define explicitamente `Decimal(12,2)` para valores e `Integer` para anos.

### C√©lulas 3, 4 e 5 (Consolidadas): Processamento Otimizado (Native Spark)
**Objetivo:** Limpeza, padroniza√ß√£o e transforma√ß√£o dos dados em uma √∫nica passagem, utilizando apenas a JVM do Spark.

> **Mudan√ßa Arquitetural:** Abandonou-se o uso de **UDFs (User Defined Functions)** em Python.
> **Motivo:** UDFs exigem que o Spark serialize os dados da JVM para o Python e vice-versa para cada linha processada, causando lentid√£o severa. A abordagem nativa roda inteiramente em mem√≥ria otimizada (C++/Java).

#### 1. Padroniza√ß√£o de Colunas (`clean_column_names`)
* **T√©cnica:** Em vez de usar um loop com `withColumnRenamed` (que gera m√∫ltiplos planos de execu√ß√£o), utiliza-se uma √∫nica proje√ß√£o `select` com `alias`.
* **L√≥gica:**
    * Remove acentos e caracteres especiais dos nomes das colunas.
    * Resolve colis√µes de nomes (ex: `valor`, `valor_1`) automaticamente.

#### 2. Limpeza de Dados (`process_dataframe`)
Utiliza express√µes *Lazy Evaluation* do Spark SQL (`pyspark.sql.functions`):

* **Texto (`objeto_aquisicao`):**
    * `regexp_replace`: Remove caracteres n√£o alfanum√©ricos via Regex nativo.
    * `trim` e `lower`: Normaliza√ß√£o de strings.
* **Valores Monet√°rios (`valor`):**
    * Remove s√≠mbolos (R$, pontos) e converte para `Decimal(20,0)`.
    * Aplica divis√£o aritm√©tica por 100.0 para ajuste de centavos.
* **Documentos (CPF/CNPJ):**
    * Regex para manter apenas d√≠gitos (`[^0-9]`).

---

## üèóÔ∏è Decis√µes T√©cnicas e Motiva√ß√µes

| Decis√£o | Motivo T√©cnico |
| :--- | :--- |
| **Remo√ß√£o de UDFs Python** | **Performance:** Elimina o overhead de serializa√ß√£o (Pickle) entre o executor Java e o processo Python. Permite que o *Catalyst Optimizer* do Spark planeje a query de forma eficiente. |
| **Select vs withColumnRenamed** | **Otimiza√ß√£o de Plano:** `withColumnRenamed` em loop cria um plano l√≥gico novo a cada itera√ß√£o (DAG complexo). Um √∫nico `select` com aliases resolve tudo em uma opera√ß√£o. |
| **Spark 3.5.3 (Downgrade)** | **Estabilidade:** A vers√£o 4.0.1 apresentou instabilidade no Windows (erros de Netty/BlockManager). A vers√£o 3.5.3 √© est√°vel e compat√≠vel com `winutils`. |
| **Convers√£o ODS -> XLSX** | **Compatibilidade:** O Spark n√£o possui leitura nativa robusta para OpenDocument (`.ods`). A convers√£o pr√©via via Pandas garante a integridade da ingest√£o. |

