# Configura√ß√£o Inicial e Importa√ß√µes

import os
import sys
import shutil
import pandas as pd
import unicodedata
import re
import traceback
from datetime import datetime

# Importa√ß√µes do PySpark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col, regexp_replace, trim, lower, lit, when
from pyspark.sql.types import (
    DecimalType, StringType, DateType, IntegerType,
    StructType, StructField
)

# --- 1. Configura√ß√£o Cr√≠tica para Windows ---
# For√ßa o PySpark a usar o mesmo Python do ambiente virtual atual
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

print("--- Configura√ß√£o Inicial e Importa√ß√µes ---")
print("Ambiente: Local (Windows/VS Code) adaptado.")
print(f"Vers√£o do Python: {sys.version.split()[0]}")
print(f"Vers√£o do Pandas: {pd.__version__}")
print("Diagn√≥stico da C√©lula 1 conclu√≠do.\n---")

# --- 2. Inicializando a Sess√£o Spark Manualmente (Vers√£o Est√°vel) ---
print("--- Iniciando Sess√£o Spark ---")
print("Nota: Na primeira execu√ß√£o, pode demorar para baixar o pacote do Excel...")

# Defini√ß√£o da biblioteca de Excel correta para Spark 3.x
# Vers√£o antiga: "com.crealytics:spark-excel_2.12:0.14.0" (Causava erro)
# Vers√£o nova: "com.crealytics:spark-excel_2.12:3.5.0_0.20.3" (Est√°vel)
excel_maven_package = "com.crealytics:spark-excel_2.12:3.5.0_0.20.3"

spark = SparkSession.builder \
    .appName("ProjetoMineracao_Mestrado") \
    .config("spark.jars.packages", excel_maven_package) \
    .config("spark.sql.parquet.datetimeRebaseModeInWrite", "LEGACY") \
    .config("spark.sql.parquet.int96RebaseModeInWrite", "LEGACY") \
    .config("spark.sql.shuffle.partitions", "32") \
    .config("spark.driver.bindAddress", "127.0.0.1") \
    .config("spark.driver.host", "127.0.0.1") \
    .master("local[*]") \
    .getOrCreate()

# Ajuste do n√≠vel de log para reduzir polui√ß√£o no terminal
spark.sparkContext.setLogLevel("WARN")

# --- 3. Configura√ß√£o de Diret√≥rios Locais ---
print("--- Configurando par√¢metros ---")

# DEFINA AQUI SEU CAMINHO LOCAL BASE
# Sugest√£o: Usar caminho relativo ou absoluto da sua pasta de projeto
BASE_DIR = os.path.join(os.getcwd(), "dados")

input_base_path = os.path.join(BASE_DIR, "input")  # Onde voc√™ colocar√° os .xlsx
output_base_path = os.path.join(BASE_DIR, "Parquet") # Onde ser√£o salvos os resultados

# Garante que as pastas existam
os.makedirs(input_base_path, exist_ok=True)
os.makedirs(output_base_path, exist_ok=True)

anos_a_processar = [str(ano) for ano in range(2016, 2022)]

print(f"Caminho de entrada base configurado: {input_base_path}")
print(f"Caminho de sa√≠da base para Parquet configurado: {output_base_path}")
print(f"Anos a processar: {anos_a_processar}")

# --- 4. Verifica√ß√£o de Arquivos (Substituto do DBUtils) ---
# Como n√£o temos dbutils.fs.ls, usamos os.listdir
try:
    arquivos = os.listdir(input_base_path)
    print(f"Diagn√≥stico: Caminho de entrada base '{input_base_path}' EXISTE e cont√©m {len(arquivos)} itens.")
    
    if len(arquivos) == 0:
        print("‚ö†Ô∏è AVISO: A pasta de entrada est√° vazia. Coloque seus arquivos Excel em subpastas por ano (ex: dados/input/2016/)")
except Exception as e:
    print(f"ERRO DE DIAGN√ìSTICO: Erro ao acessar '{input_base_path}'. Erro: {e}")

print("Diagn√≥stico conclu√≠do.\n---")

# --- 5. Verifica√ß√£o da Sess√£o Spark ---
if 'spark' in locals() and spark:
    print(f"‚úÖ Sess√£o Spark Local est√° ativa. Vers√£o: {spark.version}")
    
    # Verificando configura√ß√µes definidas
    configs_to_check = [
        "spark.sql.parquet.datetimeRebaseModeInWrite",
        "spark.sql.parquet.int96RebaseModeInWrite",
        "spark.sql.shuffle.partitions"
    ]
    
    for conf in configs_to_check:
        try:
            val = spark.conf.get(conf)
            print(f"   Config '{conf}': {val}")
        except:
            print(f"   Config '{conf}': N√£o definida.")

    # Informa√ß√£o sobre paralelismo
    try:
        print(f"   Info 'default.parallelism': {spark.sparkContext.defaultParallelism}")
    except Exception:
         print("   Info 'default.parallelism': Erro ao obter.")
else:
    print("‚ùå Sess√£o Spark ('spark') n√£o encontrada.")

# DBUtils n√£o existe localmente, ent√£o removemos ou criamos um mock se necess√°rio.
# Para este script, substitu√≠mos o uso dele por 'os', ent√£o n√£o precisamos emular agora.
print("   Nota: Utilit√°rio 'dbutils' foi substitu√≠do por fun√ß√µes nativas 'os' do Python.")


# ==============================================================================
# C√âLULA 2: Mapeamento de colunas e defini√ß√£o de Schema
# ==============================================================================

print("\n--- Executando C√©lula 2: Defini√ß√£o de Schemas ---")

output_folder_name = "final"
output_path_final = os.path.join(output_base_path, output_folder_name)

# 1. Mapeamento: Traduz nomes de colunas bagun√ßados para um padr√£o √∫nico
column_name_mapping = {
    "ano": "ano",
    "cpf do suprido": "cpf_suprido",
    "cpf/cnpj favorecido": "cpf_cnpj_favorecido",
    "cpf/cnpj do favorecido": "cpf_cnpj_favorecido",
    "objeto da aquisi√ß√£o": "objeto_aquisicao",
    "motivo": "objeto_aquisicao",  # Em 2016 usavam "motivo" em vez de objeto
    " objeto da aquisi√ß√£o ": "objeto_aquisicao",
    "objeto da aquisi√ß√£o ": "objeto_aquisicao",
    "valor": "valor",
    " valor ": "valor",
    " valor": "valor",
    "objeto da aquisicao": "objeto_aquisicao"
}

# 2. Schema: Define o tipo de dado de cada coluna (Texto, Inteiro, Decimal)
schema_base = StructType([
    StructField("ano", IntegerType(), True),
    StructField("cpf_suprido", StringType(), True),
    StructField("cpf_cnpj_favorecido", StringType(), True),
    StructField("objeto_aquisicao", StringType(), True),
    StructField("valor", DecimalType(12, 2), True)
])

# Lista das colunas finais que queremos manter
desired_final_columns = [field.name for field in schema_base.fields]

print(f"‚úÖ Schema definido.")
print(f"   Colunas alvo: {desired_final_columns}")

# Verifica√ß√£o simples se a pasta de entrada tem conte√∫do
try:
    if os.path.exists(input_base_path) and os.listdir(input_base_path):
        print(f"   Verifica√ß√£o: Pasta de entrada encontrada e n√£o vazia.")
    else:
        print(f"   ‚ö†Ô∏è AVISO: A pasta de entrada '{input_base_path}' parece vazia.")
except Exception as e:
    print(f"   ‚ö†Ô∏è AVISO: Erro ao verificar entrada: {e}")

print("--- Fim da C√©lula 2 ---")



# ==============================================================================
# C√âLULAS 3, 4 e 5 (CONSOLIDADAS): Processamento Otimizado (Native Spark)
# ==============================================================================

print("\n--- Executando Processamento Otimizado (Native Spark) ---")

# Importa√ß√£o necess√°ria para a manipula√ß√£o de strings no driver (nomes de colunas)
import re

def clean_column_names(df):
    """
    Renomeia todas as colunas de uma vez s√≥ usando Select + Alias.
    """
    new_columns = []
    existing_names = set()
    
    for col_name in df.columns:
        # 1. Limpeza b√°sica da string do nome
        clean = col_name.strip().lower()
        # Remove acentos de forma simples no nome da coluna
        for a, b in zip('√°√©√≠√≥√∫√¢√™√¥√£√µ√ß', 'aeiouaeoaoc'):
            clean = clean.replace(a, b)
        
        # 2. Verifica mapeamento oficial (definido na C√©lula 2)
        final_name = column_name_mapping.get(clean)
        
        # 3. Se n√£o achar, padroniza (snake_case simplificado)
        if not final_name:
            final_name = re.sub(r'[^a-z0-9]+', '_', clean).strip('_')
            if not final_name: final_name = f"col_{df.columns.index(col_name)}"
        
        # 4. Resolve colis√µes (sufixo _1, _2)
        base_name = final_name
        count = 1
        while final_name in existing_names:
            final_name = f"{base_name}_{count}"
            count += 1
            
        existing_names.add(final_name)
        
        # Adiciona √† lista de sele√ß√£o
        new_columns.append(col(f"`{col_name}`").alias(final_name))
    
    return df.select(*new_columns)

def process_dataframe(df):
    """
    Aplica transforma√ß√µes usando APENAS fun√ß√µes nativas do Spark.
    """
    # 1. Padroniza nomes das colunas primeiro
    df = clean_column_names(df)
    
    # 2. Defini√ß√£o das Express√µes de Limpeza (Lazy Evaluation)
    
    # Texto: Remove s√≠mbolos e espa√ßos extras
    txt_clean_expr = F.trim(F.regexp_replace(F.lower(F.col("objeto_aquisicao")), r"[^a-z0-9\s]", ""))
    
    # Valor: Remove tudo que n√£o √© d√≠gito, converte e divide por 100
    val_clean_expr = (
        F.regexp_replace(F.col("valor").cast("string"), r"[^0-9]", "").cast(DecimalType(20,0)) / 100.0
    ).cast(DecimalType(12,2))

    # CPF/CNPJ: Remove pontua√ß√£o
    doc_clean_expr = lambda c: F.regexp_replace(F.col(c).cast("string"), r"[^0-9]", "")

    # 3. Montagem do Select Final
    final_cols = []
    
    # Ano
    if "ano" in df.columns:
        final_cols.append(F.col("ano").cast(IntegerType()).alias("ano"))
    else:
        final_cols.append(F.lit(None).cast(IntegerType()).alias("ano"))

    # CPFs
    for c in ["cpf_suprido", "cpf_cnpj_favorecido"]:
        if c in df.columns:
            final_cols.append(doc_clean_expr(c).alias(c))
        else:
            final_cols.append(F.lit(None).cast(StringType()).alias(c))

    # Objeto
    if "objeto_aquisicao" in df.columns:
        final_cols.append(txt_clean_expr.alias("objeto_aquisicao"))
    else:
        final_cols.append(F.lit(None).cast(StringType()).alias("objeto_aquisicao"))

    # Valor
    if "valor" in df.columns:
        final_cols.append(val_clean_expr.alias("valor"))
    else:
        final_cols.append(F.lit(None).cast(DecimalType(12,2)).alias("valor"))

    return df.select(*final_cols)

print("‚úÖ Fun√ß√µes otimizadas definidas.")

# ==============================================================================
# C√âLULA 6: Leitura de Excel e Convers√£o para Parquet (Execu√ß√£o)
# ==============================================================================

print("\n--- Executando C√©lula 6: Convers√£o XLS -> Parquet ---")

# Estat√≠sticas de execu√ß√£o
total_arquivos = 0
sucessos = 0
erros = {}

# Garante que a lista de anos existe (caso tenha pulado c√©lulas anteriores)
if 'anos_a_processar' not in locals():
    # Define o range de 2016 at√© 2025 (o range exclui o √∫ltimo n√∫mero)
    anos_a_processar = [str(ano) for ano in range(2016, 2026)]

print(f"Processando per√≠odo: {min(anos_a_processar)} a {max(anos_a_processar)}")

for ano in sorted(anos_a_processar):
    caminho_origem_ano = os.path.join(input_base_path, ano)
    caminho_destino_ano = os.path.join(output_base_path, ano)
    
    print(f"\nüìÇ Verificando ano: {ano}")
    
    # Pula anos que n√£o existem na pasta de input
    if not os.path.exists(caminho_origem_ano):
        print(f"   ‚ö†Ô∏è Pasta n√£o encontrada: {caminho_origem_ano}")
        continue
        
    # Lista apenas arquivos Excel (ignora tempor√°rios do sistema '~$')
    arquivos_ano = [
        f for f in os.listdir(caminho_origem_ano) 
        if f.lower().endswith(('.xlsx', '.xls')) and not f.startswith('~$')
    ]
    
    if not arquivos_ano:
        print(f"   ‚ÑπÔ∏è Nenhum arquivo Excel v√°lido encontrado.")
        continue
        
    # Cria a pasta de destino (Parquet)
    os.makedirs(caminho_destino_ano, exist_ok=True)
    
    print(f"   Encontrados {len(arquivos_ano)} arquivos. Iniciando convers√£o...")
    
    for arquivo in arquivos_ano:
        total_arquivos += 1
        nome_sem_extensao = os.path.splitext(arquivo)[0]
        path_origem = os.path.join(caminho_origem_ano, arquivo)
        path_destino = os.path.join(caminho_destino_ano, nome_sem_extensao)
        
        print(f"   üîÑ {arquivo} ... ", end="")
        
        try:
            # 1. Leitura do Excel
            # Tenta ler a aba 'Planilha1' (padr√£o do Excel) come√ßando da c√©lula A1
            df_raw = spark.read.format("com.crealytics.spark.excel") \
                .option("header", "true") \
                .option("inferSchema", "false") \
                .option("treatEmptyValuesAsNulls", "true") \
                .option("dataAddress", "'Planilha1'!A1") \
                .load(path_origem)
            
            # Se o DataFrame vier vazio (0 colunas ou 0 linhas), tenta ler sem especificar aba
            # Isso for√ßa o Spark a pegar a primeira aba ativa, seja qual for o nome
            if len(df_raw.columns) == 0 or df_raw.rdd.isEmpty():
                 df_raw = spark.read.format("com.crealytics.spark.excel") \
                    .option("header", "true") \
                    .option("inferSchema", "false") \
                    .load(path_origem)

            # Se continuar vazio ap√≥s as tentativas, aborta este arquivo
            if len(df_raw.columns) == 0: 
                print("‚ö†Ô∏è VAZIO ou ILEG√çVEL")
                erros[arquivo] = "Arquivo sem colunas detect√°veis"
                continue

            # 2. Processamento √önico
            # AQUI EST√Å A SIMPLIFICA√á√ÉO: Chamamos apenas process_dataframe
            # Ela j√° chama clean_column_names internamente.
            df_final = process_dataframe(df_raw)
            
            # 3. Grava√ß√£o em Parquet
            # Mode 'overwrite' substitui se j√° existir. Compression 'snappy' √© padr√£o e r√°pido.
            df_final.write \
                .mode("overwrite") \
                .option("compression", "snappy") \
                .parquet(path_destino)
            
            print("‚úÖ OK")
            sucessos += 1
            
        except Exception as e:
            # Captura erro sem parar o script todo
            msg_erro = str(e).split('\n')[0][:100] # Pega s√≥ a primeira linha do erro para n√£o poluir
            print(f"‚ùå FALHA ({msg_erro}...)")
            erros[arquivo] = str(e)

print("\n" + "="*50)
print(f"RELAT√ìRIO FINAL DE EXECU√á√ÉO")
print(f"Arquivos Processados: {sucessos} de {total_arquivos}")
if erros:
    print(f"\n‚ö†Ô∏è {len(erros)} Arquivos com Falha:")
    for arq, msg in erros.items():
        print(f" - {arq}: {msg[:150]}...")
else:
    print("\nüéâ Sucesso total! Todos os arquivos foram convertidos.")
print("="*50)
print("--- Fim da C√©lula 6 ---")