# Configura√ß√£o Inicial e Importa√ß√µes

import os
import sys
import shutil
import pandas as pd
import unicodedata
import re
import traceback
from datetime import datetime

# Importa√ß√µes do PySpark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col, regexp_replace, trim, lower, lit, when
from pyspark.sql.types import (
    DecimalType, StringType, DateType, IntegerType,
    StructType, StructField
)

# --- 1. Configura√ß√£o Cr√≠tica para Windows ---
# For√ßa o PySpark a usar o mesmo Python do ambiente virtual atual
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

print("--- Configura√ß√£o Inicial e Importa√ß√µes ---")
print("Ambiente: Local (Windows/VS Code) adaptado.")
print(f"Vers√£o do Python: {sys.version.split()[0]}")
print(f"Vers√£o do Pandas: {pd.__version__}")
print("Diagn√≥stico da C√©lula 1 conclu√≠do.\n---")

# --- 2. Inicializando a Sess√£o Spark Manualmente (Vers√£o Est√°vel) ---
print("--- Iniciando Sess√£o Spark ---")
print("Nota: Na primeira execu√ß√£o, pode demorar para baixar o pacote do Excel...")

# Defini√ß√£o da biblioteca de Excel correta para Spark 3.x
# Vers√£o antiga: "com.crealytics:spark-excel_2.12:0.14.0" (Causava erro)
# Vers√£o nova: "com.crealytics:spark-excel_2.12:3.5.0_0.20.3" (Est√°vel)
excel_maven_package = "com.crealytics:spark-excel_2.12:3.5.0_0.20.3"

# --- C√©lula 1: Ajuste de Mem√≥ria e Paralelismo ---
spark = SparkSession.builder \
    .appName("AnaliseA3_Local") \
    .config("spark.jars.packages", excel_maven_package) \
    .config("spark.sql.parquet.datetimeRebaseModeInWrite", "LEGACY") \
    .config("spark.sql.parquet.int96RebaseModeInWrite", "LEGACY") \
    .config("spark.sql.shuffle.partitions", "8") \
    .config("spark.driver.memory", "4g") \
    .config("spark.driver.bindAddress", "127.0.0.1") \
    .config("spark.driver.host", "127.0.0.1") \
    .master("local[*]") \
    .getOrCreate()

# Ajuste do n√≠vel de log para reduzir polui√ß√£o no terminal
spark.sparkContext.setLogLevel("WARN")

# --- 3. Configura√ß√£o de Diret√≥rios Locais ---
print("--- Configurando par√¢metros ---")

# DEFINA AQUI SEU CAMINHO LOCAL BASE
# Sugest√£o: Usar caminho relativo ou absoluto da sua pasta de projeto
BASE_DIR = os.path.join(os.getcwd(), "dados")

input_base_path = os.path.join(BASE_DIR, "input")  # Onde voc√™ colocar√° os .xlsx
output_base_path = os.path.join(BASE_DIR, "Parquet") # Onde ser√£o salvos os resultados

# Garante que as pastas existam
os.makedirs(input_base_path, exist_ok=True)
os.makedirs(output_base_path, exist_ok=True)

anos_a_processar = [str(ano) for ano in range(2016, 2022)]

print(f"Caminho de entrada base configurado: {input_base_path}")
print(f"Caminho de sa√≠da base para Parquet configurado: {output_base_path}")
print(f"Anos a processar: {anos_a_processar}")

# --- 4. Verifica√ß√£o de Arquivos (Substituto do DBUtils) ---
# Como n√£o temos dbutils.fs.ls, usamos os.listdir
try:
    arquivos = os.listdir(input_base_path)
    print(f"Diagn√≥stico: Caminho de entrada base '{input_base_path}' EXISTE e cont√©m {len(arquivos)} itens.")
    
    if len(arquivos) == 0:
        print("‚ö†Ô∏è AVISO: A pasta de entrada est√° vazia. Coloque seus arquivos Excel em subpastas por ano (ex: dados/input/2016/)")
except Exception as e:
    print(f"ERRO DE DIAGN√ìSTICO: Erro ao acessar '{input_base_path}'. Erro: {e}")

print("Diagn√≥stico conclu√≠do.\n---")

# --- 5. Verifica√ß√£o da Sess√£o Spark ---
if 'spark' in locals() and spark:
    print(f"‚úÖ Sess√£o Spark Local est√° ativa. Vers√£o: {spark.version}")
    
    # Verificando configura√ß√µes definidas
    configs_to_check = [
        "spark.sql.parquet.datetimeRebaseModeInWrite",
        "spark.sql.parquet.int96RebaseModeInWrite",
        "spark.sql.shuffle.partitions"
    ]
    
    for conf in configs_to_check:
        try:
            val = spark.conf.get(conf)
            print(f"   Config '{conf}': {val}")
        except:
            print(f"   Config '{conf}': N√£o definida.")

    # Informa√ß√£o sobre paralelismo
    try:
        print(f"   Info 'default.parallelism': {spark.sparkContext.defaultParallelism}")
    except Exception:
         print("   Info 'default.parallelism': Erro ao obter.")
else:
    print("‚ùå Sess√£o Spark ('spark') n√£o encontrada.")

# DBUtils n√£o existe localmente, ent√£o removemos ou criamos um mock se necess√°rio.
# Para este script, substitu√≠mos o uso dele por 'os', ent√£o n√£o precisamos emular agora.
print("   Nota: Utilit√°rio 'dbutils' foi substitu√≠do por fun√ß√µes nativas 'os' do Python.")


# ==============================================================================
# C√âLULA 2: Mapeamento de Colunas (Estrat√©gia Slugify - Sem acentos/espa√ßos)
# ==============================================================================

# A chave (esquerda) deve ser o nome da coluna:
# 1. Tudo min√∫sculo
# 2. Sem acentos
# 3. Sem espa√ßos
# Exemplo: "Objeto da Aquisi√ß√£o" vira "objetodaaquisicao"

SCHEMA_COLUMNS_MAP = {
    # --- Chaves Temporais e Organizacionais ---
    "ano": "ano",
    "unidadegestora": "unidade_gestora",
    "periododeaplicacao": "periodo_aplicacao",
    
    # --- Identifica√ß√£o ---
    "suprido": "nome_suprido",
    "nomedosuprido": "nome_suprido",
    "cpfdosuprido": "cpf_suprido",
    "cpfportador": "cpf_suprido",
    "aprovado": "aprovado",
    
    # --- Favorecido ---
    "nomedofavorecido": "nome_favorecido",
    "nomefavorecido": "nome_favorecido",
    "favorecido": "nome_favorecido",
    
    "cpfcnpjfavorecido": "cpf_cnpj_favorecido",
    "cpfcnpjdofavorecido": "cpf_cnpj_favorecido",
    "cnpjoucpffavorecido": "cpf_cnpj_favorecido",
    
    # --- Detalhes ---
    "datadaaquisicao": "data_aquisicao",
    "data": "data_aquisicao",
    
    # AQUI ESTAVA O ERRO PRINCIPAL:
    "objetodaaquisicao": "objeto_aquisicao", 
    "motivo": "objeto_aquisicao",
    
    # --- Valores ---
    "valor": "valor_transacao",
    "valortotal": "valor_transacao"
}

# Lista final de colunas desejadas (Ordem do Parquet)
COLUNAS_FINAIS_ORDENADAS = [
    "ano",
    "unidade_gestora",
    "nome_suprido",
    "cpf_suprido",
    "periodo_aplicacao",
    "aprovado",
    "data_aquisicao",
    "nome_favorecido",
    "cpf_cnpj_favorecido",
    "objeto_aquisicao",
    "valor_transacao"
]

print("‚úÖ Dicion√°rio 'Slug' atualizado. Pronto para mapear qualquer varia√ß√£o.")

# ==============================================================================
# C√âLULA 3: Fun√ß√µes de Limpeza (Corre√ß√£o Moeda e N/A)
# ==============================================================================
import re
from pyspark.sql.types import DoubleType, IntegerType, StringType
import pyspark.sql.functions as F

print("\n--- Executando C√©lula 3: Fun√ß√µes de Limpeza Blindadas ---")

def clean_column_names(df):
    """
    Renomeia colunas para o padr√£o slug (sem acento, min√∫sculo, sem espa√ßo).
    """
    new_columns = []
    # Remove acentos
    accents_src = '√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á√ë'
    accents_tgt = 'aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN'
    
    for col_name in df.columns:
        clean = col_name
        # Translitera√ß√£o manual simples para Spark/Python misto
        trans_table = str.maketrans(accents_src, accents_tgt)
        clean = clean.translate(trans_table).lower()
        
        # Remove tudo que n√£o √© letra ou n√∫mero (slug)
        clean_slug = re.sub(r'[^a-z0-9]', '', clean)
        
        # Busca no dicion√°rio
        final_name = SCHEMA_COLUMNS_MAP.get(clean_slug)
        if not final_name: final_name = clean_slug 
            
        new_columns.append(F.col(f"`{col_name}`").alias(final_name))
    
    return df.select(*new_columns)

def process_dataframe(df):
    """
    Aplica limpeza nos dados e garante o Schema final.
    """
    df = clean_column_names(df)
    
    # 1. Limpeza de Texto (Objeto, Nomes)
    src_chars = "√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á√ë"
    tgt_chars = "aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN"
    
    def clean_text_expr(col_name):
        # Remove acentos e caracteres especiais, mas mant√©m letras, n√∫meros e espa√ßos
        # Transforma "N/A" em "na"
        return F.trim(F.regexp_replace(F.translate(F.lower(F.col(col_name)), src_chars, tgt_chars), r"[^a-z0-9\s]", ""))

    # 2. VALOR (A GRANDE CORRE√á√ÉO)
    # L√≥gica:
    # Passo A: Remove TUDO que n√£o for d√≠gito (0-9), v√≠rgula (,) ou sinal de menos (-).
    #          Isso elimina "R$", ".", espa√ßos, caracteres invis√≠veis.
    #          Ex: "R$ 1.200,50" -> "1200,50"
    #          Ex: "R$    12,60" -> "12,60"
    # Passo B: Troca a v√≠rgula por ponto ("1200.50")
    # Passo C: Converte para Double
    val_clean_expr = F.regexp_replace(
        F.regexp_replace(F.col("valor_transacao").cast("string"), r"[^0-9,-]", ""), 
        ",", "."
    ).cast(DoubleType())

    # 3. CPF/CNPJ
    doc_clean_expr = lambda c: F.regexp_replace(F.col(c).cast("string"), r"[^0-9]", "")

    # Montagem do Select Final
    final_cols = []
    col_defs = {
        "ano": (IntegerType(), F.col("ano") if "ano" in df.columns else F.lit(None)),
        "unidade_gestora": (StringType(), clean_text_expr("unidade_gestora") if "unidade_gestora" in df.columns else F.lit(None)),
        "periodo_aplicacao": (StringType(), F.col("periodo_aplicacao").cast(StringType()) if "periodo_aplicacao" in df.columns else F.lit(None)),
        "nome_suprido": (StringType(), clean_text_expr("nome_suprido") if "nome_suprido" in df.columns else F.lit(None)),
        "cpf_suprido": (StringType(), doc_clean_expr("cpf_suprido") if "cpf_suprido" in df.columns else F.lit(None)),
        "aprovado": (StringType(), clean_text_expr("aprovado") if "aprovado" in df.columns else F.lit(None)),
        "data_aquisicao": (StringType(), F.trim(F.col("data_aquisicao").cast("string")) if "data_aquisicao" in df.columns else F.lit(None)),
        "nome_favorecido": (StringType(), clean_text_expr("nome_favorecido") if "nome_favorecido" in df.columns else F.lit(None)),
        "cpf_cnpj_favorecido": (StringType(), doc_clean_expr("cpf_cnpj_favorecido") if "cpf_cnpj_favorecido" in df.columns else F.lit(None)),
        "objeto_aquisicao": (StringType(), clean_text_expr("objeto_aquisicao") if "objeto_aquisicao" in df.columns else F.lit(None)),
        "valor_transacao": (DoubleType(), val_clean_expr if "valor_transacao" in df.columns else F.lit(None))
    }

    for name in COLUNAS_FINAIS_ORDENADAS:
        dtype, expr = col_defs[name]
        final_cols.append(expr.cast(dtype).alias(name))

    return df.select(*final_cols)

print("‚úÖ Fun√ß√µes de limpeza corrigidas (Regex de Valor por Allowlist).")


# ==============================================================================
# C√âLULA 4: Execu√ß√£o do Pipeline (Leitura -> Corre√ß√£o -> Grava√ß√£o)
# ==============================================================================
from functools import reduce

print("\n--- Executando C√©lula 4: Reprocessamento ---")

output_path_final = os.path.join(output_base_path, "final")

for ano in anos_a_processar:
    caminho_ano = os.path.join(input_base_path, ano)
    
    if not os.path.exists(caminho_ano):
        continue

    arquivos = [f for f in os.listdir(caminho_ano) if f.endswith(('.xlsx', '.xls'))]
    if not arquivos: continue
        
    print(f"\n>>> Processando {len(arquivos)} arquivos de {ano}...")
    
    dfs_ano = []
    
    for arquivo in arquivos:
        path_file = os.path.join(caminho_ano, arquivo)
        try:
            # For√ßamos inferSchema=False e lemos tudo como String primeiro para evitar erro de tipo
            # Isso √© mais seguro para a limpeza manual que fazemos na C√©lula 3
            df_raw = spark.read.format("com.crealytics.spark.excel") \
                .option("header", "true") \
                .option("inferSchema", "false") \
                .load(path_file)
            
            df_clean = process_dataframe(df_raw)
            
            # Garante coluna ano
            df_clean = df_clean.withColumn("ano", F.when(F.col("ano").isNull(), F.lit(int(ano))).otherwise(F.col("ano")))
                
            dfs_ano.append(df_clean)
            
        except Exception as e:
            print(f"‚ùå Erro em {arquivo}: {e}")

    if dfs_ano:
        try:
            df_ano_final = reduce(lambda df1, df2: df1.unionByName(df2), dfs_ano)
            output_dir = os.path.join(output_path_final, f"ano_partition={ano}")
            df_ano_final.write.mode("overwrite").parquet(output_dir)
            print(f"   üíæ {ano}: Salvo ({df_ano_final.count()} linhas)")
        except Exception as e:
            print(f"‚ùå Erro consolidando {ano}: {e}")

print("\n‚úÖ Reprocessamento conclu√≠do.")


from pyspark.sql.functions import col, when, lit, trim

print("\n--- Executando C√©lula 5: Consolida√ß√£o Final (Gold) ---")

input_parquet_path = os.path.join(output_base_path, "final")
output_consolidado = os.path.join(BASE_DIR, "Consolidado_Final")

try:
    df_full = spark.read.option("basePath", input_parquet_path).parquet(input_parquet_path)
    
    total_bruto = df_full.count()
    print(f"‚úÖ Total Bruto Carregado: {total_bruto}")
    
    # 1. TRATAMENTO DE CAMPOS VAZIOS (Para n√£o perder dinheiro real)
    # Se o Objeto for nulo, vazio ou "na", vira "NAO INFORMADO"
    # Assim salvamos os registros da Leroy Merlin/Drogaria SP que estavam sem descri√ß√£o
    df_treated = df_full.withColumn(
        "objeto_aquisicao",
        when(
            col("objeto_aquisicao").isNull() | 
            (trim(col("objeto_aquisicao")) == "") | 
            (col("objeto_aquisicao") == "na"), 
            lit("NAO INFORMADO")
        ).otherwise(col("objeto_aquisicao"))
    )

    # 2. FILTRO FINANCEIRO (Obrigat√≥rio ter Valor)
    # Agora s√≥ descartamos se n√£o tiver VALOR. Se tiver valor, a gente guarda.
    df_gold = df_treated.filter(
        col("valor_transacao").isNotNull() & 
        (col("valor_transacao") > 0)
    )
    
    total_liquido = df_gold.count()
    descartados = total_bruto - total_liquido
    
    print(f"‚úÖ Total V√°lido Final: {total_liquido}")
    print(f"üöÆ Descartados (Sem Valor / Lixo Excel): {descartados}")
    
    # 3. SALVAMENTO
    print(f"üíæ Salvando Dataset Consolidado em: {output_consolidado}")
    
    df_gold.coalesce(1).write \
        .mode("overwrite") \
        .option("compression", "snappy") \
        .parquet(output_consolidado)
        
    print("‚úÖ Consolida√ß√£o conclu√≠da!")
    
    # 4. PROVA DOS 9
    print("\n--- Verificando registros recuperados (Ex: Leroy Merlin/Drogaria) ---")
    df_gold.filter(col("objeto_aquisicao") == "NAO INFORMADO").select("ano", "valor_transacao", "nome_favorecido").show(5, truncate=False)

except Exception as e:
    print(f"‚ùå Erro na consolida√ß√£o: {e}")