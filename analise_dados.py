# Configura√ß√£o Inicial e Importa√ß√µes

import os
import sys
import shutil
import pandas as pd
import unicodedata
import re
import traceback
from datetime import datetime

# Importa√ß√µes do PySpark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col, regexp_replace, trim, lower, lit, when
from pyspark.sql.types import (
    DecimalType, StringType, DateType, IntegerType,
    StructType, StructField
)

# --- 1. Configura√ß√£o Cr√≠tica para Windows ---
# For√ßa o PySpark a usar o mesmo Python do ambiente virtual atual
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

print("--- Configura√ß√£o Inicial e Importa√ß√µes ---")
print("Ambiente: Local (Windows/VS Code) adaptado.")
print(f"Vers√£o do Python: {sys.version.split()[0]}")
print(f"Vers√£o do Pandas: {pd.__version__}")
print("Diagn√≥stico da C√©lula 1 conclu√≠do.\n---")

# --- 2. Inicializando a Sess√£o Spark Manualmente (Vers√£o Est√°vel) ---
print("--- Iniciando Sess√£o Spark ---")
print("Nota: Na primeira execu√ß√£o, pode demorar para baixar o pacote do Excel...")

# Defini√ß√£o da biblioteca de Excel correta para Spark 3.x
# Vers√£o antiga: "com.crealytics:spark-excel_2.12:0.14.0" (Causava erro)
# Vers√£o nova: "com.crealytics:spark-excel_2.12:3.5.0_0.20.3" (Est√°vel)
excel_maven_package = "com.crealytics:spark-excel_2.12:3.5.0_0.20.3"

# --- C√©lula 1: Ajuste de Mem√≥ria e Paralelismo ---
spark = SparkSession.builder \
    .appName("AnaliseA3_Local") \
    .config("spark.jars.packages", excel_maven_package) \
    .config("spark.sql.parquet.datetimeRebaseModeInWrite", "LEGACY") \
    .config("spark.sql.parquet.int96RebaseModeInWrite", "LEGACY") \
    .config("spark.sql.shuffle.partitions", "8") \
    .config("spark.driver.memory", "4g") \
    .config("spark.driver.bindAddress", "127.0.0.1") \
    .config("spark.driver.host", "127.0.0.1") \
    .master("local[*]") \
    .getOrCreate()

# Ajuste do n√≠vel de log para reduzir polui√ß√£o no terminal
spark.sparkContext.setLogLevel("WARN")

# --- 3. Configura√ß√£o de Diret√≥rios Locais ---
print("--- Configurando par√¢metros ---")

# DEFINA AQUI SEU CAMINHO LOCAL BASE
# Sugest√£o: Usar caminho relativo ou absoluto da sua pasta de projeto
BASE_DIR = os.path.join(os.getcwd(), "dados")

input_base_path = os.path.join(BASE_DIR, "input")  # Onde voc√™ colocar√° os .xlsx
output_base_path = os.path.join(BASE_DIR, "Parquet") # Onde ser√£o salvos os resultados

# Garante que as pastas existam
os.makedirs(input_base_path, exist_ok=True)
os.makedirs(output_base_path, exist_ok=True)

anos_a_processar = [str(ano) for ano in range(2016, 2022)]

print(f"Caminho de entrada base configurado: {input_base_path}")
print(f"Caminho de sa√≠da base para Parquet configurado: {output_base_path}")
print(f"Anos a processar: {anos_a_processar}")

# --- 4. Verifica√ß√£o de Arquivos (Substituto do DBUtils) ---
# Como n√£o temos dbutils.fs.ls, usamos os.listdir
try:
    arquivos = os.listdir(input_base_path)
    print(f"Diagn√≥stico: Caminho de entrada base '{input_base_path}' EXISTE e cont√©m {len(arquivos)} itens.")
    
    if len(arquivos) == 0:
        print("‚ö†Ô∏è AVISO: A pasta de entrada est√° vazia. Coloque seus arquivos Excel em subpastas por ano (ex: dados/input/2016/)")
except Exception as e:
    print(f"ERRO DE DIAGN√ìSTICO: Erro ao acessar '{input_base_path}'. Erro: {e}")

print("Diagn√≥stico conclu√≠do.\n---")

# --- 5. Verifica√ß√£o da Sess√£o Spark ---
if 'spark' in locals() and spark:
    print(f"‚úÖ Sess√£o Spark Local est√° ativa. Vers√£o: {spark.version}")
    
    # Verificando configura√ß√µes definidas
    configs_to_check = [
        "spark.sql.parquet.datetimeRebaseModeInWrite",
        "spark.sql.parquet.int96RebaseModeInWrite",
        "spark.sql.shuffle.partitions"
    ]
    
    for conf in configs_to_check:
        try:
            val = spark.conf.get(conf)
            print(f"   Config '{conf}': {val}")
        except:
            print(f"   Config '{conf}': N√£o definida.")

    # Informa√ß√£o sobre paralelismo
    try:
        print(f"   Info 'default.parallelism': {spark.sparkContext.defaultParallelism}")
    except Exception:
         print("   Info 'default.parallelism': Erro ao obter.")
else:
    print("‚ùå Sess√£o Spark ('spark') n√£o encontrada.")

# DBUtils n√£o existe localmente, ent√£o removemos ou criamos um mock se necess√°rio.
# Para este script, substitu√≠mos o uso dele por 'os', ent√£o n√£o precisamos emular agora.
print("   Nota: Utilit√°rio 'dbutils' foi substitu√≠do por fun√ß√µes nativas 'os' do Python.")


# ==============================================================================
# C√âLULA 2: Mapeamento de colunas e defini√ß√£o de Schema
# ==============================================================================

print("\n--- Executando C√©lula 2: Defini√ß√£o de Schemas ---")

output_folder_name = "final"
output_path_final = os.path.join(output_base_path, output_folder_name)

# 1. Mapeamento: Traduz nomes de colunas bagun√ßados para um padr√£o √∫nico
column_name_mapping = {
    "ano": "ano",
    "cpf do suprido": "cpf_suprido",
    "cpf/cnpj favorecido": "cpf_cnpj_favorecido",
    "cpf/cnpj do favorecido": "cpf_cnpj_favorecido",
    "objeto da aquisi√ß√£o": "objeto_aquisicao",
    "motivo": "objeto_aquisicao",  # Em 2016 usavam "motivo" em vez de objeto
    " objeto da aquisi√ß√£o ": "objeto_aquisicao",
    "objeto da aquisi√ß√£o ": "objeto_aquisicao",
    "valor": "valor",
    " valor ": "valor",
    " valor": "valor",
    "objeto da aquisicao": "objeto_aquisicao"
}

# 2. Schema: Define o tipo de dado de cada coluna (Texto, Inteiro, Decimal)
schema_base = StructType([
    StructField("ano", IntegerType(), True),
    StructField("cpf_suprido", StringType(), True),
    StructField("cpf_cnpj_favorecido", StringType(), True),
    StructField("objeto_aquisicao", StringType(), True),
    StructField("valor", DecimalType(12, 2), True)
])

# Lista das colunas finais que queremos manter
desired_final_columns = [field.name for field in schema_base.fields]

print(f"‚úÖ Schema definido.")
print(f"   Colunas alvo: {desired_final_columns}")

# Verifica√ß√£o simples se a pasta de entrada tem conte√∫do
try:
    if os.path.exists(input_base_path) and os.listdir(input_base_path):
        print(f"   Verifica√ß√£o: Pasta de entrada encontrada e n√£o vazia.")
    else:
        print(f"   ‚ö†Ô∏è AVISO: A pasta de entrada '{input_base_path}' parece vazia.")
except Exception as e:
    print(f"   ‚ö†Ô∏è AVISO: Erro ao verificar entrada: {e}")

print("--- Fim da C√©lula 2 ---")



# ==============================================================================
# C√âLULAS 3, 4 e 5 (CORRIGIDAS E FINALIZADAS): Processamento Nativo Seguro
# ==============================================================================

print("\n--- Executando Processamento Otimizado (Native Spark) ---")

import re

def clean_column_names(df):
    """
    Renomeia colunas removendo acentos e padronizando (Python-side).
    Garante que nomes de colunas como 'Descri√ß√£o' virem 'descricao'.
    """
    new_columns = []
    existing_names = set()
    
    # Mapeamento completo de acentos para nomes de colunas
    accents_src = '√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±'
    accents_tgt = 'aaaaaeeeeiiiiooooouuuucn'
    
    for col_name in df.columns:
        clean = col_name.strip().lower()
        
        # Remove acentos do nome da coluna
        for src, tgt in zip(accents_src, accents_tgt):
            clean = clean.replace(src, tgt)
        
        # Verifica mapeamento oficial (definido na C√©lula 2)
        final_name = column_name_mapping.get(clean)
        
        if not final_name:
            # Remove qualquer coisa que n√£o seja letra ou n√∫mero (snake_case)
            final_name = re.sub(r'[^a-z0-9]+', '_', clean).strip('_')
            if not final_name: final_name = f"col_{df.columns.index(col_name)}"
        
        # Resolve conflitos de nomes iguais
        base_name = final_name
        count = 1
        while final_name in existing_names:
            final_name = f"{base_name}_{count}"
            count += 1
            
        existing_names.add(final_name)
        new_columns.append(col(f"`{col_name}`").alias(final_name))
    
    return df.select(*new_columns)

def process_dataframe(df):
    """
    Aplica limpeza nos dados usando fun√ß√µes nativas do Spark.
    CORRE√á√ÉO FINAL: Garante substitui√ß√£o de todos os acentos antes da limpeza de s√≠mbolos.
    """
    # 1. Padroniza nomes das colunas
    df = clean_column_names(df)
    
    # 2. Defini√ß√£o da limpeza de Texto (Objeto da Aquisi√ß√£o)
    
    # Lista completa de caracteres acentuados do Portugu√™s
    # O Spark vai procurar qualquer caractere da primeira string e trocar pelo correspondente na segunda.
    src_chars = "√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß√±√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á√ë"
    tgt_chars = "aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN"
    
    # Passo A: Converte para min√∫sculo
    txt_lower = F.lower(F.col("objeto_aquisicao"))
    
    # Passo B: Troca acentos por letras normais (√ß->c, √£->a, √©->e...)
    txt_translated = F.translate(txt_lower, src_chars, tgt_chars)
    
    # Passo C: Remove caracteres que n√£o s√£o letras(a-z), n√∫meros(0-9) ou espa√ßo
    # Isso elimina tra√ßos, pontos, par√™nteses, etc.
    txt_clean_expr = F.trim(F.regexp_replace(txt_translated, r"[^a-z0-9\s]", ""))
    
    # Limpeza de Valor (R$ 1.000,00 -> 1000.00)
    val_clean_expr = (
        F.regexp_replace(F.col("valor").cast("string"), r"[^0-9]", "").cast(DecimalType(20,0)) / 100.0
    ).cast(DecimalType(12,2))

    # Limpeza de CPF/CNPJ (Remove pontua√ß√£o)
    doc_clean_expr = lambda c: F.regexp_replace(F.col(c).cast("string"), r"[^0-9]", "")

    # 3. Montagem do Select Final
    final_cols = []
    
    # Ano
    if "ano" in df.columns:
        final_cols.append(F.col("ano").cast(IntegerType()).alias("ano"))
    else:
        final_cols.append(F.lit(None).cast(IntegerType()).alias("ano"))

    # CPFs
    for c in ["cpf_suprido", "cpf_cnpj_favorecido"]:
        if c in df.columns:
            final_cols.append(doc_clean_expr(c).alias(c))
        else:
            final_cols.append(F.lit(None).cast(StringType()).alias(c))

    # Objeto
    if "objeto_aquisicao" in df.columns:
        final_cols.append(txt_clean_expr.alias("objeto_aquisicao"))
    else:
        final_cols.append(F.lit(None).cast(StringType()).alias("objeto_aquisicao"))

    # Valor
    if "valor" in df.columns:
        final_cols.append(val_clean_expr.alias("valor"))
    else:
        final_cols.append(F.lit(None).cast(DecimalType(12,2)).alias("valor"))

    return df.select(*final_cols)

print("‚úÖ Fun√ß√µes otimizadas (Corre√ß√£o total de acentua√ß√£o: √£, √ß, √© -> a, c, e) definidas.")
print("--- Fim das C√©lulas 4 e 5 ---")

# ==============================================================================
# C√âLULA 6 (CORRIGIDA): Leitura Inteligente de Abas + Convers√£o
# ==============================================================================

print("\n--- Executando C√©lula 6: Convers√£o XLS -> Parquet ---")

# Importa√ß√£o necess√°ria para ler nomes de abas
import pandas as pd

total_arquivos = 0
sucessos = 0
erros = {}

if 'anos_a_processar' not in locals():
    anos_a_processar = [str(ano) for ano in range(2016, 2026)]

print(f"Processando per√≠odo: {min(anos_a_processar)} a {max(anos_a_processar)}")

for ano in sorted(anos_a_processar):
    caminho_origem_ano = os.path.join(input_base_path, ano)
    caminho_destino_ano = os.path.join(output_base_path, ano)
    
    print(f"\nüìÇ Processando ano: {ano}")
    
    if not os.path.exists(caminho_origem_ano):
        print(f"   ‚ö†Ô∏è Pasta n√£o encontrada: {caminho_origem_ano}")
        continue
        
    arquivos_ano = [
        f for f in os.listdir(caminho_origem_ano) 
        if f.lower().endswith(('.xlsx', '.xls')) and not f.startswith('~$')
    ]
    
    if not arquivos_ano:
        print(f"   ‚ÑπÔ∏è Nenhum arquivo Excel na pasta {ano}.")
        continue
        
    os.makedirs(caminho_destino_ano, exist_ok=True)
    
    for arquivo in arquivos_ano:
        total_arquivos += 1
        nome_sem_extensao = os.path.splitext(arquivo)[0]
        path_origem = os.path.join(caminho_origem_ano, arquivo)
        path_destino = os.path.join(caminho_destino_ano, nome_sem_extensao)
        
        print(f"   üîÑ {arquivo} ... ", end="")
        
        try:
            # ESTRAT√âGIA NOVA: Descobrir o nome da aba com Pandas (r√°pido e seguro)
            # O Pandas l√™ apenas os metadados, n√£o o arquivo todo, ent√£o √© r√°pido.
            xl = pd.ExcelFile(path_origem)
            nome_primeira_aba = xl.sheet_names[0]
            
            # Agora mandamos o Spark ler exatamente essa aba
            df_raw = spark.read.format("com.crealytics.spark.excel") \
                .option("header", "true") \
                .option("inferSchema", "false") \
                .option("treatEmptyValuesAsNulls", "true") \
                .option("dataAddress", f"'{nome_primeira_aba}'!A1") \
                .load(path_origem)

            if len(df_raw.columns) == 0 or df_raw.rdd.isEmpty():
                print("‚ö†Ô∏è VAZIO")
                erros[arquivo] = "Arquivo vazio ou sem colunas"
                continue

            # Processamento
            df_final = process_dataframe(df_raw)
            
            # Grava√ß√£o
            df_final.write.mode("overwrite").option("compression", "snappy").parquet(path_destino)
            
            print("‚úÖ OK")
            sucessos += 1
            
        except Exception as e:
            msg_erro = str(e).split('\n')[0][:100]
            print(f"‚ùå FALHA ({msg_erro}...)")
            erros[arquivo] = str(e)

print("\n" + "="*40)
print(f"RELAT√ìRIO FINAL: {sucessos}/{total_arquivos} arquivos.")
if erros:
    print(f"Falhas: {len(erros)}")
    # Salva log de erros em arquivo para facilitar debug
    with open("erros_conversao.log", "w") as f:
        for arq, msg in erros.items():
            f.write(f"{arq}: {msg}\n")
    print("Detalhes salvos em 'erros_conversao.log'")
print("="*40)


# ==============================================================================
# C√âLULA 7 (FINAL BLINDADA): Consolida√ß√£o com Schema Expl√≠cito
# ==============================================================================

print("\n--- Executando C√©lula 7: Consolida√ß√£o dos Dados ---")

import glob

try:
    # 1. Encontrar todas as subpastas de dados (Ano -> Arquivo)
    # Padr√£o: dados/Parquet/20*/despesas_*
    padrao_busca = os.path.join(output_base_path, "20*", "*")
    candidatos = glob.glob(padrao_busca)
    
    # 2. Filtrar apenas pastas que cont√™m arquivos .parquet v√°lidos
    pastas_validas = []
    print("Verificando integridade das pastas...")
    
    for pasta in candidatos:
        # Verifica se tem algum arquivo terminando em .parquet dentro
        tem_parquet = any(f.endswith('.parquet') for f in os.listdir(pasta))
        if tem_parquet:
            pastas_validas.append(pasta)
    
    if not pastas_validas:
        raise Exception(f"Nenhuma pasta v√°lida com arquivos Parquet encontrada em: {output_base_path}")
        
    print(f"Pastas v√°lidas encontradas: {len(pastas_validas)}")
    
    # 3. Leitura com Schema FOR√áADO
    # Ao passar 'schema=schema_base', o Spark n√£o tenta adivinhar nada, ele apenas l√™.
    # Isso resolve o erro UNABLE_TO_INFER_SCHEMA e √© muito mais r√°pido.
    df_consolidado = spark.read \
        .schema(schema_base) \
        .option("mergeSchema", "false") \
        .parquet(*pastas_validas)
    
    total_registros = df_consolidado.count()
    print(f"‚úÖ Leitura conclu√≠da. Total de registros: {total_registros}")

    # 4. Filtragem de Qualidade
    print("Aplicando filtros de qualidade...")
    
    df_filtered = df_consolidado.filter(
        F.col("valor").isNotNull() & 
        (F.col("valor") > 0) & 
        F.col("objeto_aquisicao").isNotNull() & 
        (F.trim(F.col("objeto_aquisicao")) != "")
    )
    
    total_filtrado = df_filtered.count()
    print(f"Registros v√°lidos: {total_filtrado}")
    print(f"Descartados: {total_registros - total_filtrado}")

    # 5. Salvamento Final
    path_consolidado = os.path.join(BASE_DIR, "Consolidado")
    print(f"Salvando consolidado em: {path_consolidado}")
    
    # Removemos .coalesce(1) se o arquivo for muito grande, mas para 100k linhas √© seguro
    df_filtered.coalesce(1).write \
        .mode("overwrite") \
        .option("compression", "snappy") \
        .parquet(path_consolidado)
        
    print("‚úÖ Consolida√ß√£o conclu√≠da com sucesso!")
    
    # 6. Amostra
    print("\n--- Amostra dos Dados Finais ---")
    df_filtered.select("ano", "valor", "objeto_aquisicao").show(5, truncate=80)

except Exception as e:
    print(f"‚ùå Erro na consolida√ß√£o: {e}")
    import traceback
    traceback.print_exc()

print("--- Fim da C√©lula 7 ---")