# ==============================================================================
# FASE 2: An√°lise Explorat√≥ria de Dados (EDA) e Minera√ß√£o
# Arquivo: analise_fase2.py
# ==============================================================================

import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# --- 1. Configura√ß√£o de Ambiente (Windows) ---
# Garante que o PySpark use o Python correto do ambiente virtual
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

print("--- Iniciando Fase 2: An√°lise Explorat√≥ria ---")

# --- 2. Inicializando Sess√£o Spark (Leve) ---
# Nota: N√£o precisamos mais do JAR de Excel, pois leremos Parquet nativo.
spark = SparkSession.builder \
    .appName("Analise_Gastos_Fase2") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "8") \
    .config("spark.driver.bindAddress", "127.0.0.1") \
    .config("spark.driver.host", "127.0.0.1") \
    .master("local[*]") \
    .getOrCreate()

# Otimiza√ß√£o: Habilita Apache Arrow para converter Spark -> Pandas mais r√°pido (√∫til para gr√°ficos)
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

spark.sparkContext.setLogLevel("WARN")
print(f"‚úÖ Sess√£o Spark iniciada (Vers√£o {spark.version})")

# --- 3. Carregamento dos Dados ---
BASE_DIR = os.path.join(os.getcwd(), "dados")
input_path = os.path.join(BASE_DIR, "Consolidado_Final")

print(f"üìÇ Buscando base consolidada em: {input_path}")

if os.path.exists(input_path):
    try:
        # Leitura do Parquet (O Spark j√° entende o schema automaticamente)
        df = spark.read.parquet(input_path)
        
        # Cache: Como vamos usar esse DataFrame repetidas vezes para v√°rias an√°lises,
        # colocamos ele na mem√≥ria para n√£o ler do disco toda hora.
        df.cache()
        
        count = df.count()
        print(f"‚úÖ Sucesso! Base carregada com {count} registros.")
        
        print("\n--- Estrutura dos Dados (Schema) ---")
        df.printSchema()
        
        print("\n--- Amostra Inicial (Top 5) ---")
        df.show(5, truncate=False)
        
    except Exception as e:
        print(f"‚ùå Erro ao ler o arquivo Parquet: {e}")
else:
    print(f"‚ùå ARQUIVO N√ÉO ENCONTRADO. Verifique se a Fase 1 gerou a pasta: {input_path}")

# --- Fim do Script Inicial ---